{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 1.08M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(out_channels)\n",
    "        self.act1  = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        self.act2 = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.act1(self.bn1(self.conv1(x)))\n",
    "        out = self.act2(self.bn2(self.conv2(out)))\n",
    "        if self.shortcut is not None:\n",
    "            identity = self.shortcut(x)\n",
    "        return out + identity\n",
    "\n",
    "class CenterSubtileEncoder(nn.Module):\n",
    "    \"\"\"專門處理中心 subtile 的 Encoder\"\"\"\n",
    "    def __init__(self, out_dim, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.SiLU(), nn.MaxPool2d(2)\n",
    "        )  # 26→13\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResidualBlock(32, 64), nn.MaxPool2d(2)\n",
    "        )  # 13→6\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128)\n",
    "        )  # 6×6\n",
    "\n",
    "        # 多尺度池化\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.mid_pool    = nn.AdaptiveAvgPool2d((2,2))\n",
    "        self.large_pool  = nn.AdaptiveAvgPool2d((3,3))\n",
    "\n",
    "        total_dim = 128*1*1 + 128*2*2 + 128*3*3\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(total_dim, out_dim*2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(out_dim*2, out_dim),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        g = self.global_pool(x).view(x.size(0), -1)\n",
    "        m = self.mid_pool(x).view(x.size(0), -1)\n",
    "        l = self.large_pool(x).view(x.size(0), -1)\n",
    "        return self.fc(torch.cat([g, m, l], dim=1))\n",
    "\n",
    "class NeighborSubtileEncoder(nn.Module):\n",
    "    \"\"\"共享權重，對多個鄰居 subtiles 做 mean pooling\"\"\"\n",
    "    def __init__(self, out_dim, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.SiLU(), nn.MaxPool2d(2)\n",
    "        )  # h→13\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResidualBlock(32, 64), nn.MaxPool2d(2)\n",
    "        )  # 13→6\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128)\n",
    "        )  # 6×6\n",
    "\n",
    "        # 多尺度池化\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.mid_pool    = nn.AdaptiveAvgPool2d((2,2))\n",
    "        self.large_pool  = nn.AdaptiveAvgPool2d((3,3))\n",
    "\n",
    "        total_dim = 128*1*1 + 128*2*2  + 128*3*3\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(total_dim, out_dim*2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(out_dim*2, out_dim),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, subtiles):\n",
    "        # subtiles: [B,9,3,26,26]\n",
    "        B, N, C, H, W = subtiles.shape\n",
    "        x = subtiles.view(B*N, C, H, W)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        g = self.global_pool(x).view(B, N, -1)\n",
    "        m = self.mid_pool(x).view(B, N, -1)\n",
    "        l = self.large_pool(x).view(B, N, -1)\n",
    "        feats = torch.cat([g, m, l], dim=2)  # [B,N,total_dim]\n",
    "        f = self.fc(feats.view(B*N, -1)).view(B, N, -1)\n",
    "        return f.mean(dim=1)  # [B,out_dim]\n",
    "                  # [B,out_dim]\n",
    "\n",
    "class VisionMLP_MultiTask(nn.Module):\n",
    "    \"\"\"融合 Tile、Center、Neighbor 三路特徵的多任務模型\"\"\"\n",
    "    def __init__(self, center_dim=64, neighbor_dim=64, output_dim=35):\n",
    "        super().__init__()\n",
    "        self.encoder_center    = CenterSubtileEncoder(center_dim)\n",
    "        self.encoder_neighbors = NeighborSubtileEncoder(neighbor_dim)\n",
    "        fusion_dim = center_dim + neighbor_dim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 128), nn.SiLU(), nn.Dropout(0.1), nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, tile, subtiles):\n",
    "        center = subtiles[:, 4]\n",
    "        f_center = self.encoder_center(center)\n",
    "        f_neigh = self.encoder_neighbors(subtiles)\n",
    "        x = torch.cat([f_center, f_neigh], dim=1)\n",
    "        return self.decoder(x)\n",
    "\n",
    "# Instantiate and count parameters\n",
    "model = VisionMLP_MultiTask( center_dim=64, neighbor_dim=64, output_dim=35)\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same in multiple .pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded keys: dict_keys(['subtiles', 'label', 'tile', 'slide_idx', 'source_idx'])\n",
      "Samples: 8348\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import inspect\n",
    "from python_scripts.import_data import load_all_tile_data\n",
    "\n",
    "# 用法範例\n",
    "#folder = \"dataset/spot-rank/version-3/only_tile_sub/original_train\"\n",
    "\n",
    "folder = \"dataset/spot-rank/version-4/realign/log2_nor_rank_spots/realign_all/train_data\"\n",
    "grouped_data = load_all_tile_data( \n",
    "        folder_path=folder,\n",
    "        model=model,\n",
    "        fraction=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # grouped_data 現在只會有 model.forward() 需要的 key，\n",
    "    # 像 ['tile','subtiles','neighbors','norm_coord','node_feat','adj_list','edge_feat','label','source_idx']\n",
    "print(\"Loaded keys:\", grouped_data.keys())\n",
    "print(\"Samples:\", len(next(iter(grouped_data.values()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_scripts.resampling import check_source_idx_consistency\n",
    "\n",
    "    # 示例调用（假设已有 train_aug 字典）\n",
    "result = check_source_idx_consistency(train_aug, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from python_scripts.operate_model import train_one_epoch, evaluate, predict, EarlyStopping, plot_losses, plot_per_cell_metrics\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------------------\n",
    "# 指定儲存資料夾\n",
    "# ---------------------------\n",
    "save_folder = \"/Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/resampling/CNN+Res+MLP/log2_nor_rank/k-fold_v1/realign_all/\"  # 修改為你想要的資料夾名稱\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 2) 從 grouped_data 取出 slide_idx，轉成 numpy\n",
    "# --------------------------------------------\n",
    "import numpy as np\n",
    "slide_idx = np.array(grouped_data['slide_idx'])   # shape (N,)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 3) 建立 LOGO（或改成 GroupKFold）\n",
    "# --------------------------------------------\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# X 可以給虛擬矩陣，因為分組只靠 groups\n",
    "X_dummy = np.zeros(len(slide_idx))\n",
    "X_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_scripts.aug import plot_augmented_by_source\n",
    "\n",
    "from python_scripts.aug import augment_grouped_data\n",
    "\n",
    "# grouped_data: load_all_tile_data 返回的 dict\n",
    "augmented_data = augment_grouped_data(\n",
    "    grouped_data=grouped_data,\n",
    "    image_keys=['tile','subtiles'],\n",
    "    repeats=4   # 比如对每张做 2 次增强\n",
    ")\n",
    "source_ids = list(dict.fromkeys(grouped_data['source_idx']))  # 保持顺序去重\n",
    "\n",
    "plot_augmented_by_source(source_ids[:1], augmented_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.6 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 0 =====\n",
      "Starting oversample and augment grouped data by top3...\n",
      "每组目标大小：66，最大组大小：1281，总样本数：6151，分组数：1199\n",
      "Starting importDataset...\n",
      "Starting DataLoader...\n",
      "Starting model...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 68/1237 [00:12<02:34,  7.58it/s, avg=33.9, loss=14.8]"
     ]
    }
   ],
   "source": [
    "import os, math, numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.aug import (\n",
    "    subset_grouped_data,\n",
    "    identity,\n",
    ")\n",
    "from python_scripts.resampling import oversample_and_augment_grouped_data_by_topk\n",
    "\n",
    "# ----------- 超参 -----------\n",
    "start_fold    = 0\n",
    "BATCH_SIZE    = 64\n",
    "num_epochs    = 150\n",
    "K_top         = 3    # Top-K\n",
    "N_max         = 80000\n",
    "fill_to_n_max = False\n",
    "rank_cols     = [f\"rank_C{i}\" for i in range(1,36)]\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "logo   = LeaveOneGroupOut()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "overall = []\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "        logo.split(X=np.zeros(len(slide_idx)), y=None, groups=slide_idx)):\n",
    "\n",
    "    if fold_id < start_fold:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n===== Fold {fold_id} =====\")\n",
    "\n",
    "    # A) 拆出当折的 train_base / val_base\n",
    "    train_base = subset_grouped_data(grouped_data, tr_idx)\n",
    "    val_base   = subset_grouped_data(grouped_data, va_idx)\n",
    "    # 1) Top-K capped oversample + 立即 augmentation\n",
    "    print(f\"Starting oversample and augment grouped data by top{K_top}...\")\n",
    "    train_ds = oversample_and_augment_grouped_data_by_topk(\n",
    "        grouped_data   = train_base,\n",
    "        rank_cols      = rank_cols,\n",
    "        K              = K_top,\n",
    "        N_max          = N_max,\n",
    "        fill_to_n_max  = fill_to_n_max,\n",
    "        image_keys     = ['tile','subtiles'],\n",
    "        slide_key      = 'slide_idx',\n",
    "        label_key      = 'label',\n",
    "        random_state   = 42\n",
    "    )\n",
    "    # from python_scripts.resampling import check_source_idx_consistency\n",
    "\n",
    "    # # 示例调用（假设已有 train_aug 字典）\n",
    "    # result = check_source_idx_consistency(train_aug, random_state=42)\n",
    "    print(\"Starting importDataset...\")\n",
    "    # 1) 原始 dataset\n",
    "    train_ds = importDataset(train_ds, model,\n",
    "                            image_keys=['tile','subtiles'],\n",
    "                            transform=identity)\n",
    "\n",
    "\n",
    "    # 3) validation raw\n",
    "    val_ds     = subset_grouped_data(grouped_data, va_idx)\n",
    "    val_ds     = importDataset(val_ds, model,\n",
    "                            image_keys=['tile','subtiles'],\n",
    "                            transform=identity)\n",
    "\n",
    "    print(\"Starting DataLoader...\")\n",
    "    # 4) DataLoader 不再動態增強\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=0, pin_memory=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "    # 5) 一切其餘步驟同之前：定模型、optimizer、train_one_epoch、evaluate……\n",
    "\n",
    "    print(\"Starting model...\")\n",
    "    # ----- 新建模型 / 優化器 -----\n",
    "    net = VisionMLP_MultiTask( center_dim=64, neighbor_dim=64, output_dim=35).to(device)\n",
    "\n",
    "    # 我们把原来 Adam 换成 AdamW，稍微加一点 weight decay\n",
    "    peak_lr       = 5e-4\n",
    "    min_lr        = 1e-6\n",
    "    warmup_epochs = 0\n",
    "    total_epochs  = 20\n",
    "\n",
    "    optimizer = torch.optim.AdamW(net.parameters(),\n",
    "                                  lr=peak_lr,\n",
    "                                  weight_decay=1e-3)\n",
    "\n",
    "    # 定义 lr_lambda\n",
    "    def lr_lambda(cur_epoch):\n",
    "        if cur_epoch < warmup_epochs:\n",
    "            # 线性 warm-up: 从 0 → 1\n",
    "            return float(cur_epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            # 余弦退火：从 1 → min_lr/peak_lr\n",
    "            progress = (cur_epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "            # cos(0)=1 → cos(pi)=−1, remap to [min_ratio,1]\n",
    "            min_ratio = min_lr / peak_lr\n",
    "            return min_ratio + 0.5 * (1 - min_ratio) * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    stopper = EarlyStopping(patience=10)\n",
    "\n",
    "    # ----- fold 專屬輸出路徑 -----\n",
    "    fold_dir  = os.path.join(save_folder, f\"fold{fold_id}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(fold_dir, \"best_model.pt\")\n",
    "    loss_plot_path  = os.path.join(fold_dir, \"loss_curve.png\")\n",
    "    csv_path        = os.path.join(fold_dir, \"training_log.csv\")\n",
    "\n",
    "    # ----- CSV log -----\n",
    "    log_f = open(csv_path, \"w\", newline=\"\")\n",
    "    csv_w = csv.writer(log_f)\n",
    "    csv_w.writerow([\"Epoch\",\"TrainLoss\",\"ValLoss\",\"ValSpearman\",\"LR\"])\n",
    "\n",
    "    # ----- 圖形 -----\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "    display(fig)\n",
    "\n",
    "    train_losses = []; val_losses = []\n",
    "    train_rhos   = []; val_rhos   = []\n",
    "\n",
    "    best_rho = -1.0\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        tloss, trho = train_one_epoch(\n",
    "            net, train_loader, optimizer, device,\n",
    "            current_epoch=epoch, initial_alpha=0, final_alpha=0, target_epoch=20)\n",
    "\n",
    "        vloss, vrho, mse_cell, rho_cell = evaluate(\n",
    "            net, val_loader, device,\n",
    "            current_epoch=epoch, initial_alpha=0, final_alpha=0, target_epoch=20)\n",
    "\n",
    "        clear_output(wait=True)  # 清除之前的輸出\n",
    "        axes[0][0].clear()\n",
    "        axes[0][1].clear()\n",
    "        axes[1][0].clear()\n",
    "        axes[1][1].clear()\n",
    "        # --- save best ---\n",
    "        if vrho > best_rho:\n",
    "            best_rho = vrho\n",
    "            torch.save(net.state_dict(), best_model_path)\n",
    "            print(f\"✅ Saved best model in {best_model_path}!\")\n",
    "\n",
    "        # --- scheduler / early stop ---\n",
    "        scheduler.step()\n",
    "        stopper(vloss)\n",
    "\n",
    "\n",
    "    \n",
    "        # --- logging ---\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        csv_w.writerow([epoch+1, tloss, vloss, vrho, lr])\n",
    "\n",
    "        train_losses.append(tloss); val_losses.append(vloss)\n",
    "        train_rhos.append(trho);   val_rhos.append(vrho)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # 印出 Epoch 結果\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"[Epoch {epoch+1}]  lr={lr:.2e}, train_loss={tloss:.4f}, val_loss={vloss:.4f}\")\n",
    "        print(f\"train spearman: {trho:.4f} | Val spearman: {vrho:.4f} | best: {best_rho:.4f}\")\n",
    "        # --- update plots ---\n",
    "        plot_losses(train_losses, val_losses, axes[0][0], \"MSE Loss\")\n",
    "        plot_losses(train_rhos,   val_rhos,   axes[0][1], \"Spearman\")\n",
    "        cell_names = [f\"C{i+1}\" for i in range(35)]\n",
    "        plot_per_cell_metrics(mse_cell, rho_cell, cell_names,\n",
    "                              ax_mse=axes[1][0], ax_spearman=axes[1][1])\n",
    "        plt.tight_layout(); display(fig); plt.pause(0.1)\n",
    "        fig.savefig(loss_plot_path)\n",
    "        print(f\"曲線圖已儲存至 {loss_plot_path}\")\n",
    "        if stopper.early_stop:\n",
    "            print(\"⛔ early stop\"); break\n",
    "\n",
    "    log_f.close(); plt.close(fig)\n",
    "    overall_best.append(best_rho) \n",
    "    print(f\"📈 Fold {fold_id} best ρ = {best_rho:.4f}\")\n",
    "\n",
    "# ========= 整體結果 =========\n",
    "overall_best = np.array(overall_best)\n",
    "print(\"\\n=========== CV summary ===========\")\n",
    "for i, r in enumerate(overall_best):\n",
    "    print(f\"fold {i}: best ρ = {r:.4f}\")\n",
    "print(f\"overall best (mean) ρ = {overall_best.mean():.4f} ± {overall_best.std():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import rankdata\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.operate_model import predict\n",
    "full_dataset = importDataset(grouped_data, model,\n",
    "                             image_keys=['tile','subtiles'],\n",
    "                             transform=lambda x: x)\n",
    "# ---------------- Settings ----------------\n",
    "save_root  = save_folder  # your save_folder path\n",
    "n_folds    = len([d for d in os.listdir(save_root) if d.startswith('fold')])\n",
    "n_samples  = len(full_dataset)\n",
    "C          = 35  # num cell types\n",
    "# If optimizing Spearman, convert labels to ranks\n",
    "use_rank   = False\n",
    "\n",
    "# --- 1) Prepare OOF meta-features ---\n",
    "# Initialize matrix for OOF predictions\n",
    "n_samples = len(full_dataset)\n",
    "oof_preds = np.zeros((n_samples, C), dtype=np.float32)\n",
    "# True labels (raw or rank)\n",
    "# importDataset returns a dict-like sample, so label is under key 'label'\n",
    "y_true = np.vstack([ full_dataset[i]['label'].cpu().numpy() for i in range(n_samples) ])\n",
    "if use_rank:\n",
    "    y_meta = np.apply_along_axis(rankdata, 1, y_true)\n",
    "else:\n",
    "    y_meta = y_true\n",
    "\n",
    "# Build CV splitter (must match first stage splits)\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over folds, load best model, predict on validation indices\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "        logo.split(X=np.zeros(n_samples), y=None, groups=slide_idx)):\n",
    "    # Load model\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    print(f\"Loading model from {ckpt_path}...\")\n",
    "    net = VisionMLP_MultiTask( center_dim=64, neighbor_dim=64, output_dim=35).to(device)\n",
    "    # Alternatively, if your model requires specific args, replace with:\n",
    "    # net = VisionMLP_MultiTask(tile_dim=64, subtile_dim=64, output_dim=35).to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.to(device).eval()\n",
    "\n",
    "    # Predict on validation set\n",
    "    val_ds = Subset(full_dataset, va_idx)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = predict(net, val_loader, device)  # (n_val, C)\n",
    "    oof_preds[va_idx] = preds\n",
    "    print(f\"Fold {fold_id}: OOF preds shape {preds.shape}\")\n",
    "\n",
    "# --- 2) Train LightGBM meta-model ---\n",
    "# Choose objective: regression on rank (for Spearman) or raw (for MSE)\n",
    "lgb_base = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=1000,\n",
    "    num_leaves=31,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    n_jobs=-1,\n",
    "    force_col_wise=True\n",
    ")\n",
    "meta_model = MultiOutputRegressor(lgb_base)\n",
    "print(\"Training LightGBM on OOF meta-features...\")\n",
    "meta_model.fit(oof_preds, y_meta)\n",
    "# Save meta-model\n",
    "joblib.dump(meta_model, os.path.join(save_root, 'meta_model.pkl'))\n",
    "\n",
    "# --- 3) Prepare test meta-features ---\n",
    "n_test = len(test_dataset)\n",
    "test_meta = np.zeros((n_test, C), dtype=np.float32)\n",
    "for fold_id in range(n_folds):\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = VisionMLP_MultiTask( center_dim=64, neighbor_dim=64, output_dim=35).to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.to(device).eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = predict(net, test_loader, device)\n",
    "    test_meta += preds\n",
    "# Average across folds\n",
    "test_meta /= n_folds\n",
    "\n",
    "# --- 4) Meta-model predict ---\n",
    "if use_rank:\n",
    "    final_preds = meta_model.predict(test_meta) / (C + 1)\n",
    "else:\n",
    "    final_preds = meta_model.predict(test_meta)\n",
    "\n",
    "# --- Save submission ---\n",
    "import h5py\n",
    "import pandas as pd\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spot_ids = pd.DataFrame(np.array(f[\"spots/Test\"][\"S_7\"]))\n",
    "sub = pd.DataFrame(final_preds, columns=[f\"C{i+1}\" for i in range(C)])\n",
    "sub.insert(0, 'ID', test_spot_ids.index)\n",
    "sub.to_csv(os.path.join(save_root, 'submission_stacked.csv'), index=False)\n",
    "print(\"✅ Saved stacked submission.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import rankdata\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.operate_model import predict\n",
    "full_dataset = importDataset(grouped_data, model,\n",
    "                             image_keys=['tile','subtiles'],\n",
    "                             transform=lambda x: x)\n",
    "# ---------------- Settings ----------------\n",
    "save_root  = save_folder  # your save_folder path\n",
    "n_folds    = len([d for d in os.listdir(save_root) if d.startswith('fold')])\n",
    "n_samples  = len(full_dataset)\n",
    "C          = 35  # num cell types\n",
    "# If optimizing Spearman, convert labels to ranks\n",
    "use_rank   = True\n",
    "\n",
    "# --- 1) Prepare OOF meta-features ---\n",
    "# Initialize matrix for OOF predictions\n",
    "n_samples = len(full_dataset)\n",
    "oof_preds = np.zeros((n_samples, C), dtype=np.float32)\n",
    "# True labels (raw or rank)\n",
    "# importDataset returns a dict-like sample, so label is under key 'label'\n",
    "y_true = np.vstack([ full_dataset[i]['label'].cpu().numpy() for i in range(n_samples) ])\n",
    "if use_rank:\n",
    "    y_meta = np.apply_along_axis(rankdata, 1, y_true)\n",
    "else:\n",
    "    y_meta = y_true\n",
    "\n",
    "# Build CV splitter (must match first stage splits)\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over folds, load best model, predict on validation indices\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "        logo.split(X=np.zeros(n_samples), y=None, groups=slide_idx)):\n",
    "    # Load model\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    print(f\"Loading model from {ckpt_path}...\")\n",
    "    net = model.to(device)\n",
    "    # Alternatively, if your model requires specific args, replace with:\n",
    "    # net = VisionMLP_MultiTask(tile_dim=64, subtile_dim=64, output_dim=35).to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.to(device).eval()\n",
    "\n",
    "    # Predict on validation set\n",
    "    val_ds = Subset(full_dataset, va_idx)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = predict(net, val_loader, device)  # (n_val, C)\n",
    "    oof_preds[va_idx] = preds\n",
    "    print(f\"Fold {fold_id}: OOF preds shape {preds.shape}\")\n",
    "\n",
    "# --- 2) Train LightGBM meta-model ---\n",
    "# Choose objective: regression on rank (for Spearman) or raw (for MSE)\n",
    "lgb_base = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    learning_rate=0.02,\n",
    "    n_estimators=1000,\n",
    "    num_leaves=31,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    n_jobs=-1\n",
    ")\n",
    "meta_model = MultiOutputRegressor(lgb_base)\n",
    "print(\"Training LightGBM on OOF meta-features...\")\n",
    "meta_model.fit(oof_preds, y_meta)\n",
    "# Save meta-model\n",
    "joblib.dump(meta_model, os.path.join(save_root, 'meta_model.pkl'))\n",
    "\n",
    "# --- 2.5) 读取 Test spot IDs 一次即可 ---\n",
    "import h5py, pandas as pd\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spot_ids = pd.DataFrame(np.array(f[\"spots/Test\"][\"S_7\"]))\n",
    "test_ids = test_spot_ids.index\n",
    "\n",
    "# --- 3) Prepare test meta-features & per-fold submissions ---\n",
    "n_test = len(test_dataset)\n",
    "test_meta = np.zeros((n_test, C), dtype=np.float32)\n",
    "\n",
    "for fold_id in range(n_folds):\n",
    "    # 3.1) load fold model\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = model.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    # 3.2) predict on test set\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    preds = predict(net, test_loader, device)  # shape (n_test, C)\n",
    "\n",
    "    # 3.3) 保存这一折的 submission_{fold_id}.csv\n",
    "    sub_fold = pd.DataFrame(preds, columns=[f\"C{i+1}\" for i in range(C)])\n",
    "    sub_fold.insert(0, 'ID', test_ids)\n",
    "    sub_fold.to_csv(os.path.join(save_root, f\"submission_fold{fold_id}.csv\"), index=False)\n",
    "    print(f\"✅ Saved fold-{fold_id} submission: submission_fold{fold_id}.csv\")\n",
    "\n",
    "    # 3.4) 累加到 test_meta\n",
    "    test_meta += preds\n",
    "\n",
    "# 3.5) 平均\n",
    "test_meta /= n_folds\n",
    "\n",
    "# --- 4) 最终 Stacked Submission ---\n",
    "if use_rank:\n",
    "    final_preds = meta_model.predict(test_meta) / (C + 1)\n",
    "else:\n",
    "    final_preds = meta_model.predict(test_meta)\n",
    "\n",
    "sub = pd.DataFrame(final_preds, columns=[f\"C{i+1}\" for i in range(C)])\n",
    "sub.insert(0, 'ID', test_ids)\n",
    "sub.to_csv(os.path.join(save_root, 'submission_stacked.csv'), index=False)\n",
    "print(\"✅ Saved stacked submission: submission_stacked.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "\n",
    "def load_node_feature_data(pt_path: str, model, num_cells: int = 35) -> dict:\n",
    "    \"\"\"\n",
    "    根据 model.forward 的参数自动加载 .pt 里对应的字段，\n",
    "    并且如果没有 label，就自动创建一个全 0 的 label 张量，\n",
    "    其尺寸为 (样本数, num_cells)，样本数从第一个有 __len__ 的输入推断。\n",
    "\n",
    "    参数：\n",
    "      pt_path:     str，.pt 文件路径\n",
    "      model:       已实例化的 PyTorch 模型\n",
    "      num_cells:   int，label 的列数（默认 35）\n",
    "\n",
    "    返回：\n",
    "      dict: key 对应模型 forward 中的参数名（不含 self），\n",
    "            value 是对应的 Tensor/ndarray，\n",
    "            并额外保证有 'label' 字段。\n",
    "    \"\"\"\n",
    "    # 1) 载入原始数据\n",
    "    raw = torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "    # 2) 取模型 forward 入参签名（不含 self）\n",
    "    sig = inspect.signature(model.forward)\n",
    "    param_names = [p for p in sig.parameters if p != \"self\"]\n",
    "\n",
    "    out = {}\n",
    "    for name in param_names:\n",
    "        # a) 直接同名\n",
    "        if name in raw:\n",
    "            out[name] = raw[name]\n",
    "            continue\n",
    "        # b) 复数形式\n",
    "        if name + \"s\" in raw:\n",
    "            out[name] = raw[name + \"s\"]\n",
    "            continue\n",
    "        # c) 模糊匹配（下划线、复数或前后缀）\n",
    "        cands = [k for k in raw if name in k or k in name]\n",
    "        if len(cands) == 1:\n",
    "            out[name] = raw[cands[0]]\n",
    "            continue\n",
    "        raise KeyError(f\"无法找到 '{name}' 在 pt 文件中的对应字段，raw keys: {list(raw.keys())}\")\n",
    "\n",
    "    # 3) 用第一个支持 len() 的输入推断样本数\n",
    "    dataset_size = None\n",
    "    for v in out.keys():\n",
    "        if hasattr(out[v], \"__len__\"):\n",
    "            dataset_size = len(out[v])\n",
    "            print(f\"⚠️ 从 '{v}' 推断样本数量: {dataset_size}\")\n",
    "            break\n",
    "    if dataset_size is None:\n",
    "        raise RuntimeError(\"无法从任何输入中推断样本数量，请检查 pt 文件内容。\")\n",
    "\n",
    "    # 4) 自动补 label\n",
    "\n",
    "    out[\"label\"] = torch.zeros((dataset_size, num_cells), dtype=torch.float32)\n",
    "    return out\n",
    "\n",
    "\n",
    "image_keys = [ 'tile', 'subtiles']\n",
    "\n",
    "\n",
    "# 用法示例\n",
    "from python_scripts.import_data import importDataset\n",
    "# 假设你的 model 已经定义好并实例化为 `model`\n",
    "test_dataset = load_node_feature_data(\"dataset/spot-rank/version-1/only_tile_sub/test/test_dataset.pt\", model)\n",
    "test_dataset = importDataset(\n",
    "        data_dict=test_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 讀 test spot index\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spots     = f[\"spots/Test\"]\n",
    "    test_spot_table= pd.DataFrame(np.array(test_spots['S_7']))\n",
    "\n",
    "fold_ckpts = sorted(glob.glob(os.path.join(save_folder, \"fold*\", \"best_model.pt\")))\n",
    "models = []\n",
    "for ckpt in fold_ckpts:\n",
    "    net = VisionMLP_MultiTask(center_dim=64, neighbor_dim=64, output_dim=35).to(device)\n",
    "    net.load_state_dict(torch.load(ckpt, map_location=\"cpu\"))\n",
    "    net.to(device).eval()\n",
    "    models.append(net)\n",
    "\n",
    "all_fold_preds = []\n",
    "for fold_id, net in enumerate(models):\n",
    "    # 推論\n",
    "    with torch.no_grad():\n",
    "        preds = predict(net, test_loader, device)  # (N_test,35) numpy array\n",
    "\n",
    "    # 1) 存每一折的原始預測\n",
    "    df_fold = pd.DataFrame(preds, columns=[f\"C{i+1}\" for i in range(preds.shape[1])])\n",
    "    df_fold.insert(0, \"ID\", test_spot_table.index)\n",
    "    path_fold = os.path.join(save_folder, f\"submission_fold{fold_id}.csv\")\n",
    "    df_fold.to_csv(path_fold, index=False)\n",
    "    print(f\"✅ Saved fold {fold_id} predictions to {path_fold}\")\n",
    "\n",
    "    all_fold_preds.append(preds)\n",
    "\n",
    "# 2) 做 rank‐average ensemble\n",
    "all_fold_preds = np.stack(all_fold_preds, axis=0)       # (K, N_test, 35)\n",
    "ranks          = all_fold_preds.argsort(axis=2).argsort(axis=2).astype(float)\n",
    "mean_rank      = ranks.mean(axis=0)                    # (N_test,35)\n",
    "\n",
    "# 3) 存 final ensemble\n",
    "df_ens = pd.DataFrame(mean_rank, columns=[f\"C{i+1}\" for i in range(mean_rank.shape[1])])\n",
    "df_ens.insert(0, \"ID\", test_spot_table.index)\n",
    "path_ens = os.path.join(save_folder, \"submission_rank_ensemble.csv\")\n",
    "df_ens.to_csv(path_ens, index=False)\n",
    "print(f\"✅ Saved rank‐ensemble submission to {path_ens}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
