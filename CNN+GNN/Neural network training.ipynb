{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6db04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b70981c-db23-4ea7-bd9c-dca6279d7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13fdff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/2116005886.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_image_data = torch.load(\"../SML_train_dataset.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/2116005886.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(\"../train_graph_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ËºâÂÖ•Ë≥áÊñô\n",
    "train_image_data = torch.load(\"../SML_train_dataset.pt\")\n",
    "graph_data = torch.load(\"../train_graph_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db02b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class importDataset(Dataset):\n",
    "    def __init__(self, S_tiles, M_tiles, L_tiles, labels, meta_info, normal_coords, node_feats, adj_lists,edge_feats):\n",
    "        \"\"\"\n",
    "        Custom Dataset to load image tiles, labels, and edge indices for training.\n",
    "        \n",
    "        Args:\n",
    "            S_tiles (list): Small scale image tiles (spot-level features).\n",
    "            M_tiles (list): Medium scale image tiles (spot-level features).\n",
    "            L_tiles (list): Large scale image tiles (spot-level features).\n",
    "            labels (list): Ground truth labels (spot-level cell type compositions).\n",
    "            meta_info (list): Metadata containing slide_id, x, y coordinates for each spot.\n",
    "            normal_coords (list): Normalized coordinates for each spot.\n",
    "            slide_edge_indices (dict): Dictionary containing the slide-level edge indices for each slide.\n",
    "        \"\"\"\n",
    "        self.S_tiles = S_tiles\n",
    "        self.M_tiles = M_tiles\n",
    "        self.L_tiles = L_tiles\n",
    "        self.labels = labels\n",
    "        self.meta_info = meta_info\n",
    "        self.normal_coords = normal_coords\n",
    "        self.node_feats = node_feats\n",
    "        self.adj_lists = adj_lists\n",
    "        self.edge_feats = edge_feats\n",
    "        # Optional: Apply any additional processing to the data here\n",
    "        # e.g., scaling, normalization, etc.\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples (spots) in the dataset\n",
    "        return len(self.S_tiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the spot-level data for the sample at index `idx`\n",
    "        S_tile = self.S_tiles[idx]\n",
    "        M_tile = self.M_tiles[idx]\n",
    "        L_tile = self.L_tiles[idx]\n",
    "        label = self.labels[idx]\n",
    "        meta = self.meta_info[idx]  # This is a tuple (slide_id, x, y)\n",
    "        normal_coord = self.normal_coords[idx]\n",
    "        adj_list = self.adj_lists[idx]\n",
    "        \n",
    "        # Convert the tiles to the correct format (channels-first)\n",
    "        S_tile = torch.tensor(S_tile, dtype=torch.float).permute(2, 0, 1)  # Convert from (H, W, 3) to (3, H, W)\n",
    "        M_tile = torch.tensor(M_tile, dtype=torch.float).permute(2, 0, 1)\n",
    "        L_tile = torch.tensor(L_tile, dtype=torch.float).permute(2, 0, 1)\n",
    "        \n",
    "        # Convert label and normal_coord to tensors\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        normal_coord = torch.tensor(normal_coord, dtype=torch.float)\n",
    "        # ‚ûï Graph features\n",
    "        max_neighbors = 7  # or whatever k you use\n",
    "        adj_list = self.adj_lists[idx] if self.adj_lists is not None else []\n",
    "\n",
    "        # ËΩâÊàêÂõ∫ÂÆöÈï∑Â∫¶ÁöÑ Tensor\n",
    "        adj_array = np.zeros((max_neighbors, 2))\n",
    "        for i, (j, w) in enumerate(adj_list[:max_neighbors]):\n",
    "            adj_array[i] = [j, w]\n",
    "         # üõ†Ô∏è edge_feat ËΩâÂûãËôïÁêÜ\n",
    "        edge_feat_i = self.edge_feats[idx]\n",
    "        if isinstance(edge_feat_i, np.ndarray) and edge_feat_i.dtype == object:\n",
    "            edge_feat_i = np.array(edge_feat_i.tolist())\n",
    "        elif isinstance(edge_feat_i, list):\n",
    "            edge_feat_i = np.array(edge_feat_i)\n",
    "\n",
    "\n",
    "        graph_feats = {\n",
    "            'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float) if self.node_feats is not None else None,\n",
    "            'adj_list': torch.tensor(adj_array, dtype=torch.float),\n",
    "            'edge_feat': torch.tensor(edge_feat_i, dtype=torch.float) if edge_feat_i is not None else None,\n",
    "        }\n",
    "        # Return the data in a PyTorch-friendly format as a dictionary\n",
    "        return {\n",
    "            'S_tile': S_tile,\n",
    "            'M_tile': M_tile,\n",
    "            'L_tile': L_tile,\n",
    "            'label': label,\n",
    "            'meta': meta,\n",
    "            'normal_coord': normal_coord,\n",
    "            **graph_feats  # ‚ûï Âêà‰ΩµÈÄ≤ batch dictionary\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "401ef8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 2: Âª∫Á´ã Dataset ======\n",
    "image_train_dataset = importDataset(\n",
    "    S_tiles=train_image_data['S_tiles'],\n",
    "    M_tiles=train_image_data['M_tiles'],\n",
    "    L_tiles=train_image_data['L_tiles'],\n",
    "    labels=train_image_data['labels'],\n",
    "    meta_info=train_image_data['meta_info'],\n",
    "    normal_coords=train_image_data['normal_coords'],\n",
    "    node_feats=graph_data['node_feats'],\n",
    "    adj_lists=graph_data['adj_lists'],\n",
    "    edge_feats=graph_data['edge_feats']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e8a237d-3fff-47d5-831c-0b915cbecb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking dataset sample: 0\n",
      "üìè Center tile shape: torch.Size([3, 32, 32]) | dtype: torch.float32 | min: 0.451, max: 1.000\n",
      "üìè Subtiles shape: torch.Size([3, 64, 64]) | dtype: torch.float32\n",
      "üìè Neighbor tiles shape: torch.Size([3, 128, 128]) | dtype: torch.float32\n",
      "üß¨ Label shape: torch.Size([35]) | dtype: torch.float32\n",
      "üìå Coordinates (meta): x = 1554, y = 1297\n",
      "üìä Node feature shape: torch.Size([14]) | dtype: torch.float32\n",
      "üîó Edge feature shape: torch.Size([7, 5]) | dtype: torch.float32\n",
      "üîó Sample edge_feat[0]: tensor([26.0000, 26.0000,  0.0000,  0.0000,  0.0385])\n",
      "üìé Adjacency list shape: torch.Size([7, 2]) | dtype: torch.float32\n",
      "üìé Sample: tensor([[6.2800e+02, 3.8462e-02],\n",
      "        [2.3700e+02, 3.8462e-02],\n",
      "        [1.3880e+03, 3.7851e-02]])\n",
      "‚úÖ All checks passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/3814644018.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float) if self.node_feats is not None else None,\n"
     ]
    }
   ],
   "source": [
    "def check_dataset_item(dataset, idx=0):\n",
    "    item = dataset[idx]\n",
    "\n",
    "    print(\"üîç Checking dataset sample:\", idx)\n",
    "\n",
    "    # --- Center Tile ---\n",
    "    tile = item['S_tile']\n",
    "    print(f\"üìè Center tile shape: {tile.shape} | dtype: {tile.dtype} | min: {tile.min():.3f}, max: {tile.max():.3f}\")\n",
    "\n",
    "    # --- Subtiles ---\n",
    "    subtiles = item['M_tile']\n",
    "    print(f\"üìè Subtiles shape: {subtiles.shape} | dtype: {subtiles.dtype}\")\n",
    "\n",
    "    # --- Neighbors ---\n",
    "    neighbors = item['L_tile']\n",
    "    print(f\"üìè Neighbor tiles shape: {neighbors.shape} | dtype: {neighbors.dtype}\")\n",
    "\n",
    "    # --- Label ---\n",
    "    label = item['label']\n",
    "    print(f\"üß¨ Label shape: {label.shape} | dtype: {label.dtype}\")\n",
    "    assert label.shape[0] == 35 and label.dtype == torch.float32, \"‚ùå Label ÊáâÁÇ∫ float32 ‰∏îÈï∑Â∫¶ÁÇ∫ 35\"\n",
    "\n",
    "    # --- Coordinates (meta) ---\n",
    "    coordinates = item['meta']\n",
    "    print(f\"üìå Coordinates (meta): x = {coordinates[1]}, y = {coordinates[2]}\")\n",
    "\n",
    "    # --- Node Features ---\n",
    "    if 'node_feat' in item and item['node_feat'] is not None:\n",
    "        node_feat = item['node_feat']\n",
    "        print(f\"üìä Node feature shape: {node_feat.shape} | dtype: {node_feat.dtype}\")\n",
    "\n",
    "\n",
    "    # --- Edge Features ---\n",
    "    if 'edge_feat' in item and item['edge_feat'] is not None:\n",
    "        edge_feat = item['edge_feat']\n",
    "        print(f\"üîó Edge feature shape: {edge_feat.shape} | dtype: {edge_feat.dtype}\")\n",
    "        print(f\"üîó Sample edge_feat[0]: {edge_feat[0]}\")\n",
    "        assert edge_feat.ndim == 2 and edge_feat.shape[1] == 5, \"‚ùå Edge feature ÊáâÁÇ∫ (k, 5)\"\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Edge features Êú™Êèê‰æõ\")\n",
    "\n",
    "    # --- Adjacency List ---\n",
    "    if 'adj_list' in item and item['adj_list'] is not None:\n",
    "        adj_list = item['adj_list']\n",
    "        print(f\"üìé Adjacency list shape: {adj_list.shape} | dtype: {adj_list.dtype}\")\n",
    "        print(f\"üìé Sample: {adj_list[:3]}\")\n",
    "        assert adj_list.ndim == 2 and adj_list.shape[1] == 2, \"‚ùå Adjacency list ÊáâÁÇ∫ (k, 2)\"\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Adjacency list Êú™Êèê‰æõ\")\n",
    "\n",
    "    print(\"‚úÖ All checks passed!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "check_dataset_item(image_train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15c83a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train set size: 6679 samples\n",
      "‚úÖ Validation set size: 1670 samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "# Á¢∫ÂÆöÊï∏ÊìöÈõÜÁöÑÁ∏ΩÂ§ßÂ∞èÔºàÂÖ©ËÄÖÂøÖÈ†àÁõ∏ÂêåÔºâ\n",
    "dataset_size = len(image_train_dataset)  # ÂêåÊôÇ image_train_dataset Âíå graph_train_dataset Èï∑Â∫¶ÊáâÁõ∏Âêå\n",
    "\n",
    "# Ë®≠ÂÆöË®ìÁ∑¥ÊØî‰æãÔºåÈÄôË£°‰ª• 80% ÁÇ∫Ë®ìÁ∑¥ÈõÜÔºå20% ÁÇ∫È©óË≠âÈõÜ\n",
    "train_ratio = 0.8\n",
    "split_index = int(np.floor(train_ratio * dataset_size))\n",
    "\n",
    "# ÁîüÊàêÈö®Ê©üÁ¥¢Âºï\n",
    "indices = torch.randperm(dataset_size).tolist()\n",
    "\n",
    "# Â∞áÁ¥¢ÂºïÊãÜÂàÜÁÇ∫Ë®ìÁ∑¥ÂíåÈ©óË≠âÈÉ®ÂàÜ\n",
    "train_indices = indices[:split_index]\n",
    "val_indices = indices[split_index:]\n",
    "\n",
    "# ‰ΩøÁî® Subset Ê†πÊìöÁõ∏ÂêåÁöÑÁ¥¢ÂºïÂª∫Á´ãÂ≠êÈõÜ\n",
    "image_train_subset = Subset(image_train_dataset, train_indices)\n",
    "image_val_subset = Subset(image_train_dataset, val_indices)\n",
    "\n",
    "# ÁèæÂú®Ôºå‰Ω†ÊúâÂÖ©Â∞ç DataLoaderÔºå‰∏îÂÆÉÂÄëÁöÑÁ¥¢ÂºïÊòØ‰∏Ä‰∏ÄÂ∞çÊáâÁöÑ\n",
    "print(f\"‚úÖ Train set size: {len(image_train_subset)} samples\")\n",
    "print(f\"‚úÖ Validation set size: {len(image_val_subset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46d21906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image sample keys: dict_keys(['S_tile', 'M_tile', 'L_tile', 'label', 'meta', 'normal_coord', 'node_feat', 'adj_list', 'edge_feat'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/3814644018.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float) if self.node_feats is not None else None,\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(image_train_subset, batch_size=1, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(image_val_subset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "# Ê∏¨Ë©¶‰∏Ä‰∏ã DataLoader ÂèñÂá∫ÁöÑÊï∏Êìö\n",
    "for batch in train_loader:\n",
    "\n",
    "    print(\"Image sample keys:\", batch.keys() if hasattr(batch, 'keys') else \"Not a dict\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccdc3b",
   "metadata": {},
   "source": [
    "Poteintial issues:\n",
    "# 1. my val_set tiles image may be included in the sub_tiles of train_set\n",
    "\n",
    "Note: Since neighbor tiles are reused across samples, some mild information overlap may exist between train and val sets. However, final test set is completely held out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45327c2e",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f90c9b",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GraphSAGE, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class MultiLayerAttentionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers=1):\n",
    "        super(MultiLayerAttentionHead, self).__init__()\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim=input_dim, num_heads=1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = x\n",
    "        for attn_layer in self.attention_layers:\n",
    "            attn_output, _ = attn_layer(attn_output, attn_output, attn_output)  # Self-attention on the feature vector\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "\n",
    "# Define a CNN for each scale (S_tile, M_tile, L_tile)\n",
    "class SmallScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "class MediumScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MediumScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "class LargeScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        # GraphSAGE layer\n",
    "        self.graphsage = GraphSAGE(input_dim, hidden_dim, aggr='mean')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # For predicting 35 cell types\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Assuming `data` contains the features and edge_index for the graph\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.graphsage(x, edge_index)  # Apply GraphSAGE to the features\n",
    "        x = global_mean_pool(x, data.batch)  # Global pooling for each graph (slide)\n",
    "        return self.fc(x)  # Predict the cell type composition\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=1)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention mechanism\n",
    "        attn_output, _ = self.attention(x, x, x)  # Self-attention on the feature vector\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "\n",
    "class CellTypePredictionModel(nn.Module):\n",
    "    def __init__(self, small_cnn, medium_cnn, large_cnn, graphsage_model, attention_head):\n",
    "        super(CellTypePredictionModel, self).__init__()\n",
    "        self.small_cnn = small_cnn  # CNN for small-scale patch\n",
    "        self.medium_cnn = medium_cnn  # CNN for medium-scale patch\n",
    "        self.large_cnn = large_cnn  # CNN for large-scale patch\n",
    "        self.graphsage_model = graphsage_model  # GraphSAGE for graph-based feature learning\n",
    "        self.attention_head = attention_head  # Attention mechanism to focus on important features\n",
    "\n",
    "    def forward(self, S_tile, M_tile, L_tile, edge_index, normal_coords):\n",
    "        # Step 1: CNN feature extraction for each scale (S_tile, M_tile, L_tile)\n",
    "        S_features = self.small_cnn(S_tile)\n",
    "        M_features = self.medium_cnn(M_tile)\n",
    "        L_features = self.large_cnn(L_tile)\n",
    "        \n",
    "        # Combine features from all scales into one feature vector (F_i)\n",
    "        combined_features = torch.cat([S_features, M_features, L_features], dim=1)\n",
    "        \n",
    "        # Step 2: Create PyG Data object for Graph Neural Network\n",
    "        data = Data(x=combined_features, edge_index=edge_index, pos=normal_coords)\n",
    "        \n",
    "        # Step 3: GraphSAGE layer to learn spatial context-aware features\n",
    "        graphsage_output = self.graphsage_model(data)\n",
    "        \n",
    "        # Step 4: Attention mechanism to focus on the relevant features for cell-type prediction\n",
    "        attention_output = self.attention_head(graphsage_output.unsqueeze(0))  # Add batch dimension\n",
    "        return attention_output.squeeze(0)  # Remove batch dimension for final output\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the individual CNNs for each scale\n",
    "small_cnn = SmallScaleCNN()\n",
    "medium_cnn = MediumScaleCNN()\n",
    "large_cnn = LargeScaleCNN()\n",
    "\n",
    "# Instantiate the GraphSAGE model and attention head\n",
    "graphsage_model = GraphSAGEModel(input_dim=16 + 32 + 64, hidden_dim=256, output_dim=256)\n",
    "attention_head = AttentionHead(input_dim=256, output_dim=35)  # Predict 35 cell types\n",
    "\n",
    "# Instantiate the final model\n",
    "model = CellTypePredictionModel(small_cnn, medium_cnn, large_cnn, graphsage_model, attention_head)\n",
    "\n",
    "# Example input: Assuming you have a batch of data\n",
    "S_tile = torch.rand(32, 3, 32, 32)  # Example batch of small-scale image tiles\n",
    "M_tile = torch.rand(32, 3, 64, 64)  # Example batch of medium-scale image tiles\n",
    "L_tile = torch.rand(32, 3, 128, 128)  # Example batch of large-scale image tiles\n",
    "meta_info = [(f\"S_{i}\", torch.rand(2)) for i in range(32)]  # Dummy metadata for batch\n",
    "edge_index = torch.randint(0, 32, (2, 50))  # Dummy graph edge indices for each slide\n",
    "\n",
    "# Forward pass\n",
    "output = model(S_tile, M_tile, L_tile, meta_info, edge_index)\n",
    "print(\"Output shape:\", output.shape)  # Should be (batch_size, 35) for the 35 cell types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d68d1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "# --- MultiScaleCNN ÂÆöÁæ© ---\n",
    "class MultiScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiScaleCNN, self).__init__()\n",
    "        # S_branch: Input: [3,32,32] ‚Üí Output: [16,8,8]\n",
    "        self.conv_s = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),     # [16, 32, 32]\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [16, 16, 16]\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),     # [16, 16, 16]\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)            # [16, 8, 8]\n",
    "        )\n",
    "        \n",
    "        # M_branch: Input: [3,64,64] ‚Üí Output: [32,8,8]\n",
    "        self.conv_m = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),      # [32, 64, 64]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [32, 32, 32]\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),     # [32, 32, 32]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [32, 16, 16]\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),     # [32, 16, 16]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)            # [32, 8, 8]\n",
    "        )\n",
    "        \n",
    "        # L_branch: Input: [3,128,128] ‚Üí Output: [64,8,8]\n",
    "        self.conv_l = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),      # [64, 128, 128]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [64, 64, 64]\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),     # [64, 64, 64]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [64, 32, 32]\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),     # [64, 32, 32]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [64, 16, 16]\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),     # [64, 16, 16]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)            # [64, 8, 8]\n",
    "        )\n",
    "    def forward(self, S_tile, M_tile, L_tile):\n",
    "        # S_branch: flatten [batch, 16, 8, 8] ‚Üí [batch, 16*8*8] = [batch, 1024]\n",
    "        f_s = self.conv_s(S_tile).view(S_tile.size(0), -1)\n",
    "        # M_branch: flatten [batch, 32, 8, 8] ‚Üí [batch, 32*8*8] = [batch, 2048]\n",
    "        f_m = self.conv_m(M_tile).view(M_tile.size(0), -1)\n",
    "        # L_branch: flatten [batch, 64, 8, 8] ‚Üí [batch, 64*8*8] = [batch, 4096]\n",
    "        f_l = self.conv_l(L_tile).view(L_tile.size(0), -1)\n",
    "        # ÊãºÊé•‰∏âÂÄãÂ∞∫Â∫¶ÁöÑÁâπÂæµ\n",
    "        return torch.cat([f_s, f_m, f_l], dim=1)\n",
    "    \n",
    "# --- CustomGAT ÂÆöÁæ© ---\n",
    "class CustomGAT(nn.Module):\n",
    "    def __init__(self, in_features, n_heads=8, head_dim=32):\n",
    "        \"\"\"\n",
    "        in_features: ÊØèÂÄãÁØÄÈªûÂàùÂßãÁâπÂæµÁ∂≠Â∫¶ÔºàÈÄôË£°ÊáâÁÇ∫ 14Ôºâ\n",
    "        n_heads: Ê≥®ÊÑèÂäõÈ†≠Êï∏Ôºà‰æãÂ¶Ç 8Ôºâ\n",
    "        head_dim: ÊØèÂÄãÊ≥®ÊÑèÂäõÈ†≠Ëº∏Âá∫Á∂≠Â∫¶Ôºà‰æãÂ¶Ç 32Ôºâ\n",
    "        \"\"\"\n",
    "        super(CustomGAT, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        # Â∞çÊØèÂÄãÊ≥®ÊÑèÂäõÈ†≠ÈÉΩÊúâ‰∏ÄÂÄãÁ∑öÊÄßËÆäÊèõÁü©Èô£ÔºåËº∏Âá∫ shape: [B, head_dim]\n",
    "        self.W = nn.Parameter(torch.randn(n_heads, in_features, head_dim))\n",
    "        # Ê≥®ÊÑèÂäõË®àÁÆóÂèÉÊï∏ÔºåÂàÜÂà•Â∞ç query Âíå key ‰ΩúÁî®\n",
    "        self.a1 = nn.Parameter(torch.randn(n_heads, head_dim, 1))\n",
    "        self.a2 = nn.Parameter(torch.randn(n_heads, head_dim, 1))\n",
    "        self.leakyrelu = nn.LeakyReLU(0.01)\n",
    "    \n",
    "    def forward(self, node_features, adj_list, edge_feat):\n",
    "        \"\"\"\n",
    "        node_features: tensor, shape [B, in_features] ÔºõB = batch sizeÔºà‰ª£Ë°® B ÂÄã spotsÔºâ\n",
    "        adj_list: tensor, shape [B, num_neighbors, 2]\n",
    "                  ÂÅáË®≠ÊØèÂÄãÂÖÉÁ¥†ÁöÑÁ¨¨‰∏ÄÂÄãÂÄºÊòØÈÑ∞Â±ÖÁ¥¢ÂºïÔºàÂèØËÉΩÁÇ∫ floatÔºâÔºåÁ¨¨‰∫åÂÄãÂÄºÁÇ∫Ê¨äÈáçÔºàÂèØÂøΩÁï•Ôºâ\n",
    "        edge_feat: tensor, shape [B, num_neighbors, edge_feat_dim]Ôºõ‰æãÂ¶Ç edge_feat_dim = 5\n",
    "        \"\"\"\n",
    "        B = node_features.size(0)\n",
    "        num_neighbors = adj_list.size(1)\n",
    "        head_outputs = []\n",
    "        \n",
    "        for h in range(self.n_heads):\n",
    "            # Á∑öÊÄßËÆäÊèõÔºöÂ∞çÊØèÂÄãÁØÄÈªûÂÅöËÆäÊèõ\n",
    "            # transformed: [B, head_dim]\n",
    "            transformed = torch.matmul(node_features, self.W[h])\n",
    "            # ÂàùÂßãÂåñÊ≥®ÊÑèÂäõÂæóÂàÜÁü©Èô£ eÔºåÂΩ¢ÁãÄ [B, B]Ôºå‰ΩÜÊàëÂÄëÂè™Â∞çÊØèÂÄã node i ËàáÂÖ∂ÈÑ∞Â±Ö j ÈÄ≤Ë°åË®àÁÆó\n",
    "            # ÈÄôË£°ÊàëÂÄëÁî® -inf Â°´ÂÖÖÈùûÈÑ∞Â±Ö‰ΩçÁΩÆ\n",
    "            e = torch.full((B, B), float('-inf'), device=node_features.device)\n",
    "            \n",
    "            # Â∞çÊØèÂÄãÁØÄÈªû i\n",
    "            for i in range(B):\n",
    "                # ÂèñÂá∫Ë©≤ node ÁöÑÈÑ∞Â±ÖË≥áË®äÔºåÂΩ¢ÁãÄ [num_neighbors, 2]\n",
    "                # ÊàëÂÄëÈúÄË¶ÅÁç≤ÂæóÈÑ∞Â±ÖÁ¥¢ÂºïÔºå‰∏¶Á¢∫‰øùÁÇ∫Êï¥Êï∏\n",
    "                neighbors = adj_list[i, :, 0]  # shape: [num_neighbors]\n",
    "                # Â¶ÇÊûú neighbors ÊòØÊµÆÈªûÂûãÔºåÂ∞áÂÖ∂ËΩâÊèõÁÇ∫ int\n",
    "                neighbors = neighbors.long()  # Â¶ÇÊûúÂéüÂßãÂ≠òÁöÑÊòØ floatÔºåÂèØÁõ¥Êé•Áî® .long()\n",
    "                for k in range(num_neighbors):\n",
    "                    j = int(neighbors[k].item())  # j ÁÇ∫ÈÑ∞Â±ÖÁ¥¢Âºï\n",
    "                    # ÂèñÂæóËÆäÊèõÂæåÁöÑÁâπÂæµÔºöquery Ëàá key ÂàÜÂà• [head_dim, 1]\n",
    "                    query = transformed[i].unsqueeze(1)\n",
    "                    key   = transformed[j].unsqueeze(1)\n",
    "                    e_ij = torch.matmul(self.a1[h].T, query) + torch.matmul(self.a2[h].T, key)\n",
    "                    e_ij = self.leakyrelu(e_ij)\n",
    "                    # Â¶ÇÊûúÊèê‰æõ‰∫Ü edge_featÔºåÊ†πÊìöÈÇäÁâπÂæµË™øÊï¥ e_ij\n",
    "                    if edge_feat is not None:\n",
    "                        # ÂèñÂá∫Â∞çÊáâÈÇäÁöÑÁâπÂæµÔºåÂΩ¢ÁãÄ: [edge_feat_dim]\n",
    "                        current_edge_feat = edge_feat[i, k]  # tensor, shape: [edge_feat_dim]\n",
    "                        # ÈÄôË£°Á∞°ÂñÆ‰ΩøÁî®Á¨¨‰∏ÄÂÄãÂÄº‰ΩúÁÇ∫Ë™øÊï¥Âõ†Â≠ê\n",
    "                        e_ij *= 1.0 / (current_edge_feat[0] + 1e-6)\n",
    "                    # Â∞áË®àÁÆóÂ•ΩÁöÑ e_ij Â≠òÂÖ•Â∞çÊáâÁöÑ‰ΩçÁΩÆ [i, j]\n",
    "                    e[i, j] = e_ij.squeeze()\n",
    "            \n",
    "            # Â∞çÊØèÂÄãÁØÄÈªûÁöÑÈÑ∞Â±ÖÊ≥®ÊÑèÂäõÂæóÂàÜÈÄ≤Ë°å softmax\n",
    "            attention = F.softmax(e, dim=1)  # [B, B]\n",
    "            # ËÅöÂêàÈÑ∞Â±ÖÁöÑË®äÊÅØÔºöÂ∞çÊØèÂÄãÁØÄÈªû iÔºåÁî®ÈÑ∞Â±Ö j ÁöÑËÆäÊèõÂæåÁâπÂæµÂä†Ê¨äÊ±ÇÂíå\n",
    "            head_output = torch.zeros(B, self.head_dim, device=node_features.device)\n",
    "            for i in range(B):\n",
    "                weighted_sum = torch.zeros(self.head_dim, device=node_features.device)\n",
    "                for j in range(B):\n",
    "                    if attention[i, j] > 0:\n",
    "                        weighted_sum += attention[i, j] * transformed[j]\n",
    "                head_output[i] = F.relu(weighted_sum)\n",
    "            head_outputs.append(head_output)\n",
    "        \n",
    "        # Â§öÈ†≠ÊãºÊé•ÔºöÊúÄÁµÇËº∏Âá∫ shape [B, n_heads*head_dim]\n",
    "        multi_head_output = torch.cat(head_outputs, dim=1)\n",
    "        return multi_head_output\n",
    "# --- HEVisium Ê®°Âûã ---\n",
    "class HEVisium(nn.Module):\n",
    "    def __init__(self, cnn_backbone, custom_gat, final_out_dim, cnn_out_dim, gat_out_dim):\n",
    "        super(HEVisium, self).__init__()\n",
    "        self.cnn_backbone = cnn_backbone  # Áî®ÊñºÂúñÂÉèÁâπÂæµÊèêÂèñ\n",
    "        self.custom_gat = custom_gat      # Áî®ÊñºÂúñÁµêÊßãÁâπÂæµÊèêÂèñ\n",
    "        self.fc = nn.Linear(cnn_out_dim + gat_out_dim, final_out_dim)\n",
    "    \n",
    "    def forward(self, S_tile, M_tile, L_tile, node_features, adj_lists, edge_features):\n",
    "        # CNN ÈÉ®ÂàÜÔºåÊèêÂèñ image-based ÁâπÂæµ\n",
    "        cnn_features = self.cnn_backbone(S_tile, M_tile, L_tile)  # shape: [N, cnn_out_dim]\n",
    "        # GAT ÈÉ®ÂàÜÔºåÂü∫Êñº engineered node featuresÔºàÁ∂≠Â∫¶ 14Ôºâ„ÄÅadj_lists Ëàá edge_features\n",
    "        gat_embedding = self.custom_gat(node_features, adj_lists, edge_features)  # [N, gat_out_dim]\n",
    "        combined = torch.cat([cnn_features, gat_embedding], dim=1)\n",
    "        out = self.fc(combined)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8eb6c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader['adj_list'] shape: torch.Size([1, 7, 2])\n",
      "train_loader['node_feat'] shape: torch.Size([1, 14])\n",
      "train_loader['edge_feat'] shape: torch.Size([1, 7, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/3814644018.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float) if self.node_feats is not None else None,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch in train_loader:\n",
    "    \n",
    "    print(\"train_loader['adj_list'] shape:\", batch['adj_list'].shape)\n",
    "    print(\"train_loader['node_feat'] shape:\", batch['node_feat'].shape)\n",
    "    print(\"train_loader['edge_feat'] shape:\", batch['edge_feat'].shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16e85ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 15, 5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- ‰ΩøÁî®Á§∫‰æã ---\n",
    "if __name__ == \"__main__\":\n",
    "    # ÂÅáË®≠ÊàëÂÄëÊúâ‰ª•‰∏ãÊï∏ÊìöÔºåÂ∞çÊáâ spot Á¥öÂà•\n",
    "    N = 100  # 100 spots\n",
    "    # ÂúñÂÉè patch Êï∏ÊìöÔºà‰æÜËá™ CNN ÈÉ®ÂàÜÔºâÔºåÊØèÂÄãÂ∞∫ÂØ∏ÂàÜÂà•Â¶Ç‰∏ãÔºö\n",
    "    S_tile = torch.randn(N, 3, 32, 32)\n",
    "    M_tile = torch.randn(N, 3, 64, 64)\n",
    "    L_tile = torch.randn(N, 3, 128, 128)\n",
    "    # engineered node featuresÔºöÂÅáË®≠ÊØèÂÄã spot ÁÇ∫ 256 Á∂≠\n",
    "    node_features = torch.randn(N, 14)\n",
    "    # ÈÇä‰ø°ÊÅØÔºöadjacency_lists ÁÇ∫ÊØèÂÄã spot ÁöÑÈÑ∞Êé•ÂàóË°®ÔºåÈÄôË£°Ê®°Êì¨\n",
    "    adj_lists = [[(j, 1.0) for j in range(N) if j != i][:15] for i in range(N)]\n",
    "    # ÈÇäÁâπÂæµÔºöÂÅáË®≠ÊØèÊ¢ùÈÇäÁî® 5 Á∂≠ÁâπÂæµË°®Á§∫\n",
    "    edge_features = [[np.random.rand(5) for _ in range(15)] for i in range(N)]\n",
    "    \n",
    "    final_out_dim = 35  # È†êÊ∏¨ 35 ÂÄãÁ¥∞ËÉûÈ°ûÂûãÁöÑ abundance\n",
    "edge_features = np.array(edge_features)\n",
    "edge_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9d3692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# üß† Ë®ìÁ∑¥‰∏ÄÂÄã epoch\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        \n",
    "        S_tile, M_tile, L_tile = batch['S_tile'].to(device), batch['M_tile'].to(device), batch['L_tile'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        node_feat = batch['node_feat'].to(device)\n",
    "        adj_list = batch['adj_list'].to(device)\n",
    "        edge_feat = batch['edge_feat'].to(device)\n",
    "        # ÂÑ™ÂåñÂô®Ê∏ÖÁ©∫Ê¢ØÂ∫¶\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ÂâçÂêëÂÇ≥Êí≠ÔºöË®àÁÆóÈ†êÊ∏¨ÁµêÊûú\n",
    "        out = model(S_tile, M_tile, L_tile, node_feat, adj_list, edge_feat)\n",
    "\n",
    "        # Ë®àÁÆóÊêçÂ§±\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()  # ÂèçÂêëÂÇ≥Êí≠\n",
    "        optimizer.step()  # Êõ¥Êñ∞Ê®°ÂûãÂèÉÊï∏\n",
    "\n",
    "        total_loss += loss.item() * S_tile.size(0)\n",
    "        avg_loss = total_loss / ((pbar.n + 1) * dataloader.batch_size)\n",
    "        pbar.set_postfix(loss=loss.item(), avg=avg_loss)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "# üìè È©óË≠âÊ®°Âûã\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # Áç≤ÂèñÈ©óË≠âÊï∏Êìö\n",
    "            S_tile, M_tile, L_tile = batch['S_tile'].to(device), batch['M_tile'].to(device), batch['L_tile'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            node_feat = batch['node_feat'].to(device)\n",
    "            adj_list = batch['adj_list'].to(device)\n",
    "            edge_feat = batch['edge_feat'].to(device)\n",
    "            # ÂÑ™ÂåñÂô®Ê∏ÖÁ©∫Ê¢ØÂ∫¶\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # ÂâçÂêëÂÇ≥Êí≠ÔºöË®àÁÆóÈ†êÊ∏¨ÁµêÊûú\n",
    "            out = model(S_tile, M_tile, L_tile, node_feat, adj_list, edge_feat)\n",
    "\n",
    "\n",
    "            total_loss += loss.item() * S_tile.size(0)\n",
    "            preds.append(out.cpu())\n",
    "            targets.append(label.cpu())\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    targets = torch.cat(targets).numpy()\n",
    "\n",
    "    # ‰ΩøÁî® Spearman Áõ∏ÈóúÊÄß‰æÜË®àÁÆóÈ†êÊ∏¨ÁöÑÊ∫ñÁ¢∫ÊÄß\n",
    "    scores = [spearmanr(preds[:, i], targets[:, i])[0] for i in range(preds.shape[1])]\n",
    "    spearman_avg = np.nanmean(scores)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), spearman_avg\n",
    "\n",
    "\n",
    "\n",
    "    # ‚úÖ Spearman correlation for each gene\n",
    "    scores = [spearmanr(preds[:, i], targets[:, i])[0] for i in range(preds.shape[1])]\n",
    "    spearman_avg = np.nanmean(scores)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), spearman_avg\n",
    "\n",
    "# üîÆ È†êÊ∏¨\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_meta = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Áç≤ÂèñÊ∏¨Ë©¶Êï∏Êìö\n",
    "            S_tile, M_tile, L_tile = batch['S_tile'].to(device), batch['M_tile'].to(device), batch['L_tile'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            node_feat = batch['node_feat'].to(device)\n",
    "            adj_list = batch['adj_list'].to(device)\n",
    "            edge_feat = batch['edge_feat'].to(device)\n",
    "            # ÂÑ™ÂåñÂô®Ê∏ÖÁ©∫Ê¢ØÂ∫¶\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # ÂâçÂêëÂÇ≥Êí≠ÔºöË®àÁÆóÈ†êÊ∏¨ÁµêÊûú\n",
    "            out = model(S_tile, M_tile, L_tile, node_feat, adj_list, edge_feat)\n",
    "\n",
    "            all_preds.append(out.cpu())\n",
    "            all_meta.extend(batch['meta'])  # Áî®‰æÜÂ≠òÂÑ≤Ê∏¨Ë©¶ÈõÜÁöÑmeta‰ø°ÊÅØ\n",
    "\n",
    "    return torch.cat(all_preds).numpy(), all_meta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b268",
   "metadata": {},
   "source": [
    "# callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6e2ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None or val_loss < self.best_score:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(val_losses, label=\"Val Loss\", marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.show()\n",
    "# Êî∂ÈõÜË≥áÊñô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c9b4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: mps\n",
      "Calculated CNN output dimension: 7168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/6679 [00:00<?, ?it/s]/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/3814644018.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float) if self.node_feats is not None else None,\n",
      "                                                  \r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1292 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 57\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     val_loss, val_spearman \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, loss_fn, device)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# ‚úÖ ÂÑ≤Â≠òÊúÄÂ•ΩÁöÑÊ®°Âûã\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 23\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# ÂâçÂêëÂÇ≥Êí≠ÔºöË®àÁÆóÈ†êÊ∏¨ÁµêÊûú\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS_tile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM_tile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_tile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Ë®àÁÆóÊêçÂ§±\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out, label)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 155\u001b[0m, in \u001b[0;36mHEVisium.forward\u001b[0;34m(self, S_tile, M_tile, L_tile, node_features, adj_lists, edge_features)\u001b[0m\n\u001b[1;32m    153\u001b[0m cnn_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_backbone(S_tile, M_tile, L_tile)  \u001b[38;5;66;03m# shape: [N, cnn_out_dim]\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# GAT ÈÉ®ÂàÜÔºåÂü∫Êñº engineered node featuresÔºàÁ∂≠Â∫¶ 14Ôºâ„ÄÅadj_lists Ëàá edge_features\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m gat_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_gat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_features\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N, gat_out_dim]\u001b[39;00m\n\u001b[1;32m    156\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cnn_features, gat_embedding], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    157\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(combined)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 116\u001b[0m, in \u001b[0;36mCustomGAT.forward\u001b[0;34m(self, node_features, adj_list, edge_feat)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# ÂèñÂæóËÆäÊèõÂæåÁöÑÁâπÂæµÔºöquery Ëàá key ÂàÜÂà• [head_dim, 1]\u001b[39;00m\n\u001b[1;32m    115\u001b[0m query \u001b[38;5;241m=\u001b[39m transformed[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m key   \u001b[38;5;241m=\u001b[39m \u001b[43mtransformed\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    117\u001b[0m e_ij \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma1[h]\u001b[38;5;241m.\u001b[39mT, query) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma2[h]\u001b[38;5;241m.\u001b[39mT, key)\n\u001b[1;32m    118\u001b[0m e_ij \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleakyrelu(e_ij)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1292 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# üîß Ë®≠ÂÆöË£ùÁΩÆ\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "# üîß ÂàùÂßãÂåñÊ®°Âûã & ÂÑ™ÂåñÂô®\n",
    "# üîß ÂàùÂßãÂåñÊ®°Âûã & ÂÑ™ÂåñÂô®\n",
    "cnn_backbone = MultiScaleCNN()  # ÈÄôÊòØ‰Ω†ÂÆöÁæ©ÁöÑÂ§öÂ∞∫Â∫¶CNN\n",
    "#attention_head = MultiLayerAttentionHead(input_dim=256, output_dim=35)  # ÂÅáË®≠ÁöÑ AttentionHead\n",
    " \n",
    "in_features = 14  # ÂøÖÈ†àÂíå node_features ÁöÑÁ¨¨‰∫åÂÄãÁ∂≠Â∫¶‰∏ÄËá¥\n",
    "\n",
    "    # ÂâµÂª∫ CustomGAT Ê®°ÂûãÔºåÈÄôË£°ÂÅáË®≠‰ªç‰ΩøÁî® 8 ÂÄã headsÔºåÊØèÂÄã head 32 Á∂≠ÔºåÈÇ£È∫ºËº∏Âá∫Á∂≠Â∫¶ÁÇ∫ 8*32 = 256\n",
    "custom_gat = CustomGAT(in_features=in_features, n_heads=8, head_dim=32)\n",
    "    \n",
    "    # ÂãïÊÖãË®àÁÆó cnn_out_dimÔºö\n",
    "dummy_output = cnn_backbone(torch.randn(1,3,32,32), torch.randn(1,3,64,64), torch.randn(1,3,128,128))\n",
    "cnn_out_dim = dummy_output.shape[1]\n",
    "print(\"Calculated CNN output dimension:\", cnn_out_dim)\n",
    "    \n",
    "# È†êË®≠ gat_out_dim = 8*32 = 256\n",
    "gat_out_dim = 256\n",
    "\n",
    "final_out_dim = 35  # È†êÊ∏¨ 35 ÂÄãÁ¥∞ËÉûÈ°ûÂûãÁöÑ abundance\n",
    "\n",
    "model = HEVisium(cnn_backbone=cnn_backbone, custom_gat=custom_gat,\n",
    "                     final_out_dim=final_out_dim,\n",
    "                     cnn_out_dim=cnn_out_dim, gat_out_dim=gat_out_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()  # ÊàñËÄÖ‰ΩøÁî®ÂÖ∂‰ªñÊêçÂ§±ÂáΩÊï∏Ôºå‰æãÂ¶Ç spearman_loss\n",
    "#loss_fn = spearman_loss\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# üîß ÂÑ≤Â≠ò log ÁöÑË®≠ÂÆö\n",
    "log_file = open(\"training_log.csv\", mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# üîß Áî®‰æÜÁï´Âúñ\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "# üîÅ ÈñãÂßãË®ìÁ∑¥\n",
    "num_epochs = 150\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_spearman = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "    # ‚úÖ ÂÑ≤Â≠òÊúÄÂ•ΩÁöÑÊ®°Âûã\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"‚úÖ Saved best model!\")\n",
    "\n",
    "    # ‚úÖ Ë™øÊï¥Â≠∏ÁøíÁéá\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # ‚úÖ ÂØ´ÂÖ• CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch+1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # ‚úÖ Âç∞ epoch ÁµêÊûú\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | œÅ: {val_spearman:.4f} | lr: {lr:.2e}\")\n",
    "\n",
    "    # ‚úÖ Êõ¥Êñ∞ loss list ‰∏¶Áï´Âúñ\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    plot_losses(train_losses, val_losses)\n",
    "\n",
    "    # ‚úÖ Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"‚õî Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# ‚úÖ ÈóúÈñâ log Ê™îÊ°à\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1478ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'abc.DataBatch'>\n",
      "‚úÖ This batch is a Data object!\n",
      "Batch keys: ['L_tile', 'label', 'S_tile', 'ptr', 'M_tile', 'edge_index', 'meta', 'normal_coord', 'batch']\n",
      "S_tile shape: torch.Size([96, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Ê∏¨Ë©¶ train_loader ÊòØÂê¶ËøîÂõûÊ≠£Á¢∫Ê†ºÂºèÁöÑÊï∏Êìö\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch type: {type(batch)}\")\n",
    "    if isinstance(batch, Data):\n",
    "        print(\"‚úÖ This batch is a Data object!\")\n",
    "        print(f\"Batch keys: {batch.keys()}\")\n",
    "        \n",
    "        # ÊâìÂç∞ S_tile ÁöÑ shape ‰æÜÊ™¢Êü•\n",
    "        if 'S_tile' in batch:\n",
    "            print(f\"S_tile shape: {batch['S_tile'].shape}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è S_tile not found in batch!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è This batch is not a Data object\")\n",
    "    break  # Âè™Ê™¢Êü•Á¨¨‰∏ÄÂÄãÊâπÊ¨°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f55274",
   "metadata": {},
   "source": [
    "ÔºÉ# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7220265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/3135847424.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionMLPModelWithCoord(\n",
       "  (encoder_spot): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_subtiles): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_neighbors): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): MLPDecoder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=194, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=35, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== ÈúÄË¶ÅÁöÑ Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "\n",
    "# ===== ËºâÂÖ•Ë®ìÁ∑¥Â•ΩÁöÑÊ®°ÂûãÊ¨äÈáç =====\n",
    "from hevisum_model import HEVisumModel\n",
    "from hevisum_dataset import importDataset\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "model = VisionMLPModelWithCoord().to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77d22647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/1343659088.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"test_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "# Ê≠£Á¢∫ÊñπÂºè\n",
    "test_data = torch.load(\"test_dataset.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "# ====== Step 2: Âª∫Á´ã Dataset ======\n",
    "test_dataset = importDataset(\n",
    "    center_tile=test_data['tiles'],\n",
    "    subtiles=test_data['subtiles'],\n",
    "    neighbor_tiles=test_data['neighbor_tiles'],\n",
    "    label=np.zeros((len(test_data['tiles']), 35)),  # dummy label\n",
    "    meta=normalized_coords\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea4d8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "test_preds, test_meta = predict(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ddbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.25503415, 0.13337179, 0.1524582 , ..., 0.04139816, 0.01808124,\n",
       "        0.03039141],\n",
       "       ...,\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.23385419, 0.13244098, 0.13941698, ..., 0.04153137, 0.01910294,\n",
       "        0.03005139]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f26de3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== ËÆÄÂèñ test spot index Áî®ÊñºÂ∞çÊáâ ID ====\n",
    "with h5py.File(\"./elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "\n",
    "ensemble_df = pd.DataFrame(test_preds, columns=[f\"C{i+1}\" for i in range(test_preds.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"‚úÖ Saved submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
