{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6db04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b70981c-db23-4ea7-bd9c-dca6279d7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13fdff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/2116005886.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_image_data = torch.load(\"../SML_train_dataset.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/2116005886.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(\"../train_graph_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™\n",
    "train_image_data = torch.load(\"../SML_train_dataset.pt\")\n",
    "graph_data = torch.load(\"../train_graph_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db02b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class importDataset(Dataset):\n",
    "    def __init__(self, S_tiles, M_tiles, L_tiles, labels, meta_info, normal_coords, node_feats, adj_lists,edge_feats):\n",
    "        \"\"\"\n",
    "        Custom Dataset to load image tiles, labels, and edge indices for training.\n",
    "        \n",
    "        Args:\n",
    "            S_tiles (list): Small scale image tiles (spot-level features).\n",
    "            M_tiles (list): Medium scale image tiles (spot-level features).\n",
    "            L_tiles (list): Large scale image tiles (spot-level features).\n",
    "            labels (list): Ground truth labels (spot-level cell type compositions).\n",
    "            meta_info (list): Metadata containing slide_id, x, y coordinates for each spot.\n",
    "            normal_coords (list): Normalized coordinates for each spot.\n",
    "            slide_edge_indices (dict): Dictionary containing the slide-level edge indices for each slide.\n",
    "        \"\"\"\n",
    "        self.S_tiles = S_tiles\n",
    "        self.M_tiles = M_tiles\n",
    "        self.L_tiles = L_tiles\n",
    "        self.labels = labels\n",
    "        self.meta_info = meta_info\n",
    "        self.normal_coords = normal_coords\n",
    "        self.node_feats = node_feats\n",
    "        self.adj_lists = adj_lists\n",
    "        self.edge_feats = edge_feats\n",
    "        # Optional: Apply any additional processing to the data here\n",
    "        # e.g., scaling, normalization, etc.\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples (spots) in the dataset\n",
    "        return len(self.S_tiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the spot-level data for the sample at index `idx`\n",
    "        S_tile = self.S_tiles[idx]\n",
    "        M_tile = self.M_tiles[idx]\n",
    "        L_tile = self.L_tiles[idx]\n",
    "        label = self.labels[idx]\n",
    "        meta = self.meta_info[idx]  # This is a tuple (slide_id, x, y)\n",
    "        normal_coord = self.normal_coords[idx]\n",
    "        adj_list = self.adj_lists[idx]\n",
    "        \n",
    "        # Convert the tiles to the correct format (channels-first)\n",
    "        S_tile = torch.tensor(S_tile, dtype=torch.float).permute(2, 0, 1)  # Convert from (H, W, 3) to (3, H, W)\n",
    "        M_tile = torch.tensor(M_tile, dtype=torch.float).permute(2, 0, 1)\n",
    "        L_tile = torch.tensor(L_tile, dtype=torch.float).permute(2, 0, 1)\n",
    "        \n",
    "        # Convert label and normal_coord to tensors\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        normal_coord = torch.tensor(normal_coord, dtype=torch.float)\n",
    "        # â• Graph features\n",
    "        max_neighbors = 7  # or whatever k you use\n",
    "        adj_list = self.adj_lists[idx] if self.adj_lists is not None else []\n",
    "\n",
    "        # è½‰æˆå›ºå®šé•·åº¦çš„ Tensor\n",
    "        adj_array = np.zeros((max_neighbors, 2))\n",
    "        for i, (j, w) in enumerate(adj_list[:max_neighbors]):\n",
    "            adj_array[i] = [j, w]\n",
    "         # ğŸ› ï¸ edge_feat è½‰å‹è™•ç†\n",
    "        edge_feat_i = self.edge_feats[idx]\n",
    "        if isinstance(edge_feat_i, np.ndarray) and edge_feat_i.dtype == object:\n",
    "            edge_feat_i = np.array(edge_feat_i.tolist())\n",
    "        elif isinstance(edge_feat_i, list):\n",
    "            edge_feat_i = np.array(edge_feat_i)\n",
    "\n",
    "\n",
    "        graph_feats = {\n",
    "            'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float) if self.node_feats is not None else None,\n",
    "            'adj_list': torch.tensor(adj_array, dtype=torch.float),\n",
    "            'edge_feat': torch.tensor(edge_feat_i, dtype=torch.float) if edge_feat_i is not None else None,\n",
    "        }\n",
    "        # Return the data in a PyTorch-friendly format as a dictionary\n",
    "        return {\n",
    "            'S_tile': S_tile,\n",
    "            'M_tile': M_tile,\n",
    "            'L_tile': L_tile,\n",
    "            'label': label,\n",
    "            'meta': meta,\n",
    "            'normal_coord': normal_coord,\n",
    "            **graph_feats  # â• åˆä½µé€² batch dictionary\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "401ef8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 2: å»ºç«‹ Dataset ======\n",
    "image_train_dataset = importDataset(\n",
    "    S_tiles=train_image_data['S_tiles'],\n",
    "    M_tiles=train_image_data['M_tiles'],\n",
    "    L_tiles=train_image_data['L_tiles'],\n",
    "    labels=train_image_data['labels'],\n",
    "    meta_info=train_image_data['meta_info'],\n",
    "    normal_coords=train_image_data['normal_coords'],\n",
    "    node_feats=graph_data['node_feats'],\n",
    "    adj_lists=graph_data['adj_lists'],\n",
    "    edge_feats=graph_data['edge_feats']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e8a237d-3fff-47d5-831c-0b915cbecb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking dataset sample: 0\n",
      "ğŸ“ Center tile shape: torch.Size([3, 32, 32]) | dtype: torch.float32 | min: 0.451, max: 1.000\n",
      "ğŸ“ Subtiles shape: torch.Size([3, 64, 64]) | dtype: torch.float32\n",
      "ğŸ“ Neighbor tiles shape: torch.Size([3, 128, 128]) | dtype: torch.float32\n",
      "ğŸ§¬ Label shape: torch.Size([35]) | dtype: torch.float32\n",
      "ğŸ“Œ Coordinates (meta): x = 1554, y = 1297\n",
      "ğŸ“Š Node feature shape: torch.Size([14]) | dtype: torch.float32\n",
      "ğŸ”— Edge feature shape: torch.Size([7, 5]) | dtype: torch.float32\n",
      "ğŸ”— Sample edge_feat[0]: tensor([26.0000, 26.0000,  0.0000,  0.0000,  0.0385])\n",
      "ğŸ“ Adjacency list shape: torch.Size([7, 2]) | dtype: torch.float32\n",
      "ğŸ“ Sample: tensor([[6.2800e+02, 3.8462e-02],\n",
      "        [2.3700e+02, 3.8462e-02],\n",
      "        [1.3880e+03, 3.7851e-02]])\n",
      "âœ… All checks passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/3814644018.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float) if self.node_feats is not None else None,\n"
     ]
    }
   ],
   "source": [
    "def check_dataset_item(dataset, idx=0):\n",
    "    item = dataset[idx]\n",
    "\n",
    "    print(\"ğŸ” Checking dataset sample:\", idx)\n",
    "\n",
    "    # --- Center Tile ---\n",
    "    tile = item['S_tile']\n",
    "    print(f\"ğŸ“ Center tile shape: {tile.shape} | dtype: {tile.dtype} | min: {tile.min():.3f}, max: {tile.max():.3f}\")\n",
    "\n",
    "    # --- Subtiles ---\n",
    "    subtiles = item['M_tile']\n",
    "    print(f\"ğŸ“ Subtiles shape: {subtiles.shape} | dtype: {subtiles.dtype}\")\n",
    "\n",
    "    # --- Neighbors ---\n",
    "    neighbors = item['L_tile']\n",
    "    print(f\"ğŸ“ Neighbor tiles shape: {neighbors.shape} | dtype: {neighbors.dtype}\")\n",
    "\n",
    "    # --- Label ---\n",
    "    label = item['label']\n",
    "    print(f\"ğŸ§¬ Label shape: {label.shape} | dtype: {label.dtype}\")\n",
    "    assert label.shape[0] == 35 and label.dtype == torch.float32, \"âŒ Label æ‡‰ç‚º float32 ä¸”é•·åº¦ç‚º 35\"\n",
    "\n",
    "    # --- Coordinates (meta) ---\n",
    "    coordinates = item['meta']\n",
    "    print(f\"ğŸ“Œ Coordinates (meta): x = {coordinates[1]}, y = {coordinates[2]}\")\n",
    "\n",
    "    # --- Node Features ---\n",
    "    if 'node_feat' in item and item['node_feat'] is not None:\n",
    "        node_feat = item['node_feat']\n",
    "        print(f\"ğŸ“Š Node feature shape: {node_feat.shape} | dtype: {node_feat.dtype}\")\n",
    "\n",
    "\n",
    "    # --- Edge Features ---\n",
    "    if 'edge_feat' in item and item['edge_feat'] is not None:\n",
    "        edge_feat = item['edge_feat']\n",
    "        print(f\"ğŸ”— Edge feature shape: {edge_feat.shape} | dtype: {edge_feat.dtype}\")\n",
    "        print(f\"ğŸ”— Sample edge_feat[0]: {edge_feat[0]}\")\n",
    "        assert edge_feat.ndim == 2 and edge_feat.shape[1] == 5, \"âŒ Edge feature æ‡‰ç‚º (k, 5)\"\n",
    "    else:\n",
    "        print(\"âš ï¸ Edge features æœªæä¾›\")\n",
    "\n",
    "    # --- Adjacency List ---\n",
    "    if 'adj_list' in item and item['adj_list'] is not None:\n",
    "        adj_list = item['adj_list']\n",
    "        print(f\"ğŸ“ Adjacency list shape: {adj_list.shape} | dtype: {adj_list.dtype}\")\n",
    "        print(f\"ğŸ“ Sample: {adj_list[:3]}\")\n",
    "        assert adj_list.ndim == 2 and adj_list.shape[1] == 2, \"âŒ Adjacency list æ‡‰ç‚º (k, 2)\"\n",
    "    else:\n",
    "        print(\"âš ï¸ Adjacency list æœªæä¾›\")\n",
    "\n",
    "    print(\"âœ… All checks passed!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "check_dataset_item(image_train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15c83a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train set size: 6679 samples\n",
      "âœ… Validation set size: 1670 samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "# ç¢ºå®šæ•¸æ“šé›†çš„ç¸½å¤§å°ï¼ˆå…©è€…å¿…é ˆç›¸åŒï¼‰\n",
    "dataset_size = len(image_train_dataset)  # åŒæ™‚ image_train_dataset å’Œ graph_train_dataset é•·åº¦æ‡‰ç›¸åŒ\n",
    "\n",
    "# è¨­å®šè¨“ç·´æ¯”ä¾‹ï¼Œé€™è£¡ä»¥ 80% ç‚ºè¨“ç·´é›†ï¼Œ20% ç‚ºé©—è­‰é›†\n",
    "train_ratio = 0.8\n",
    "split_index = int(np.floor(train_ratio * dataset_size))\n",
    "\n",
    "# ç”Ÿæˆéš¨æ©Ÿç´¢å¼•\n",
    "indices = torch.randperm(dataset_size).tolist()\n",
    "\n",
    "# å°‡ç´¢å¼•æ‹†åˆ†ç‚ºè¨“ç·´å’Œé©—è­‰éƒ¨åˆ†\n",
    "train_indices = indices[:split_index]\n",
    "val_indices = indices[split_index:]\n",
    "\n",
    "# ä½¿ç”¨ Subset æ ¹æ“šç›¸åŒçš„ç´¢å¼•å»ºç«‹å­é›†\n",
    "image_train_subset = Subset(image_train_dataset, train_indices)\n",
    "image_val_subset = Subset(image_train_dataset, val_indices)\n",
    "\n",
    "# ç¾åœ¨ï¼Œä½ æœ‰å…©å° DataLoaderï¼Œä¸”å®ƒå€‘çš„ç´¢å¼•æ˜¯ä¸€ä¸€å°æ‡‰çš„\n",
    "print(f\"âœ… Train set size: {len(image_train_subset)} samples\")\n",
    "print(f\"âœ… Validation set size: {len(image_val_subset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46d21906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image sample keys: dict_keys(['S_tile', 'M_tile', 'L_tile', 'label', 'meta', 'normal_coord', 'node_feat', 'adj_list', 'edge_feat'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/3814644018.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float) if self.node_feats is not None else None,\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(image_train_subset, batch_size=1, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(image_val_subset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ä¸€ä¸‹ DataLoader å–å‡ºçš„æ•¸æ“š\n",
    "for batch in train_loader:\n",
    "\n",
    "    print(\"Image sample keys:\", batch.keys() if hasattr(batch, 'keys') else \"Not a dict\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccdc3b",
   "metadata": {},
   "source": [
    "Poteintial issues:\n",
    "# 1. my val_set tiles image may be included in the sub_tiles of train_set\n",
    "\n",
    "Note: Since neighbor tiles are reused across samples, some mild information overlap may exist between train and val sets. However, final test set is completely held out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45327c2e",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f90c9b",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GraphSAGE, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class MultiLayerAttentionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers=1):\n",
    "        super(MultiLayerAttentionHead, self).__init__()\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim=input_dim, num_heads=1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = x\n",
    "        for attn_layer in self.attention_layers:\n",
    "            attn_output, _ = attn_layer(attn_output, attn_output, attn_output)  # Self-attention on the feature vector\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "\n",
    "# Define a CNN for each scale (S_tile, M_tile, L_tile)\n",
    "class SmallScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "class MediumScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MediumScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "class LargeScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        # GraphSAGE layer\n",
    "        self.graphsage = GraphSAGE(input_dim, hidden_dim, aggr='mean')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # For predicting 35 cell types\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Assuming `data` contains the features and edge_index for the graph\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.graphsage(x, edge_index)  # Apply GraphSAGE to the features\n",
    "        x = global_mean_pool(x, data.batch)  # Global pooling for each graph (slide)\n",
    "        return self.fc(x)  # Predict the cell type composition\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=1)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention mechanism\n",
    "        attn_output, _ = self.attention(x, x, x)  # Self-attention on the feature vector\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "\n",
    "class CellTypePredictionModel(nn.Module):\n",
    "    def __init__(self, small_cnn, medium_cnn, large_cnn, graphsage_model, attention_head):\n",
    "        super(CellTypePredictionModel, self).__init__()\n",
    "        self.small_cnn = small_cnn  # CNN for small-scale patch\n",
    "        self.medium_cnn = medium_cnn  # CNN for medium-scale patch\n",
    "        self.large_cnn = large_cnn  # CNN for large-scale patch\n",
    "        self.graphsage_model = graphsage_model  # GraphSAGE for graph-based feature learning\n",
    "        self.attention_head = attention_head  # Attention mechanism to focus on important features\n",
    "\n",
    "    def forward(self, S_tile, M_tile, L_tile, edge_index, normal_coords):\n",
    "        # Step 1: CNN feature extraction for each scale (S_tile, M_tile, L_tile)\n",
    "        S_features = self.small_cnn(S_tile)\n",
    "        M_features = self.medium_cnn(M_tile)\n",
    "        L_features = self.large_cnn(L_tile)\n",
    "        \n",
    "        # Combine features from all scales into one feature vector (F_i)\n",
    "        combined_features = torch.cat([S_features, M_features, L_features], dim=1)\n",
    "        \n",
    "        # Step 2: Create PyG Data object for Graph Neural Network\n",
    "        data = Data(x=combined_features, edge_index=edge_index, pos=normal_coords)\n",
    "        \n",
    "        # Step 3: GraphSAGE layer to learn spatial context-aware features\n",
    "        graphsage_output = self.graphsage_model(data)\n",
    "        \n",
    "        # Step 4: Attention mechanism to focus on the relevant features for cell-type prediction\n",
    "        attention_output = self.attention_head(graphsage_output.unsqueeze(0))  # Add batch dimension\n",
    "        return attention_output.squeeze(0)  # Remove batch dimension for final output\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the individual CNNs for each scale\n",
    "small_cnn = SmallScaleCNN()\n",
    "medium_cnn = MediumScaleCNN()\n",
    "large_cnn = LargeScaleCNN()\n",
    "\n",
    "# Instantiate the GraphSAGE model and attention head\n",
    "graphsage_model = GraphSAGEModel(input_dim=16 + 32 + 64, hidden_dim=256, output_dim=256)\n",
    "attention_head = AttentionHead(input_dim=256, output_dim=35)  # Predict 35 cell types\n",
    "\n",
    "# Instantiate the final model\n",
    "model = CellTypePredictionModel(small_cnn, medium_cnn, large_cnn, graphsage_model, attention_head)\n",
    "\n",
    "# Example input: Assuming you have a batch of data\n",
    "S_tile = torch.rand(32, 3, 32, 32)  # Example batch of small-scale image tiles\n",
    "M_tile = torch.rand(32, 3, 64, 64)  # Example batch of medium-scale image tiles\n",
    "L_tile = torch.rand(32, 3, 128, 128)  # Example batch of large-scale image tiles\n",
    "meta_info = [(f\"S_{i}\", torch.rand(2)) for i in range(32)]  # Dummy metadata for batch\n",
    "edge_index = torch.randint(0, 32, (2, 50))  # Dummy graph edge indices for each slide\n",
    "\n",
    "# Forward pass\n",
    "output = model(S_tile, M_tile, L_tile, meta_info, edge_index)\n",
    "print(\"Output shape:\", output.shape)  # Should be (batch_size, 35) for the 35 cell types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d68d1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "# --- MultiScaleCNN å®šç¾© ---\n",
    "class MultiScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiScaleCNN, self).__init__()\n",
    "        # S_branch: Input: [3,32,32] â†’ Output: [16,8,8]\n",
    "        self.conv_s = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),     # [16, 32, 32]\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [16, 16, 16]\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),     # [16, 16, 16]\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)            # [16, 8, 8]\n",
    "        )\n",
    "        \n",
    "        # M_branch: Input: [3,64,64] â†’ Output: [32,8,8]\n",
    "        self.conv_m = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),      # [32, 64, 64]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [32, 32, 32]\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),     # [32, 32, 32]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [32, 16, 16]\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),     # [32, 16, 16]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)            # [32, 8, 8]\n",
    "        )\n",
    "        \n",
    "        # L_branch: Input: [3,128,128] â†’ Output: [64,8,8]\n",
    "        self.conv_l = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),      # [64, 128, 128]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [64, 64, 64]\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),     # [64, 64, 64]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [64, 32, 32]\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),     # [64, 32, 32]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),           # [64, 16, 16]\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),     # [64, 16, 16]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)            # [64, 8, 8]\n",
    "        )\n",
    "    def forward(self, S_tile, M_tile, L_tile):\n",
    "        # S_branch: flatten [batch, 16, 8, 8] â†’ [batch, 16*8*8] = [batch, 1024]\n",
    "        f_s = self.conv_s(S_tile).view(S_tile.size(0), -1)\n",
    "        # M_branch: flatten [batch, 32, 8, 8] â†’ [batch, 32*8*8] = [batch, 2048]\n",
    "        f_m = self.conv_m(M_tile).view(M_tile.size(0), -1)\n",
    "        # L_branch: flatten [batch, 64, 8, 8] â†’ [batch, 64*8*8] = [batch, 4096]\n",
    "        f_l = self.conv_l(L_tile).view(L_tile.size(0), -1)\n",
    "        # æ‹¼æ¥ä¸‰å€‹å°ºåº¦çš„ç‰¹å¾µ\n",
    "        return torch.cat([f_s, f_m, f_l], dim=1)\n",
    "    \n",
    "# --- CustomGAT å®šç¾© ---\n",
    "class CustomGAT(nn.Module):\n",
    "    def __init__(self, in_features, n_heads=8, head_dim=32):\n",
    "        \"\"\"\n",
    "        in_features: æ¯å€‹ç¯€é»åˆå§‹ç‰¹å¾µç¶­åº¦ï¼ˆé€™è£¡æ‡‰ç‚º 14ï¼‰\n",
    "        n_heads: æ³¨æ„åŠ›é ­æ•¸ï¼ˆä¾‹å¦‚ 8ï¼‰\n",
    "        head_dim: æ¯å€‹æ³¨æ„åŠ›é ­è¼¸å‡ºç¶­åº¦ï¼ˆä¾‹å¦‚ 32ï¼‰\n",
    "        \"\"\"\n",
    "        super(CustomGAT, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        # å°æ¯å€‹æ³¨æ„åŠ›é ­éƒ½æœ‰ä¸€å€‹ç·šæ€§è®Šæ›çŸ©é™£ï¼Œè¼¸å‡º shape: [B, head_dim]\n",
    "        self.W = nn.Parameter(torch.randn(n_heads, in_features, head_dim))\n",
    "        # æ³¨æ„åŠ›è¨ˆç®—åƒæ•¸ï¼Œåˆ†åˆ¥å° query å’Œ key ä½œç”¨\n",
    "        self.a1 = nn.Parameter(torch.randn(n_heads, head_dim, 1))\n",
    "        self.a2 = nn.Parameter(torch.randn(n_heads, head_dim, 1))\n",
    "        self.leakyrelu = nn.LeakyReLU(0.01)\n",
    "    \n",
    "    def forward(self, node_features, adj_list, edge_feat):\n",
    "        \"\"\"\n",
    "        node_features: tensor, shape [B, in_features] ï¼›B = batch sizeï¼ˆä»£è¡¨ B å€‹ spotsï¼‰\n",
    "        adj_list: tensor, shape [B, num_neighbors, 2]\n",
    "                  å‡è¨­æ¯å€‹å…ƒç´ çš„ç¬¬ä¸€å€‹å€¼æ˜¯é„°å±…ç´¢å¼•ï¼ˆå¯èƒ½ç‚º floatï¼‰ï¼Œç¬¬äºŒå€‹å€¼ç‚ºæ¬Šé‡ï¼ˆå¯å¿½ç•¥ï¼‰\n",
    "        edge_feat: tensor, shape [B, num_neighbors, edge_feat_dim]ï¼›ä¾‹å¦‚ edge_feat_dim = 5\n",
    "        \"\"\"\n",
    "        B = node_features.size(0)\n",
    "        num_neighbors = adj_list.size(1)\n",
    "        head_outputs = []\n",
    "        \n",
    "        for h in range(self.n_heads):\n",
    "            # ç·šæ€§è®Šæ›ï¼šå°æ¯å€‹ç¯€é»åšè®Šæ›\n",
    "            # transformed: [B, head_dim]\n",
    "            transformed = torch.matmul(node_features, self.W[h])\n",
    "            # åˆå§‹åŒ–æ³¨æ„åŠ›å¾—åˆ†çŸ©é™£ eï¼Œå½¢ç‹€ [B, B]ï¼Œä½†æˆ‘å€‘åªå°æ¯å€‹ node i èˆ‡å…¶é„°å±… j é€²è¡Œè¨ˆç®—\n",
    "            # é€™è£¡æˆ‘å€‘ç”¨ -inf å¡«å……éé„°å±…ä½ç½®\n",
    "            e = torch.full((B, B), float('-inf'), device=node_features.device)\n",
    "            \n",
    "            # å°æ¯å€‹ç¯€é» i\n",
    "            for i in range(B):\n",
    "                # å–å‡ºè©² node çš„é„°å±…è³‡è¨Šï¼Œå½¢ç‹€ [num_neighbors, 2]\n",
    "                # æˆ‘å€‘éœ€è¦ç²å¾—é„°å±…ç´¢å¼•ï¼Œä¸¦ç¢ºä¿ç‚ºæ•´æ•¸\n",
    "                neighbors = adj_list[i, :, 0]  # shape: [num_neighbors]\n",
    "                # å¦‚æœ neighbors æ˜¯æµ®é»å‹ï¼Œå°‡å…¶è½‰æ›ç‚º int\n",
    "                neighbors = neighbors.long()  # å¦‚æœåŸå§‹å­˜çš„æ˜¯ floatï¼Œå¯ç›´æ¥ç”¨ .long()\n",
    "                for k in range(num_neighbors):\n",
    "                    j = int(neighbors[k].item())  # j ç‚ºé„°å±…ç´¢å¼•\n",
    "                    # å–å¾—è®Šæ›å¾Œçš„ç‰¹å¾µï¼šquery èˆ‡ key åˆ†åˆ¥ [head_dim, 1]\n",
    "                    query = transformed[i].unsqueeze(1)\n",
    "                    key   = transformed[j].unsqueeze(1)\n",
    "                    e_ij = torch.matmul(self.a1[h].T, query) + torch.matmul(self.a2[h].T, key)\n",
    "                    e_ij = self.leakyrelu(e_ij)\n",
    "                    # å¦‚æœæä¾›äº† edge_featï¼Œæ ¹æ“šé‚Šç‰¹å¾µèª¿æ•´ e_ij\n",
    "                    if edge_feat is not None:\n",
    "                        # å–å‡ºå°æ‡‰é‚Šçš„ç‰¹å¾µï¼Œå½¢ç‹€: [edge_feat_dim]\n",
    "                        current_edge_feat = edge_feat[i, k]  # tensor, shape: [edge_feat_dim]\n",
    "                        # é€™è£¡ç°¡å–®ä½¿ç”¨ç¬¬ä¸€å€‹å€¼ä½œç‚ºèª¿æ•´å› å­\n",
    "                        e_ij *= 1.0 / (current_edge_feat[0] + 1e-6)\n",
    "                    # å°‡è¨ˆç®—å¥½çš„ e_ij å­˜å…¥å°æ‡‰çš„ä½ç½® [i, j]\n",
    "                    e[i, j] = e_ij.squeeze()\n",
    "            \n",
    "            # å°æ¯å€‹ç¯€é»çš„é„°å±…æ³¨æ„åŠ›å¾—åˆ†é€²è¡Œ softmax\n",
    "            attention = F.softmax(e, dim=1)  # [B, B]\n",
    "            # èšåˆé„°å±…çš„è¨Šæ¯ï¼šå°æ¯å€‹ç¯€é» iï¼Œç”¨é„°å±… j çš„è®Šæ›å¾Œç‰¹å¾µåŠ æ¬Šæ±‚å’Œ\n",
    "            head_output = torch.zeros(B, self.head_dim, device=node_features.device)\n",
    "            for i in range(B):\n",
    "                weighted_sum = torch.zeros(self.head_dim, device=node_features.device)\n",
    "                for j in range(B):\n",
    "                    if attention[i, j] > 0:\n",
    "                        weighted_sum += attention[i, j] * transformed[j]\n",
    "                head_output[i] = F.relu(weighted_sum)\n",
    "            head_outputs.append(head_output)\n",
    "        \n",
    "        # å¤šé ­æ‹¼æ¥ï¼šæœ€çµ‚è¼¸å‡º shape [B, n_heads*head_dim]\n",
    "        multi_head_output = torch.cat(head_outputs, dim=1)\n",
    "        return multi_head_output\n",
    "# --- HEVisium æ¨¡å‹ ---\n",
    "class HEVisium(nn.Module):\n",
    "    def __init__(self, cnn_backbone, custom_gat, final_out_dim, cnn_out_dim, gat_out_dim):\n",
    "        super(HEVisium, self).__init__()\n",
    "        self.cnn_backbone = cnn_backbone  # ç”¨æ–¼åœ–åƒç‰¹å¾µæå–\n",
    "        self.custom_gat = custom_gat      # ç”¨æ–¼åœ–çµæ§‹ç‰¹å¾µæå–\n",
    "        self.fc = nn.Linear(cnn_out_dim + gat_out_dim, final_out_dim)\n",
    "    \n",
    "    def forward(self, S_tile, M_tile, L_tile, node_features, adj_lists, edge_features):\n",
    "        # CNN éƒ¨åˆ†ï¼Œæå– image-based ç‰¹å¾µ\n",
    "        cnn_features = self.cnn_backbone(S_tile, M_tile, L_tile)  # shape: [N, cnn_out_dim]\n",
    "        # GAT éƒ¨åˆ†ï¼ŒåŸºæ–¼ engineered node featuresï¼ˆç¶­åº¦ 14ï¼‰ã€adj_lists èˆ‡ edge_features\n",
    "        gat_embedding = self.custom_gat(node_features, adj_lists, edge_features)  # [N, gat_out_dim]\n",
    "        combined = torch.cat([cnn_features, gat_embedding], dim=1)\n",
    "        out = self.fc(combined)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8eb6c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader['adj_list'] shape: torch.Size([1, 7, 2])\n",
      "train_loader['node_feat'] shape: torch.Size([1, 14])\n",
      "train_loader['edge_feat'] shape: torch.Size([1, 7, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/3814644018.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float) if self.node_feats is not None else None,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch in train_loader:\n",
    "    \n",
    "    print(\"train_loader['adj_list'] shape:\", batch['adj_list'].shape)\n",
    "    print(\"train_loader['node_feat'] shape:\", batch['node_feat'].shape)\n",
    "    print(\"train_loader['edge_feat'] shape:\", batch['edge_feat'].shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16e85ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 15, 5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- ä½¿ç”¨ç¤ºä¾‹ ---\n",
    "if __name__ == \"__main__\":\n",
    "    # å‡è¨­æˆ‘å€‘æœ‰ä»¥ä¸‹æ•¸æ“šï¼Œå°æ‡‰ spot ç´šåˆ¥\n",
    "    N = 100  # 100 spots\n",
    "    # åœ–åƒ patch æ•¸æ“šï¼ˆä¾†è‡ª CNN éƒ¨åˆ†ï¼‰ï¼Œæ¯å€‹å°ºå¯¸åˆ†åˆ¥å¦‚ä¸‹ï¼š\n",
    "    S_tile = torch.randn(N, 3, 32, 32)\n",
    "    M_tile = torch.randn(N, 3, 64, 64)\n",
    "    L_tile = torch.randn(N, 3, 128, 128)\n",
    "    # engineered node featuresï¼šå‡è¨­æ¯å€‹ spot ç‚º 256 ç¶­\n",
    "    node_features = torch.randn(N, 14)\n",
    "    # é‚Šä¿¡æ¯ï¼šadjacency_lists ç‚ºæ¯å€‹ spot çš„é„°æ¥åˆ—è¡¨ï¼Œé€™è£¡æ¨¡æ“¬\n",
    "    adj_lists = [[(j, 1.0) for j in range(N) if j != i][:15] for i in range(N)]\n",
    "    # é‚Šç‰¹å¾µï¼šå‡è¨­æ¯æ¢é‚Šç”¨ 5 ç¶­ç‰¹å¾µè¡¨ç¤º\n",
    "    edge_features = [[np.random.rand(5) for _ in range(15)] for i in range(N)]\n",
    "    \n",
    "    final_out_dim = 35  # é æ¸¬ 35 å€‹ç´°èƒé¡å‹çš„ abundance\n",
    "edge_features = np.array(edge_features)\n",
    "edge_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9d3692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ğŸ§  è¨“ç·´ä¸€å€‹ epoch\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        \n",
    "        S_tile, M_tile, L_tile = batch['S_tile'].to(device), batch['M_tile'].to(device), batch['L_tile'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        node_feat = batch['node_feat'].to(device)\n",
    "        adj_list = batch['adj_list'].to(device)\n",
    "        edge_feat = batch['edge_feat'].to(device)\n",
    "        # å„ªåŒ–å™¨æ¸…ç©ºæ¢¯åº¦\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # å‰å‘å‚³æ’­ï¼šè¨ˆç®—é æ¸¬çµæœ\n",
    "        out = model(S_tile, M_tile, L_tile, node_feat, adj_list, edge_feat)\n",
    "\n",
    "        # è¨ˆç®—æå¤±\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()  # åå‘å‚³æ’­\n",
    "        optimizer.step()  # æ›´æ–°æ¨¡å‹åƒæ•¸\n",
    "\n",
    "        total_loss += loss.item() * S_tile.size(0)\n",
    "        avg_loss = total_loss / ((pbar.n + 1) * dataloader.batch_size)\n",
    "        pbar.set_postfix(loss=loss.item(), avg=avg_loss)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "# ğŸ“ é©—è­‰æ¨¡å‹\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # ç²å–é©—è­‰æ•¸æ“š\n",
    "            S_tile, M_tile, L_tile = batch['S_tile'].to(device), batch['M_tile'].to(device), batch['L_tile'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            node_feat = batch['node_feat'].to(device)\n",
    "            adj_list = batch['adj_list'].to(device)\n",
    "            edge_feat = batch['edge_feat'].to(device)\n",
    "            # å„ªåŒ–å™¨æ¸…ç©ºæ¢¯åº¦\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # å‰å‘å‚³æ’­ï¼šè¨ˆç®—é æ¸¬çµæœ\n",
    "            out = model(S_tile, M_tile, L_tile, node_feat, adj_list, edge_feat)\n",
    "\n",
    "\n",
    "            total_loss += loss.item() * S_tile.size(0)\n",
    "            preds.append(out.cpu())\n",
    "            targets.append(label.cpu())\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    targets = torch.cat(targets).numpy()\n",
    "\n",
    "    # ä½¿ç”¨ Spearman ç›¸é—œæ€§ä¾†è¨ˆç®—é æ¸¬çš„æº–ç¢ºæ€§\n",
    "    scores = [spearmanr(preds[:, i], targets[:, i])[0] for i in range(preds.shape[1])]\n",
    "    spearman_avg = np.nanmean(scores)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), spearman_avg\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Spearman correlation for each gene\n",
    "    scores = [spearmanr(preds[:, i], targets[:, i])[0] for i in range(preds.shape[1])]\n",
    "    spearman_avg = np.nanmean(scores)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), spearman_avg\n",
    "\n",
    "# ğŸ”® é æ¸¬\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_meta = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # ç²å–æ¸¬è©¦æ•¸æ“š\n",
    "            S_tile, M_tile, L_tile = batch['S_tile'].to(device), batch['M_tile'].to(device), batch['L_tile'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            node_feat = batch['node_feat'].to(device)\n",
    "            adj_list = batch['adj_list'].to(device)\n",
    "            edge_feat = batch['edge_feat'].to(device)\n",
    "            # å„ªåŒ–å™¨æ¸…ç©ºæ¢¯åº¦\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # å‰å‘å‚³æ’­ï¼šè¨ˆç®—é æ¸¬çµæœ\n",
    "            out = model(S_tile, M_tile, L_tile, node_feat, adj_list, edge_feat)\n",
    "\n",
    "            all_preds.append(out.cpu())\n",
    "            all_meta.extend(batch['meta'])  # ç”¨ä¾†å­˜å„²æ¸¬è©¦é›†çš„metaä¿¡æ¯\n",
    "\n",
    "    return torch.cat(all_preds).numpy(), all_meta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b268",
   "metadata": {},
   "source": [
    "# callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6e2ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None or val_loss < self.best_score:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(val_losses, label=\"Val Loss\", marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.show()\n",
    "# æ”¶é›†è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c9b4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: mps\n",
      "Calculated CNN output dimension: 7168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/6679 [00:00<?, ?it/s]/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_30582/3814644018.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float) if self.node_feats is not None else None,\n",
      "                                                  \r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1292 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 57\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     val_loss, val_spearman \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, loss_fn, device)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# âœ… å„²å­˜æœ€å¥½çš„æ¨¡å‹\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 23\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# å‰å‘å‚³æ’­ï¼šè¨ˆç®—é æ¸¬çµæœ\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS_tile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM_tile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_tile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# è¨ˆç®—æå¤±\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out, label)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 155\u001b[0m, in \u001b[0;36mHEVisium.forward\u001b[0;34m(self, S_tile, M_tile, L_tile, node_features, adj_lists, edge_features)\u001b[0m\n\u001b[1;32m    153\u001b[0m cnn_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_backbone(S_tile, M_tile, L_tile)  \u001b[38;5;66;03m# shape: [N, cnn_out_dim]\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# GAT éƒ¨åˆ†ï¼ŒåŸºæ–¼ engineered node featuresï¼ˆç¶­åº¦ 14ï¼‰ã€adj_lists èˆ‡ edge_features\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m gat_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_gat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_features\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N, gat_out_dim]\u001b[39;00m\n\u001b[1;32m    156\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cnn_features, gat_embedding], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    157\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(combined)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 116\u001b[0m, in \u001b[0;36mCustomGAT.forward\u001b[0;34m(self, node_features, adj_list, edge_feat)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# å–å¾—è®Šæ›å¾Œçš„ç‰¹å¾µï¼šquery èˆ‡ key åˆ†åˆ¥ [head_dim, 1]\u001b[39;00m\n\u001b[1;32m    115\u001b[0m query \u001b[38;5;241m=\u001b[39m transformed[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m key   \u001b[38;5;241m=\u001b[39m \u001b[43mtransformed\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    117\u001b[0m e_ij \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma1[h]\u001b[38;5;241m.\u001b[39mT, query) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma2[h]\u001b[38;5;241m.\u001b[39mT, key)\n\u001b[1;32m    118\u001b[0m e_ij \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleakyrelu(e_ij)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1292 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ğŸ”§ è¨­å®šè£ç½®\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "# ğŸ”§ åˆå§‹åŒ–æ¨¡å‹ & å„ªåŒ–å™¨\n",
    "# ğŸ”§ åˆå§‹åŒ–æ¨¡å‹ & å„ªåŒ–å™¨\n",
    "cnn_backbone = MultiScaleCNN()  # é€™æ˜¯ä½ å®šç¾©çš„å¤šå°ºåº¦CNN\n",
    "#attention_head = MultiLayerAttentionHead(input_dim=256, output_dim=35)  # å‡è¨­çš„ AttentionHead\n",
    " \n",
    "in_features = 14  # å¿…é ˆå’Œ node_features çš„ç¬¬äºŒå€‹ç¶­åº¦ä¸€è‡´\n",
    "\n",
    "    # å‰µå»º CustomGAT æ¨¡å‹ï¼Œé€™è£¡å‡è¨­ä»ä½¿ç”¨ 8 å€‹ headsï¼Œæ¯å€‹ head 32 ç¶­ï¼Œé‚£éº¼è¼¸å‡ºç¶­åº¦ç‚º 8*32 = 256\n",
    "custom_gat = CustomGAT(in_features=in_features, n_heads=8, head_dim=32)\n",
    "    \n",
    "    # å‹•æ…‹è¨ˆç®— cnn_out_dimï¼š\n",
    "dummy_output = cnn_backbone(torch.randn(1,3,32,32), torch.randn(1,3,64,64), torch.randn(1,3,128,128))\n",
    "cnn_out_dim = dummy_output.shape[1]\n",
    "print(\"Calculated CNN output dimension:\", cnn_out_dim)\n",
    "    \n",
    "# é è¨­ gat_out_dim = 8*32 = 256\n",
    "gat_out_dim = 256\n",
    "\n",
    "final_out_dim = 35  # é æ¸¬ 35 å€‹ç´°èƒé¡å‹çš„ abundance\n",
    "\n",
    "model = HEVisium(cnn_backbone=cnn_backbone, custom_gat=custom_gat,\n",
    "                     final_out_dim=final_out_dim,\n",
    "                     cnn_out_dim=cnn_out_dim, gat_out_dim=gat_out_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()  # æˆ–è€…ä½¿ç”¨å…¶ä»–æå¤±å‡½æ•¸ï¼Œä¾‹å¦‚ spearman_loss\n",
    "#loss_fn = spearman_loss\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# ğŸ”§ å„²å­˜ log çš„è¨­å®š\n",
    "log_file = open(\"training_log.csv\", mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# ğŸ”§ ç”¨ä¾†ç•«åœ–\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "# ğŸ” é–‹å§‹è¨“ç·´\n",
    "num_epochs = 150\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_spearman = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "    # âœ… å„²å­˜æœ€å¥½çš„æ¨¡å‹\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"âœ… Saved best model!\")\n",
    "\n",
    "    # âœ… èª¿æ•´å­¸ç¿’ç‡\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # âœ… å¯«å…¥ CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch+1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # âœ… å° epoch çµæœ\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | Ï: {val_spearman:.4f} | lr: {lr:.2e}\")\n",
    "\n",
    "    # âœ… æ›´æ–° loss list ä¸¦ç•«åœ–\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    plot_losses(train_losses, val_losses)\n",
    "\n",
    "    # âœ… Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"â›” Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# âœ… é—œé–‰ log æª”æ¡ˆ\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1478ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'abc.DataBatch'>\n",
      "âœ… This batch is a Data object!\n",
      "Batch keys: ['L_tile', 'label', 'S_tile', 'ptr', 'M_tile', 'edge_index', 'meta', 'normal_coord', 'batch']\n",
      "S_tile shape: torch.Size([96, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# æ¸¬è©¦ train_loader æ˜¯å¦è¿”å›æ­£ç¢ºæ ¼å¼çš„æ•¸æ“š\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch type: {type(batch)}\")\n",
    "    if isinstance(batch, Data):\n",
    "        print(\"âœ… This batch is a Data object!\")\n",
    "        print(f\"Batch keys: {batch.keys()}\")\n",
    "        \n",
    "        # æ‰“å° S_tile çš„ shape ä¾†æª¢æŸ¥\n",
    "        if 'S_tile' in batch:\n",
    "            print(f\"S_tile shape: {batch['S_tile'].shape}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ S_tile not found in batch!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ This batch is not a Data object\")\n",
    "    break  # åªæª¢æŸ¥ç¬¬ä¸€å€‹æ‰¹æ¬¡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f55274",
   "metadata": {},
   "source": [
    "ï¼ƒ# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7220265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/3135847424.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionMLPModelWithCoord(\n",
       "  (encoder_spot): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_subtiles): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_neighbors): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): MLPDecoder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=194, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=35, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== éœ€è¦çš„ Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "\n",
    "# ===== è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹æ¬Šé‡ =====\n",
    "from hevisum_model import HEVisumModel\n",
    "from hevisum_dataset import importDataset\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "model = VisionMLPModelWithCoord().to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77d22647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/1343659088.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"test_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "# æ­£ç¢ºæ–¹å¼\n",
    "test_data = torch.load(\"test_dataset.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "# ====== Step 2: å»ºç«‹ Dataset ======\n",
    "test_dataset = importDataset(\n",
    "    center_tile=test_data['tiles'],\n",
    "    subtiles=test_data['subtiles'],\n",
    "    neighbor_tiles=test_data['neighbor_tiles'],\n",
    "    label=np.zeros((len(test_data['tiles']), 35)),  # dummy label\n",
    "    meta=normalized_coords\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea4d8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "test_preds, test_meta = predict(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ddbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.25503415, 0.13337179, 0.1524582 , ..., 0.04139816, 0.01808124,\n",
       "        0.03039141],\n",
       "       ...,\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.23385419, 0.13244098, 0.13941698, ..., 0.04153137, 0.01910294,\n",
       "        0.03005139]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f26de3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== è®€å– test spot index ç”¨æ–¼å°æ‡‰ ID ====\n",
    "with h5py.File(\"./elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "\n",
    "ensemble_df = pd.DataFrame(test_preds, columns=[f\"C{i+1}\" for i in range(test_preds.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"âœ… Saved submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
