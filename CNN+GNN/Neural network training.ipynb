{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6db04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b70981c-db23-4ea7-bd9c-dca6279d7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93fb8f4b-1df0-484b-8931-3805eb21c5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_99658/1274620153.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_image_data = torch.load(\"../SML_train_dataset.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_99658/1274620153.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_edge_data = torch.load(\"../train_slide_edge_indices.pt\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# ËºâÂÖ•Ë≥áÊñô\n",
    "train_image_data = torch.load(\"../SML_train_dataset.pt\")\n",
    "train_edge_data = torch.load(\"../train_slide_edge_indices.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db02b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class importDataset(Dataset):\n",
    "    def __init__(self, S_tiles, M_tiles, L_tiles, labels, meta_info, normal_coords, slide_edge_indices):\n",
    "        \"\"\"\n",
    "        Custom Dataset to load image tiles, labels, and edge indices for training.\n",
    "        \n",
    "        Args:\n",
    "            S_tiles (list): Small scale image tiles (spot-level features).\n",
    "            M_tiles (list): Medium scale image tiles (spot-level features).\n",
    "            L_tiles (list): Large scale image tiles (spot-level features).\n",
    "            labels (list): Ground truth labels (spot-level cell type compositions).\n",
    "            meta_info (list): Metadata containing slide_id, x, y coordinates for each spot.\n",
    "            normal_coords (list): Normalized coordinates for each spot.\n",
    "            slide_edge_indices (dict): Dictionary containing the slide-level edge indices for each slide.\n",
    "        \"\"\"\n",
    "        self.S_tiles = S_tiles\n",
    "        self.M_tiles = M_tiles\n",
    "        self.L_tiles = L_tiles\n",
    "        self.labels = labels\n",
    "        self.meta_info = meta_info\n",
    "        self.normal_coords = normal_coords\n",
    "        self.slide_edge_indices = slide_edge_indices\n",
    "        \n",
    "        # Optional: Apply any additional processing to the data here\n",
    "        # e.g., scaling, normalization, etc.\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples (spots) in the dataset\n",
    "        return len(self.S_tiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the spot-level data for the sample at index `idx`\n",
    "        S_tile = self.S_tiles[idx]\n",
    "        M_tile = self.M_tiles[idx]\n",
    "        L_tile = self.L_tiles[idx]\n",
    "        label = self.labels[idx]\n",
    "        meta = self.meta_info[idx]  # This is a tuple (slide_id, x, y)\n",
    "        normal_coord = self.normal_coords[idx]\n",
    "        \n",
    "        # Extract slide_id from meta_info (slide_id is at index 0)\n",
    "        slide_id = meta[0]\n",
    "        \n",
    "        # Get the corresponding edge_index for this slide from slide_edge_indices\n",
    "        edge_index = self.slide_edge_indices[slide_id]\n",
    "        \n",
    "        # Convert the tiles to the correct format (channels-first)\n",
    "        S_tile = torch.tensor(S_tile, dtype=torch.float).permute(2, 0, 1)  # Convert from (H, W, 3) to (3, H, W)\n",
    "        M_tile = torch.tensor(M_tile, dtype=torch.float).permute(2, 0, 1)\n",
    "        L_tile = torch.tensor(L_tile, dtype=torch.float).permute(2, 0, 1)\n",
    "        \n",
    "        # Convert label and normal_coord to tensors\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        normal_coord = torch.tensor(normal_coord, dtype=torch.float)\n",
    "        \n",
    "        # Return the data in a PyTorch-friendly format as a dictionary\n",
    "        return {\n",
    "            'S_tile': S_tile,\n",
    "            'M_tile': M_tile,\n",
    "            'L_tile': L_tile,\n",
    "            'label': label,\n",
    "            'meta': meta,\n",
    "            'normal_coord': normal_coord,\n",
    "            'edge_index': edge_index  # slide-level edge index\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "401ef8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 2: Âª∫Á´ã Dataset ======\n",
    "train_dataset = importDataset(\n",
    "    S_tiles=train_image_data['S_tiles'],\n",
    "    M_tiles=train_image_data['M_tiles'],\n",
    "    L_tiles=train_image_data['L_tiles'],\n",
    "    labels=train_image_data['labels'],\n",
    "    meta_info=train_image_data['meta_info'],\n",
    "    normal_coords=train_image_data['normal_coords'],\n",
    "    slide_edge_indices=train_edge_data['slide_edge_indices'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4da0f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 34206])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[8000]['edge_index'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e8a237d-3fff-47d5-831c-0b915cbecb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking dataset sample: 8000\n",
      "üìè S_tile shape: torch.Size([3, 32, 32]) | dtype: torch.float32 | min: 0.694, max: 1.000\n",
      "üìè M_tile shape: torch.Size([3, 64, 64]) | dtype: torch.float32 | min: 0.463, max: 0.969\n",
      "üìè L_tile shape: torch.Size([3, 128, 128]) | dtype: torch.float32 | min: 0.220, max: 1.000\n",
      "üß¨ Label shape: torch.Size([35]) | dtype: torch.float32\n",
      "üß¨ slide_id: S_5, x: 1533, y: 1283, nor_coor: tensor([0.7631, 0.8888])\n",
      "üìä Edge_index shape: torch.Size([2, 34206]) | dtype: torch.int64\n",
      "‚úÖ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "def check_dataset_item(dataset, idx=8000):\n",
    "    item = dataset[idx]\n",
    "\n",
    "    print(\"üîç Checking dataset sample:\", idx)\n",
    "\n",
    "    # Check the center tile (small scale)\n",
    "    S_tile = item['S_tile']\n",
    "    print(f\"üìè S_tile shape: {S_tile.shape} | dtype: {S_tile.dtype} | min: {S_tile.min():.3f}, max: {S_tile.max():.3f}\")\n",
    "    assert S_tile.ndim == 3 and S_tile.shape[0] == 3, \"‚ùå S_tile shape is incorrect, expected (3, H, W)\"\n",
    "\n",
    "    # Check medium scale tile\n",
    "    M_tile = item['M_tile']\n",
    "    print(f\"üìè M_tile shape: {M_tile.shape} | dtype: {M_tile.dtype} | min: {M_tile.min():.3f}, max: {M_tile.max():.3f}\")\n",
    "    assert M_tile.ndim == 3 and M_tile.shape[0] == 3, \"‚ùå M_tile shape is incorrect, expected (3, H, W)\"\n",
    "\n",
    "    # Check large scale tile\n",
    "    L_tile = item['L_tile']\n",
    "    print(f\"üìè L_tile shape: {L_tile.shape} | dtype: {L_tile.dtype} | min: {L_tile.min():.3f}, max: {L_tile.max():.3f}\")\n",
    "    assert L_tile.ndim == 3 and L_tile.shape[0] == 3, \"‚ùå L_tile shape is incorrect, expected (3, H, W)\"\n",
    "\n",
    "    # Check label (cell composition)\n",
    "    label = item['label']\n",
    "    print(f\"üß¨ Label shape: {label.shape} | dtype: {label.dtype}\")\n",
    "    assert label.shape[0] == 35 and label.dtype == torch.float32, \"‚ùå Label should be float32 with length 35\"\n",
    "\n",
    "    # Check meta info (slide_id, x, y coordinates)\n",
    "        # Check meta info (slide_id, x, y coordinates)\n",
    "    meta = item['meta']\n",
    "    normal_coord = item['normal_coord']\n",
    "\n",
    "    print(f\"üß¨ slide_id: {meta[0]}, x: {meta[1]}, y: {meta[2]}, nor_coor: {normal_coord}\")\n",
    "\n",
    "    # Check edge_index (slide-level graph edges)\n",
    "    edge_index = item['edge_index']\n",
    "    print(f\"üìä Edge_index shape: {edge_index.shape} | dtype: {edge_index.dtype}\")\n",
    "    assert edge_index.ndimension() == 2 and edge_index.shape[0] == 2, \"‚ùå edge_index shape is incorrect, should be (2, num_edges)\"\n",
    "\n",
    "    print(\"‚úÖ All checks passed!\")\n",
    "    \n",
    "# Example of how to use the function\n",
    "check_dataset_item(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d21906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train set size: 4 samples\n",
      "‚úÖ Validation set size: 2 samples\n",
      "DataBatch(x=[7331, 64512], edge_index=[2, 131428], y=[7331, 35], pos=[7331, 2], batch=[7331], ptr=[5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "\n",
    "\n",
    "# Ë®≠ÂÆöË®ìÁ∑¥ÈõÜÂíåÈ©óË≠âÈõÜÁöÑÊØî‰æã\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "\n",
    "# Ë®àÁÆóË®ìÁ∑¥ÈõÜÂíåÈ©óË≠âÈõÜÁöÑÂ§ßÂ∞è\n",
    "total_len = len(data_list)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# ‰ΩøÁî® random_split ÊãÜÂàÜÊï∏ÊìöÈõÜ\n",
    "train_set, val_set = random_split(data_list, [train_len, val_len])\n",
    "\n",
    "print(f\"‚úÖ Train set size: {len(train_set)} samples\")\n",
    "print(f\"‚úÖ Validation set size: {len(val_set)} samples\")\n",
    "\n",
    "# ‰ΩøÁî® DataLoader Âä†ËºâÊï∏ÊìöÔºå‰∏¶Ë®≠ÁΩÆ collate_fn ‰æÜËôïÁêÜÊâπÊ¨°\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=lambda batch: Batch.from_data_list(batch))\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False, collate_fn=lambda batch: Batch.from_data_list(batch))\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "\n",
    "print(batch)\n",
    "\n",
    "print(batch.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "267716fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original batch.x shape: torch.Size([7331, 64512])\n",
      "S_tile shape: torch.Size([7331, 32])\n",
      "M_tile shape: torch.Size([7331, 64])\n",
      "L_tile shape: torch.Size([7331, 64416])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import DataLoader, Batch\n",
    "\n",
    "# ÂÅáË®≠ÈÄô‰∫õÊòØÊØèÂÄãÂ∞∫Â∫¶ÁöÑÁâπÂæµÂ∞∫ÂØ∏\n",
    "S_tile_size = 32  # ÂÅáË®≠ÊØèÂÄã S_tile ÁöÑÁâπÂæµÂ§ßÂ∞èÁÇ∫ 10\n",
    "M_tile_size = 64  # ÂÅáË®≠ÊØèÂÄã M_tile ÁöÑÁâπÂæµÂ§ßÂ∞èÁÇ∫ 20\n",
    "L_tile_size = 128  # ÂÅáË®≠ÊØèÂÄã L_tile ÁöÑÁâπÂæµÂ§ßÂ∞èÁÇ∫ 30\n",
    "\n",
    "# ‰ΩøÁî® DataLoader Âä†ËºâÊï∏ÊìöÔºå‰∏¶Ë®≠ÁΩÆ collate_fn ‰æÜËôïÁêÜÊâπÊ¨°\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=lambda batch: Batch.from_data_list(batch))\n",
    "\n",
    "for batch in train_loader:\n",
    "    # Êü•Áúã batch.x ÁöÑÂΩ¢ÁãÄ\n",
    "    print(\"Original batch.x shape:\", batch.x.shape)\n",
    "\n",
    "    # ÂÅáË®≠ batch.x ÊòØÂêà‰Ωµ‰∫Ü‰∏çÂêåÂ∞∫Â∫¶ÁâπÂæµÁöÑÂêëÈáèÔºåÁèæÂú®ÊàëÂÄëÊ†πÊìöÊØèÂÄãÂ∞∫Â∫¶ÁöÑÁâπÂæµÂ§ßÂ∞èÊãÜÂàÜÂÆÉ\n",
    "    # ÂÅáË®≠ batch.x ÁöÑÂΩ¢ÁãÄÊòØ [num_nodes, S_tile_size + M_tile_size + L_tile_size]\n",
    "    \n",
    "    x_s = batch.x[:, :S_tile_size]  # Á¨¨‰∏ÄÈÉ®ÂàÜÊòØ S_tile ÁâπÂæµ\n",
    "    x_m = batch.x[:, S_tile_size:S_tile_size + M_tile_size]  # Á¨¨‰∫åÈÉ®ÂàÜÊòØ M_tile ÁâπÂæµ\n",
    "    x_l = batch.x[:, S_tile_size + M_tile_size:]  # Á¨¨‰∏âÈÉ®ÂàÜÊòØ L_tile ÁâπÂæµ\n",
    "    \n",
    "    # ÊâìÂç∞ÊãÜÂàÜÂæåÁöÑÂΩ¢ÁãÄ‰æÜÊ™¢Êü•\n",
    "    print(\"S_tile shape:\", x_s.shape)\n",
    "    print(\"M_tile shape:\", x_m.shape)\n",
    "    print(\"L_tile shape:\", x_l.shape)\n",
    "\n",
    "    # ÈÄôË£°ÁöÑ x_s, x_m, x_l Â∞±ÊòØÈÇÑÂéüÂæåÁöÑ‰∏çÂêåÂ∞∫Â∫¶ÁöÑÁâπÂæµ\n",
    "    break  # Âè™Êü•ÁúãÁ¨¨‰∏ÄÂÄãÊâπÊ¨°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccdc3b",
   "metadata": {},
   "source": [
    "Poteintial issues:\n",
    "# 1. my val_set tiles image may be included in the sub_tiles of train_set\n",
    "\n",
    "Note: Since neighbor tiles are reused across samples, some mild information overlap may exist between train and val sets. However, final test set is completely held out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45327c2e",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f90c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GraphSAGE, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Define a CNN for each scale (S_tile, M_tile, L_tile)\n",
    "class SmallScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "class MediumScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MediumScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "class LargeScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        # GraphSAGE layer\n",
    "        self.graphsage = GraphSAGE(input_dim, hidden_dim, aggr='mean')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # For predicting 35 cell types\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Assuming `data` contains the features and edge_index for the graph\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.graphsage(x, edge_index)  # Apply GraphSAGE to the features\n",
    "        x = global_mean_pool(x, data.batch)  # Global pooling for each graph (slide)\n",
    "        return self.fc(x)  # Predict the cell type composition\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=1)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention mechanism\n",
    "        attn_output, _ = self.attention(x, x, x)  # Self-attention on the feature vector\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "\n",
    "class CellTypePredictionModel(nn.Module):\n",
    "    def __init__(self, small_cnn, medium_cnn, large_cnn, graphsage_model, attention_head):\n",
    "        super(CellTypePredictionModel, self).__init__()\n",
    "        self.small_cnn = small_cnn  # CNN for small-scale patch\n",
    "        self.medium_cnn = medium_cnn  # CNN for medium-scale patch\n",
    "        self.large_cnn = large_cnn  # CNN for large-scale patch\n",
    "        self.graphsage_model = graphsage_model  # GraphSAGE for graph-based feature learning\n",
    "        self.attention_head = attention_head  # Attention mechanism to focus on important features\n",
    "\n",
    "    def forward(self, S_tile, M_tile, L_tile, edge_index, normal_coords):\n",
    "        # Step 1: CNN feature extraction for each scale (S_tile, M_tile, L_tile)\n",
    "        S_features = self.small_cnn(S_tile)\n",
    "        M_features = self.medium_cnn(M_tile)\n",
    "        L_features = self.large_cnn(L_tile)\n",
    "        \n",
    "        # Combine features from all scales into one feature vector (F_i)\n",
    "        combined_features = torch.cat([S_features, M_features, L_features], dim=1)\n",
    "        \n",
    "        # Step 2: Create PyG Data object for Graph Neural Network\n",
    "        data = Data(x=combined_features, edge_index=edge_index, pos=normal_coords)\n",
    "        \n",
    "        # Step 3: GraphSAGE layer to learn spatial context-aware features\n",
    "        graphsage_output = self.graphsage_model(data)\n",
    "        \n",
    "        # Step 4: Attention mechanism to focus on the relevant features for cell-type prediction\n",
    "        attention_output = self.attention_head(graphsage_output.unsqueeze(0))  # Add batch dimension\n",
    "        return attention_output.squeeze(0)  # Remove batch dimension for final output\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the individual CNNs for each scale\n",
    "small_cnn = SmallScaleCNN()\n",
    "medium_cnn = MediumScaleCNN()\n",
    "large_cnn = LargeScaleCNN()\n",
    "\n",
    "# Instantiate the GraphSAGE model and attention head\n",
    "graphsage_model = GraphSAGEModel(input_dim=16 + 32 + 64, hidden_dim=256, output_dim=256)\n",
    "attention_head = AttentionHead(input_dim=256, output_dim=35)  # Predict 35 cell types\n",
    "\n",
    "# Instantiate the final model\n",
    "model = CellTypePredictionModel(small_cnn, medium_cnn, large_cnn, graphsage_model, attention_head)\n",
    "\n",
    "# Example input: Assuming you have a batch of data\n",
    "S_tile = torch.rand(32, 3, 32, 32)  # Example batch of small-scale image tiles\n",
    "M_tile = torch.rand(32, 3, 64, 64)  # Example batch of medium-scale image tiles\n",
    "L_tile = torch.rand(32, 3, 128, 128)  # Example batch of large-scale image tiles\n",
    "meta_info = [(f\"S_{i}\", torch.rand(2)) for i in range(32)]  # Dummy metadata for batch\n",
    "edge_index = torch.randint(0, 32, (2, 50))  # Dummy graph edge indices for each slide\n",
    "\n",
    "# Forward pass\n",
    "output = model(S_tile, M_tile, L_tile, meta_info, edge_index)\n",
    "print(\"Output shape:\", output.shape)  # Should be (batch_size, 35) for the 35 cell types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aad638c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0874, 0.7543])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]['normal_coord']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d68d1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphSAGE, global_mean_pool\n",
    "from torchvision import models\n",
    "\n",
    "class MultiScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiScaleCNN, self).__init__()\n",
    "        # CNN backbone for extracting features from small, medium, and large patches\n",
    "        self.conv_s = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv_m = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv_l = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, S_tile, M_tile, L_tile):\n",
    "        # Process each scale of image patch\n",
    "        f_s = self.conv_s(S_tile).view(S_tile.size(0), -1)\n",
    "        f_m = self.conv_m(M_tile).view(M_tile.size(0), -1)\n",
    "        f_l = self.conv_l(L_tile).view(L_tile.size(0), -1)\n",
    "        # Combine features from all scales\n",
    "        return torch.cat([f_s, f_m, f_l], dim=1)\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        # GraphSAGE layer\n",
    "        self.graphsage = GraphSAGE(input_dim, hidden_dim, num_layers, aggr='mean')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # For predicting 35 cell types\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Assuming `data` contains the features and edge_index for the graph\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.graphsage(x, edge_index)  # Apply GraphSAGE to the features\n",
    "        x = global_mean_pool(x, data.batch)  # Global pooling for each graph (slide)\n",
    "        return self.fc(x)  # Predict the cell type composition\n",
    "\n",
    "class MultiLayerAttentionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers=2):\n",
    "        super(MultiLayerAttentionHead, self).__init__()\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim=input_dim, num_heads=1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = x\n",
    "        for attn_layer in self.attention_layers:\n",
    "            attn_output, _ = attn_layer(attn_output, attn_output, attn_output)  # Self-attention on the feature vector\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "\n",
    "class HEVisium(nn.Module):\n",
    "    def __init__(self, cnn_backbone, graphsage_model, attention_head):\n",
    "        super(HEVisium, self).__init__()\n",
    "        self.cnn_backbone = cnn_backbone  # CNN for multi-scale feature extraction\n",
    "        self.graphsage_model = graphsage_model  # GraphSAGE for graph-based feature learning\n",
    "        self.attention_head = attention_head  # Attention mechanism to focus on important features\n",
    "\n",
    "    def forward(self, S_tile, M_tile, L_tile, edge_index, normal_coords):\n",
    "        # Step 1: CNN feature extraction\n",
    "        cnn_features = self.cnn_backbone(S_tile, M_tile, L_tile)\n",
    "        \n",
    "        # Step 2: Create PyG Data object\n",
    "        data = Data(x=cnn_features, edge_index=edge_index, pos=normal_coords)\n",
    "        \n",
    "        # Step 3: GraphSAGE layer to learn spatial context-aware features\n",
    "        graphsage_output = self.graphsage_model(data)\n",
    "        \n",
    "        # Step 4: Attention mechanism to focus on the relevant features for cell-type prediction\n",
    "        attention_output = self.attention_head(graphsage_output.unsqueeze(0))  # Add batch dimension\n",
    "        return attention_output.squeeze(0)  # Remove batch dimension for final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac1df12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(64, out_dim)\n",
    "\n",
    "    def forward(self, x):  # x: (B, 3, H, W)\n",
    "        x = self.cnn(x)     # ‚Üí (B, 64, 1, 1)\n",
    "        x = self.flatten(x) # ‚Üí (B, 64)\n",
    "        x = self.linear(x)  # ‚Üí (B, out_dim)\n",
    "        return x\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)  # üëâ Linear activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class VisionMLPModelWithCoord(nn.Module):\n",
    "    def __init__(self, cnn_out_dim=64, output_dim=35):\n",
    "        super().__init__()\n",
    "        self.encoder_spot = CNNEncoder(cnn_out_dim)\n",
    "        self.encoder_subtiles = CNNEncoder(cnn_out_dim)\n",
    "        self.encoder_neighbors = CNNEncoder(cnn_out_dim)\n",
    "\n",
    "        # Input to decoder: 3 * cnn_out_dim + 2 (for x, y)\n",
    "        self.decoder = MLPDecoder(input_dim=cnn_out_dim * 3 + 2, output_dim=output_dim)\n",
    "\n",
    "    def forward(self, center_tile, subtiles, neighbor_tiles, coords):\n",
    "        # center_tile: (B, 3, H, W)\n",
    "        # subtiles: (B, 9, 3, h, w)\n",
    "        # neighbor_tiles: (B, 8, 3, H, W)\n",
    "        # coords: (B, 2)\n",
    "        B = center_tile.size(0)\n",
    "\n",
    "        # Spot\n",
    "        f_center = self.encoder_spot(center_tile)  # (B, D)\n",
    "\n",
    "        # Subtiles\n",
    "        B, N, C, h, w = subtiles.shape\n",
    "        subtiles = subtiles.view(B * N, C, h, w)\n",
    "        f_sub = self.encoder_subtiles(subtiles).view(B, N, -1).mean(dim=1)  # (B, D)\n",
    "\n",
    "        # Neighbors\n",
    "        B, N, C, H, W = neighbor_tiles.shape\n",
    "        neighbor_tiles = neighbor_tiles.view(B * N, C, H, W)\n",
    "        f_neigh = self.encoder_neighbors(neighbor_tiles).view(B, N, -1).mean(dim=1)  # (B, D)\n",
    "        \n",
    "        # Concatenate with coordinates\n",
    "        x = torch.cat([f_center, f_sub, f_neigh, coords], dim=1)  # (B, 3D+2)\n",
    "        out = self.decoder(x)  # (B, 35)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d9d3692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# üß† Ë®ìÁ∑¥‰∏ÄÂÄã epoch\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in pbar:\n",
    "        if isinstance(batch, dict):\n",
    "            print(\"‚ö†Ô∏è Warning: batch is a dictionary, not a Data object\")\n",
    "            # ÊâìÂç∞Âá∫ batch ÁöÑÊâÄÊúâÈçµÔºåÁúãÁúãÂ≠óÂÖ∏ÂåÖÂê´‰∫Ü‰ªÄÈ∫º\n",
    "            print(f\"Batch keys: {batch.keys()}\")\n",
    "        else:\n",
    "            print(\"‚úÖ Batch is a Data object\")\n",
    "        # Áç≤ÂèñË®ìÁ∑¥Êï∏Êìö\n",
    "        S_tile, M_tile, L_tile = batch['S_tile'].to(device), batch['M_tile'].to(device), batch['L_tile'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        edge_index = batch['edge_index'].to(device)\n",
    "        normal_coords = batch['normal_coord'].to(device)\n",
    "        print(\"S_tile shape:\", S_tile.shape)\n",
    "        # ÂÑ™ÂåñÂô®Ê∏ÖÁ©∫Ê¢ØÂ∫¶\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ÂâçÂêëÂÇ≥Êí≠ÔºöË®àÁÆóÈ†êÊ∏¨ÁµêÊûú\n",
    "        out = model(S_tile, M_tile, L_tile, edge_index, normal_coords)\n",
    "\n",
    "        # Ë®àÁÆóÊêçÂ§±\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()  # ÂèçÂêëÂÇ≥Êí≠\n",
    "        optimizer.step()  # Êõ¥Êñ∞Ê®°ÂûãÂèÉÊï∏\n",
    "\n",
    "        total_loss += loss.item() * S_tile.size(0)\n",
    "        avg_loss = total_loss / ((pbar.n + 1) * dataloader.batch_size)\n",
    "        pbar.set_postfix(loss=loss.item(), avg=avg_loss)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "# üìè È©óË≠âÊ®°Âûã\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # Áç≤ÂèñÈ©óË≠âÊï∏Êìö\n",
    "            S_tile, M_tile, L_tile = batch['S_tile'].to(device), batch['M_tile'].to(device), batch['L_tile'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            edge_index = batch['edge_index'].to(device)\n",
    "            normal_coords = batch['normal_coord'].to(device)\n",
    "\n",
    "            # ÂâçÂêëÂÇ≥Êí≠ÔºöË®àÁÆóÈ†êÊ∏¨ÁµêÊûú\n",
    "            out = model(S_tile, M_tile, L_tile, edge_index, normal_coords)\n",
    "            loss = loss_fn(out, label)\n",
    "\n",
    "            total_loss += loss.item() * S_tile.size(0)\n",
    "            preds.append(out.cpu())\n",
    "            targets.append(label.cpu())\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    targets = torch.cat(targets).numpy()\n",
    "\n",
    "    # ‰ΩøÁî® Spearman Áõ∏ÈóúÊÄß‰æÜË®àÁÆóÈ†êÊ∏¨ÁöÑÊ∫ñÁ¢∫ÊÄß\n",
    "    scores = [spearmanr(preds[:, i], targets[:, i])[0] for i in range(preds.shape[1])]\n",
    "    spearman_avg = np.nanmean(scores)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), spearman_avg\n",
    "\n",
    "\n",
    "\n",
    "    # ‚úÖ Spearman correlation for each gene\n",
    "    scores = [spearmanr(preds[:, i], targets[:, i])[0] for i in range(preds.shape[1])]\n",
    "    spearman_avg = np.nanmean(scores)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), spearman_avg\n",
    "\n",
    "# üîÆ È†êÊ∏¨\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_meta = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Áç≤ÂèñÊ∏¨Ë©¶Êï∏Êìö\n",
    "            S_tile, M_tile, L_tile = batch['S_tile'].to(device), batch['M_tile'].to(device), batch['L_tile'].to(device)\n",
    "            edge_index = batch['edge_index'].to(device)\n",
    "            normal_coords = batch['normal_coord'].to(device)\n",
    "\n",
    "            # È†êÊ∏¨\n",
    "            out = model(S_tile, M_tile, L_tile, edge_index, normal_coords)\n",
    "            all_preds.append(out.cpu())\n",
    "            all_meta.extend(batch['meta'])  # Áî®‰æÜÂ≠òÂÑ≤Ê∏¨Ë©¶ÈõÜÁöÑmeta‰ø°ÊÅØ\n",
    "\n",
    "    return torch.cat(all_preds).numpy(), all_meta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b268",
   "metadata": {},
   "source": [
    "# callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f6e2ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None or val_loss < self.best_score:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(val_losses, label=\"Val Loss\", marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.show()\n",
    "# Êî∂ÈõÜË≥áÊñô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7c9b4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "                                                 \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 39822] at entry 0 and [2, 34206] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[201], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 41\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     val_loss, val_spearman \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, loss_fn, device)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# ‚úÖ ÂÑ≤Â≠òÊúÄÂ•ΩÁöÑÊ®°Âûã\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[109], line 12\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ö†Ô∏è Warning: batch is a dictionary, not a Data object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py:43\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Mapping):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[38;5;28mself\u001b[39m([data[key] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py:43\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Mapping):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py:33\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(\n\u001b[1;32m     28\u001b[0m         batch,\n\u001b[1;32m     29\u001b[0m         follow_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_batch,\n\u001b[1;32m     30\u001b[0m         exclude_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_keys,\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, TensorFrame):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_frame\u001b[38;5;241m.\u001b[39mcat(batch, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 39822] at entry 0 and [2, 34206] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# üîß Ë®≠ÂÆöË£ùÁΩÆ\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "# üîß ÂàùÂßãÂåñÊ®°Âûã & ÂÑ™ÂåñÂô®\n",
    "# üîß ÂàùÂßãÂåñÊ®°Âûã & ÂÑ™ÂåñÂô®\n",
    "cnn_backbone = MultiScaleCNN()  # ÈÄôÊòØ‰Ω†ÂÆöÁæ©ÁöÑÂ§öÂ∞∫Â∫¶CNN\n",
    "graphsage_model = GraphSAGEModel(input_dim=1024, hidden_dim=512, output_dim=35, num_layers=2)  # ÂÖ©Â±§ GraphSAGE\n",
    "attention_head = MultiLayerAttentionHead(input_dim=256, output_dim=35)  # ÂÅáË®≠ÁöÑ AttentionHead\n",
    "\n",
    "model = HEVisium(cnn_backbone, graphsage_model, attention_head).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()  # ÊàñËÄÖ‰ΩøÁî®ÂÖ∂‰ªñÊêçÂ§±ÂáΩÊï∏Ôºå‰æãÂ¶Ç spearman_loss\n",
    "#loss_fn = spearman_loss\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# üîß ÂÑ≤Â≠ò log ÁöÑË®≠ÂÆö\n",
    "log_file = open(\"training_log.csv\", mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# üîß Áî®‰æÜÁï´Âúñ\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "# üîÅ ÈñãÂßãË®ìÁ∑¥\n",
    "num_epochs = 150\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_spearman = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "    # ‚úÖ ÂÑ≤Â≠òÊúÄÂ•ΩÁöÑÊ®°Âûã\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"‚úÖ Saved best model!\")\n",
    "\n",
    "    # ‚úÖ Ë™øÊï¥Â≠∏ÁøíÁéá\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # ‚úÖ ÂØ´ÂÖ• CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch+1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # ‚úÖ Âç∞ epoch ÁµêÊûú\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | œÅ: {val_spearman:.4f} | lr: {lr:.2e}\")\n",
    "\n",
    "    # ‚úÖ Êõ¥Êñ∞ loss list ‰∏¶Áï´Âúñ\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    plot_losses(train_losses, val_losses)\n",
    "\n",
    "    # ‚úÖ Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"‚õî Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# ‚úÖ ÈóúÈñâ log Ê™îÊ°à\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1478ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'abc.DataBatch'>\n",
      "‚úÖ This batch is a Data object!\n",
      "Batch keys: ['L_tile', 'label', 'S_tile', 'ptr', 'M_tile', 'edge_index', 'meta', 'normal_coord', 'batch']\n",
      "S_tile shape: torch.Size([96, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Ê∏¨Ë©¶ train_loader ÊòØÂê¶ËøîÂõûÊ≠£Á¢∫Ê†ºÂºèÁöÑÊï∏Êìö\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch type: {type(batch)}\")\n",
    "    if isinstance(batch, Data):\n",
    "        print(\"‚úÖ This batch is a Data object!\")\n",
    "        print(f\"Batch keys: {batch.keys()}\")\n",
    "        \n",
    "        # ÊâìÂç∞ S_tile ÁöÑ shape ‰æÜÊ™¢Êü•\n",
    "        if 'S_tile' in batch:\n",
    "            print(f\"S_tile shape: {batch['S_tile'].shape}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è S_tile not found in batch!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è This batch is not a Data object\")\n",
    "    break  # Âè™Ê™¢Êü•Á¨¨‰∏ÄÂÄãÊâπÊ¨°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f55274",
   "metadata": {},
   "source": [
    "ÔºÉ# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7220265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/3135847424.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionMLPModelWithCoord(\n",
       "  (encoder_spot): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_subtiles): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_neighbors): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): MLPDecoder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=194, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=35, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== ÈúÄË¶ÅÁöÑ Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "\n",
    "# ===== ËºâÂÖ•Ë®ìÁ∑¥Â•ΩÁöÑÊ®°ÂûãÊ¨äÈáç =====\n",
    "from hevisum_model import HEVisumModel\n",
    "from hevisum_dataset import importDataset\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "model = VisionMLPModelWithCoord().to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77d22647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/1343659088.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"test_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "# Ê≠£Á¢∫ÊñπÂºè\n",
    "test_data = torch.load(\"test_dataset.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "# ====== Step 2: Âª∫Á´ã Dataset ======\n",
    "test_dataset = importDataset(\n",
    "    center_tile=test_data['tiles'],\n",
    "    subtiles=test_data['subtiles'],\n",
    "    neighbor_tiles=test_data['neighbor_tiles'],\n",
    "    label=np.zeros((len(test_data['tiles']), 35)),  # dummy label\n",
    "    meta=normalized_coords\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea4d8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "test_preds, test_meta = predict(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ddbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.25503415, 0.13337179, 0.1524582 , ..., 0.04139816, 0.01808124,\n",
       "        0.03039141],\n",
       "       ...,\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.23385419, 0.13244098, 0.13941698, ..., 0.04153137, 0.01910294,\n",
       "        0.03005139]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f26de3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== ËÆÄÂèñ test spot index Áî®ÊñºÂ∞çÊáâ ID ====\n",
    "with h5py.File(\"./elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "\n",
    "ensemble_df = pd.DataFrame(test_preds, columns=[f\"C{i+1}\" for i in range(test_preds.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"‚úÖ Saved submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
