{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6db04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b70981c-db23-4ea7-bd9c-dca6279d7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fb8f4b-1df0-484b-8931-3805eb21c5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_21962/2522590248.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_image_data = torch.load(\"../SML_train_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# ËºâÂÖ•Ë≥áÊñô\n",
    "train_image_data = torch.load(\"../SML_train_dataset.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db02b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class importDataset(Dataset):\n",
    "    def __init__(self, S_tiles, M_tiles, L_tiles, labels, meta_info, normal_coords, slide_edge_indices):\n",
    "        \"\"\"\n",
    "        Custom Dataset to load image tiles, labels, and edge indices for training.\n",
    "        \n",
    "        Args:\n",
    "            S_tiles (list): Small scale image tiles (spot-level features).\n",
    "            M_tiles (list): Medium scale image tiles (spot-level features).\n",
    "            L_tiles (list): Large scale image tiles (spot-level features).\n",
    "            labels (list): Ground truth labels (spot-level cell type compositions).\n",
    "            meta_info (list): Metadata containing slide_id, x, y coordinates for each spot.\n",
    "            normal_coords (list): Normalized coordinates for each spot.\n",
    "            slide_edge_indices (dict): Dictionary containing the slide-level edge indices for each slide.\n",
    "        \"\"\"\n",
    "        self.S_tiles = S_tiles\n",
    "        self.M_tiles = M_tiles\n",
    "        self.L_tiles = L_tiles\n",
    "        self.labels = labels\n",
    "        self.meta_info = meta_info\n",
    "        self.normal_coords = normal_coords\n",
    "        self.slide_edge_indices = slide_edge_indices\n",
    "        \n",
    "        # Optional: Apply any additional processing to the data here\n",
    "        # e.g., scaling, normalization, etc.\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples (spots) in the dataset\n",
    "        return len(self.S_tiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the spot-level data for the sample at index `idx`\n",
    "        S_tile = self.S_tiles[idx]\n",
    "        M_tile = self.M_tiles[idx]\n",
    "        L_tile = self.L_tiles[idx]\n",
    "        label = self.labels[idx]\n",
    "        meta = self.meta_info[idx]  # This is a tuple (slide_id, x, y)\n",
    "        normal_coord = self.normal_coords[idx]\n",
    "        \n",
    "        # Extract slide_id from meta_info (slide_id is at index 0)\n",
    "        slide_id = meta[0]\n",
    "        \n",
    "        # Get the corresponding edge_index for this slide from slide_edge_indices\n",
    "        edge_index = self.slide_edge_indices[idx]\n",
    "        \n",
    "        # Convert the tiles to the correct format (channels-first)\n",
    "        S_tile = torch.tensor(S_tile, dtype=torch.float).permute(2, 0, 1)  # Convert from (H, W, 3) to (3, H, W)\n",
    "        M_tile = torch.tensor(M_tile, dtype=torch.float).permute(2, 0, 1)\n",
    "        L_tile = torch.tensor(L_tile, dtype=torch.float).permute(2, 0, 1)\n",
    "        \n",
    "        # Convert label and normal_coord to tensors\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        normal_coord = torch.tensor(normal_coord, dtype=torch.float)\n",
    "        \n",
    "        # Return the data in a PyTorch-friendly format as a dictionary\n",
    "        return {\n",
    "            'S_tile': S_tile,\n",
    "            'M_tile': M_tile,\n",
    "            'L_tile': L_tile,\n",
    "            'label': label,\n",
    "            'meta': meta,\n",
    "            'normal_coord': normal_coord,\n",
    "            'edge_index': edge_index,  # slide-level edge index\n",
    "            'slide_id': slide_id  # slide_id for the current sample\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401ef8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 2: Âª∫Á´ã Dataset ======\n",
    "image_train_dataset = importDataset(\n",
    "    S_tiles=train_image_data['S_tiles'],\n",
    "    M_tiles=train_image_data['M_tiles'],\n",
    "    L_tiles=train_image_data['L_tiles'],\n",
    "    labels=train_image_data['labels'],\n",
    "    meta_info=train_image_data['meta_info'],\n",
    "    normal_coords=train_image_data['normal_coords'],\n",
    "    slide_edge_indices=train_image_data['edge_indices']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e8a237d-3fff-47d5-831c-0b915cbecb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking dataset sample: 8000\n",
      "üìè S_tile shape: torch.Size([3, 32, 32]) | dtype: torch.float32 | min: 0.694, max: 1.000\n",
      "üìè M_tile shape: torch.Size([3, 64, 64]) | dtype: torch.float32 | min: 0.463, max: 0.969\n",
      "üìè L_tile shape: torch.Size([3, 128, 128]) | dtype: torch.float32 | min: 0.220, max: 1.000\n",
      "üß¨ Label shape: torch.Size([35]) | dtype: torch.float32\n",
      "üß¨ slide_id: S_5, x: 1533, y: 1283, nor_coor: tensor([0.7631, 0.8888])\n",
      "‚úÖ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "def check_dataset_item(dataset, idx=8000):\n",
    "    item = dataset[idx]\n",
    "\n",
    "    print(\"üîç Checking dataset sample:\", idx)\n",
    "\n",
    "    # Check the center tile (small scale)\n",
    "    S_tile = item['S_tile']\n",
    "    print(f\"üìè S_tile shape: {S_tile.shape} | dtype: {S_tile.dtype} | min: {S_tile.min():.3f}, max: {S_tile.max():.3f}\")\n",
    "    assert S_tile.ndim == 3 and S_tile.shape[0] == 3, \"‚ùå S_tile shape is incorrect, expected (3, H, W)\"\n",
    "\n",
    "    # Check medium scale tile\n",
    "    M_tile = item['M_tile']\n",
    "    print(f\"üìè M_tile shape: {M_tile.shape} | dtype: {M_tile.dtype} | min: {M_tile.min():.3f}, max: {M_tile.max():.3f}\")\n",
    "    assert M_tile.ndim == 3 and M_tile.shape[0] == 3, \"‚ùå M_tile shape is incorrect, expected (3, H, W)\"\n",
    "\n",
    "    # Check large scale tile\n",
    "    L_tile = item['L_tile']\n",
    "    print(f\"üìè L_tile shape: {L_tile.shape} | dtype: {L_tile.dtype} | min: {L_tile.min():.3f}, max: {L_tile.max():.3f}\")\n",
    "    assert L_tile.ndim == 3 and L_tile.shape[0] == 3, \"‚ùå L_tile shape is incorrect, expected (3, H, W)\"\n",
    "\n",
    "    # Check label (cell composition)\n",
    "    label = item['label']\n",
    "    print(f\"üß¨ Label shape: {label.shape} | dtype: {label.dtype}\")\n",
    "    assert label.shape[0] == 35 and label.dtype == torch.float32, \"‚ùå Label should be float32 with length 35\"\n",
    "\n",
    "    # Check meta info (slide_id, x, y coordinates)\n",
    "        # Check meta info (slide_id, x, y coordinates)\n",
    "    meta = item['meta']\n",
    "    normal_coord = item['normal_coord']\n",
    "\n",
    "    print(f\"üß¨ slide_id: {meta[0]}, x: {meta[1]}, y: {meta[2]}, nor_coor: {normal_coord}\")\n",
    "\n",
    "    print(\"‚úÖ All checks passed!\")\n",
    "    \n",
    "# Example of how to use the function\n",
    "check_dataset_item(image_train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c83a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validation set size: 1670 samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "# Á¢∫ÂÆöÊï∏ÊìöÈõÜÁöÑÁ∏ΩÂ§ßÂ∞èÔºàÂÖ©ËÄÖÂøÖÈ†àÁõ∏ÂêåÔºâ\n",
    "dataset_size = len(image_train_dataset)  # ÂêåÊôÇ image_train_dataset Âíå graph_train_dataset Èï∑Â∫¶ÊáâÁõ∏Âêå\n",
    "\n",
    "# Ë®≠ÂÆöË®ìÁ∑¥ÊØî‰æãÔºåÈÄôË£°‰ª• 80% ÁÇ∫Ë®ìÁ∑¥ÈõÜÔºå20% ÁÇ∫È©óË≠âÈõÜ\n",
    "train_ratio = 0.8\n",
    "split_index = int(np.floor(train_ratio * dataset_size))\n",
    "\n",
    "# ÁîüÊàêÈö®Ê©üÁ¥¢Âºï\n",
    "indices = torch.randperm(dataset_size).tolist()\n",
    "\n",
    "# Â∞áÁ¥¢ÂºïÊãÜÂàÜÁÇ∫Ë®ìÁ∑¥ÂíåÈ©óË≠âÈÉ®ÂàÜ\n",
    "train_indices = indices[:split_index]\n",
    "val_indices = indices[split_index:]\n",
    "\n",
    "# ‰ΩøÁî® Subset Ê†πÊìöÁõ∏ÂêåÁöÑÁ¥¢ÂºïÂª∫Á´ãÂ≠êÈõÜ\n",
    "image_train_subset = Subset(image_train_dataset, train_indices)\n",
    "image_val_subset = Subset(image_train_dataset, val_indices)\n",
    "\n",
    "\n",
    "# ÁèæÂú®Ôºå‰Ω†ÊúâÂÖ©Â∞ç DataLoaderÔºå‰∏îÂÆÉÂÄëÁöÑÁ¥¢ÂºïÊòØ‰∏Ä‰∏ÄÂ∞çÊáâÁöÑ\n",
    "print(f\"‚úÖ Validation set size: {len(image_val_subset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46d21906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "combined_train_loader = DataLoader(image_train_subset, batch_size=32, shuffle=True)\n",
    "\n",
    "combined_val_loader = DataLoader(image_val_subset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a9e1ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 299008])\n",
      "tensor(0) tensor(3985)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  5,\n",
       "          5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,\n",
       "          7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14,\n",
       "         14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16,\n",
       "         16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n",
       "         18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21,\n",
       "         21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21],\n",
       "        [ 7, 18, 12,  2, 10,  8,  3,  6, 21,  8, 16,  6,  4, 21, 15,  9, 19, 14,\n",
       "          7, 18,  5, 12, 17, 10,  0,  8,  3,  6, 21,  9, 14, 13, 18,  5, 12, 17,\n",
       "          2,  0,  8, 21,  9, 19, 14, 13, 11, 10,  8, 16,  6,  1, 21, 15,  9, 18,\n",
       "         12, 17,  2,  3, 13,  7,  2, 10,  0,  8, 16,  4,  1, 21,  9, 18,  2, 10,\n",
       "          0,  8,  6,  7, 18, 12,  2, 10,  0,  3,  6,  4,  1, 21,  9, 14,  2,  8,\n",
       "          3, 16,  6,  4,  1, 21, 15, 19, 14, 13, 20, 11,  7, 18,  2,  0,  8,  6,\n",
       "          4, 21, 17,  3, 15,  9, 19, 14, 13, 20, 18,  5, 17,  2,  0,  8,  3, 14,\n",
       "         13,  5, 12, 17,  2,  3,  9, 19, 14, 20, 11, 12, 17,  2,  8,  3,  1, 21,\n",
       "         15,  9, 19, 13, 20, 11, 16,  4,  1, 21,  9, 19, 14, 20, 11,  6,  4,  1,\n",
       "         21, 15,  9, 18,  5, 12,  2,  3, 14, 13, 11,  7,  5, 12, 17,  2, 10,  0,\n",
       "          8,  3,  3,  1, 21, 15,  9, 14, 13, 20, 11, 15,  9, 19, 14, 13, 11,  2,\n",
       "         10,  0,  8,  3, 16,  6,  4,  1, 15,  9, 19, 14]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.x.shape)          # ÊáâË©≤ÂèçÊò†Âúñ‰∏≠ÁØÄÈªûÊï∏\n",
    "print(data.edge_index.min(), data.edge_index.max())\n",
    "graph_train_dataset[4][\"edge_index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccdc3b",
   "metadata": {},
   "source": [
    "Poteintial issues:\n",
    "# 1. my val_set tiles image may be included in the sub_tiles of train_set\n",
    "\n",
    "Note: Since neighbor tiles are reused across samples, some mild information overlap may exist between train and val sets. However, final test set is completely held out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45327c2e",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f90c9b",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GraphSAGE, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Define a CNN for each scale (S_tile, M_tile, L_tile)\n",
    "class SmallScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "class MediumScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MediumScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "class LargeScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeScaleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).view(x.size(0), -1)  # Flatten the output for feature vector\n",
    "\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        # GraphSAGE layer\n",
    "        self.graphsage = GraphSAGE(input_dim, hidden_dim, aggr='mean')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # For predicting 35 cell types\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Assuming `data` contains the features and edge_index for the graph\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.graphsage(x, edge_index)  # Apply GraphSAGE to the features\n",
    "        x = global_mean_pool(x, data.batch)  # Global pooling for each graph (slide)\n",
    "        return self.fc(x)  # Predict the cell type composition\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=1)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention mechanism\n",
    "        attn_output, _ = self.attention(x, x, x)  # Self-attention on the feature vector\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "\n",
    "class CellTypePredictionModel(nn.Module):\n",
    "    def __init__(self, small_cnn, medium_cnn, large_cnn, graphsage_model, attention_head):\n",
    "        super(CellTypePredictionModel, self).__init__()\n",
    "        self.small_cnn = small_cnn  # CNN for small-scale patch\n",
    "        self.medium_cnn = medium_cnn  # CNN for medium-scale patch\n",
    "        self.large_cnn = large_cnn  # CNN for large-scale patch\n",
    "        self.graphsage_model = graphsage_model  # GraphSAGE for graph-based feature learning\n",
    "        self.attention_head = attention_head  # Attention mechanism to focus on important features\n",
    "\n",
    "    def forward(self, S_tile, M_tile, L_tile, edge_index, normal_coords):\n",
    "        # Step 1: CNN feature extraction for each scale (S_tile, M_tile, L_tile)\n",
    "        S_features = self.small_cnn(S_tile)\n",
    "        M_features = self.medium_cnn(M_tile)\n",
    "        L_features = self.large_cnn(L_tile)\n",
    "        \n",
    "        # Combine features from all scales into one feature vector (F_i)\n",
    "        combined_features = torch.cat([S_features, M_features, L_features], dim=1)\n",
    "        \n",
    "        # Step 2: Create PyG Data object for Graph Neural Network\n",
    "        data = Data(x=combined_features, edge_index=edge_index, pos=normal_coords)\n",
    "        \n",
    "        # Step 3: GraphSAGE layer to learn spatial context-aware features\n",
    "        graphsage_output = self.graphsage_model(data)\n",
    "        \n",
    "        # Step 4: Attention mechanism to focus on the relevant features for cell-type prediction\n",
    "        attention_output = self.attention_head(graphsage_output.unsqueeze(0))  # Add batch dimension\n",
    "        return attention_output.squeeze(0)  # Remove batch dimension for final output\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the individual CNNs for each scale\n",
    "small_cnn = SmallScaleCNN()\n",
    "medium_cnn = MediumScaleCNN()\n",
    "large_cnn = LargeScaleCNN()\n",
    "\n",
    "# Instantiate the GraphSAGE model and attention head\n",
    "graphsage_model = GraphSAGEModel(input_dim=16 + 32 + 64, hidden_dim=256, output_dim=256)\n",
    "attention_head = AttentionHead(input_dim=256, output_dim=35)  # Predict 35 cell types\n",
    "\n",
    "# Instantiate the final model\n",
    "model = CellTypePredictionModel(small_cnn, medium_cnn, large_cnn, graphsage_model, attention_head)\n",
    "\n",
    "# Example input: Assuming you have a batch of data\n",
    "S_tile = torch.rand(32, 3, 32, 32)  # Example batch of small-scale image tiles\n",
    "M_tile = torch.rand(32, 3, 64, 64)  # Example batch of medium-scale image tiles\n",
    "L_tile = torch.rand(32, 3, 128, 128)  # Example batch of large-scale image tiles\n",
    "meta_info = [(f\"S_{i}\", torch.rand(2)) for i in range(32)]  # Dummy metadata for batch\n",
    "edge_index = torch.randint(0, 32, (2, 50))  # Dummy graph edge indices for each slide\n",
    "\n",
    "# Forward pass\n",
    "output = model(S_tile, M_tile, L_tile, meta_info, edge_index)\n",
    "print(\"Output shape:\", output.shape)  # Should be (batch_size, 35) for the 35 cell types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d68d1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphSAGE, global_mean_pool\n",
    "from torchvision import models\n",
    "\n",
    "class MultiScaleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiScaleCNN, self).__init__()\n",
    "        # CNN backbone for extracting features from small, medium, and large patches\n",
    "        self.conv_s = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv_m = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv_l = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, S_tile, M_tile, L_tile):\n",
    "        # Process each scale of image patch\n",
    "        f_s = self.conv_s(S_tile).view(S_tile.size(0), -1)\n",
    "        f_m = self.conv_m(M_tile).view(M_tile.size(0), -1)\n",
    "        f_l = self.conv_l(L_tile).view(L_tile.size(0), -1)\n",
    "        # Combine features from all scales\n",
    "        return torch.cat([f_s, f_m, f_l], dim=1)\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        # GraphSAGE layer\n",
    "        self.graphsage = GraphSAGE(input_dim, hidden_dim, num_layers, aggr='mean')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # For predicting 35 cell types\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Assuming `data` contains the features and edge_index for the graph\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.graphsage(x, edge_index)  # Apply GraphSAGE to the features\n",
    "        x = global_mean_pool(x, data.batch)  # Global pooling for each graph (slide)\n",
    "        return self.fc(x)  # Predict the cell type composition\n",
    "\n",
    "class MultiLayerAttentionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers=1):\n",
    "        super(MultiLayerAttentionHead, self).__init__()\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim=input_dim, num_heads=1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = x\n",
    "        for attn_layer in self.attention_layers:\n",
    "            attn_output, _ = attn_layer(attn_output, attn_output, attn_output)  # Self-attention on the feature vector\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "\n",
    "class HEVisium(nn.Module):\n",
    "    def __init__(self, cnn_backbone, graphsage_model, attention_head):\n",
    "        super(HEVisium, self).__init__()\n",
    "        self.cnn_backbone = cnn_backbone  # CNN for multi-scale feature extraction\n",
    "        self.graphsage_model = graphsage_model  # GraphSAGE for graph-based feature learning\n",
    "        self.attention_head = attention_head  # Attention mechanism to focus on important features\n",
    "\n",
    "    def forward(self, S_tile, M_tile, L_tile, edge_index, normal_coords):\n",
    "        # Step 1: CNN feature extraction\n",
    "        cnn_features = self.cnn_backbone(S_tile, M_tile, L_tile)\n",
    "        print(\"finished cnn\")\n",
    "\n",
    "        # Step 2: Create PyG Data object\n",
    "        data = Data(x=cnn_features, edge_index=edge_index, pos=normal_coords)\n",
    "        print(\"finished PyG\")\n",
    "\n",
    "        # Step 3: GraphSAGE layer to learn spatial context-aware features\n",
    "        graphsage_output = self.graphsage_model(data)\n",
    "        print(\"finished GraphSAGE\")\n",
    "\n",
    "        # Step 4: Attention mechanism to focus on the relevant features for cell-type prediction\n",
    "        attention_output = self.attention_head(graphsage_output.unsqueeze(0))  # Add batch dimension\n",
    "        return attention_output.squeeze(0)  # Remove batch dimension for final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9d3692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# üß† Ë®ìÁ∑¥‰∏ÄÂÄã epoch\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in pbar:\n",
    "        \n",
    "        S_tile, M_tile, L_tile = batch[0]['S_tile'].to(device), batch[0]['M_tile'].to(device), batch[0]['L_tile'].to(device)\n",
    "        label = batch[0]['label'].to(device)\n",
    "        edge_index = batch[1]['edge_index'].to(device)\n",
    "        normal_coords = batch[0]['normal_coord'].to(device)\n",
    "        # ÂÑ™ÂåñÂô®Ê∏ÖÁ©∫Ê¢ØÂ∫¶\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ÂâçÂêëÂÇ≥Êí≠ÔºöË®àÁÆóÈ†êÊ∏¨ÁµêÊûú\n",
    "        out = model(S_tile, M_tile, L_tile, edge_index, normal_coords)\n",
    "\n",
    "        # Ë®àÁÆóÊêçÂ§±\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()  # ÂèçÂêëÂÇ≥Êí≠\n",
    "        optimizer.step()  # Êõ¥Êñ∞Ê®°ÂûãÂèÉÊï∏\n",
    "\n",
    "        total_loss += loss.item() * S_tile.size(0)\n",
    "        avg_loss = total_loss / ((pbar.n + 1) * dataloader.batch_size)\n",
    "        pbar.set_postfix(loss=loss.item(), avg=avg_loss)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "# üìè È©óË≠âÊ®°Âûã\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # Áç≤ÂèñÈ©óË≠âÊï∏Êìö\n",
    "            S_tile, M_tile, L_tile = batch[0]['S_tile'].to(device), batch[0]['M_tile'].to(device), batch[0]['L_tile'].to(device)\n",
    "            label = batch[0]['label'].to(device)\n",
    "            edge_index = batch[1]['edge_index'].to(device)\n",
    "            normal_coords = batch[0]['normal_coord'].to(device)\n",
    "\n",
    "            # ÂâçÂêëÂÇ≥Êí≠ÔºöË®àÁÆóÈ†êÊ∏¨ÁµêÊûú\n",
    "            out = model(S_tile, M_tile, L_tile, edge_index, normal_coords)\n",
    "            loss = loss_fn(out, label)\n",
    "\n",
    "            total_loss += loss.item() * S_tile.size(0)\n",
    "            preds.append(out.cpu())\n",
    "            targets.append(label.cpu())\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    targets = torch.cat(targets).numpy()\n",
    "\n",
    "    # ‰ΩøÁî® Spearman Áõ∏ÈóúÊÄß‰æÜË®àÁÆóÈ†êÊ∏¨ÁöÑÊ∫ñÁ¢∫ÊÄß\n",
    "    scores = [spearmanr(preds[:, i], targets[:, i])[0] for i in range(preds.shape[1])]\n",
    "    spearman_avg = np.nanmean(scores)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), spearman_avg\n",
    "\n",
    "\n",
    "\n",
    "    # ‚úÖ Spearman correlation for each gene\n",
    "    scores = [spearmanr(preds[:, i], targets[:, i])[0] for i in range(preds.shape[1])]\n",
    "    spearman_avg = np.nanmean(scores)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), spearman_avg\n",
    "\n",
    "# üîÆ È†êÊ∏¨\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_meta = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Áç≤ÂèñÊ∏¨Ë©¶Êï∏Êìö\n",
    "            S_tile, M_tile, L_tile = batch[0]['S_tile'].to(device), batch[0]['M_tile'].to(device), batch[0]['L_tile'].to(device)\n",
    "            edge_index = batch[1]['edge_index'].to(device)\n",
    "            normal_coords = batch[0]['normal_coord'].to(device)\n",
    "\n",
    "            # È†êÊ∏¨\n",
    "            out = model(S_tile, M_tile, L_tile, edge_index, normal_coords)\n",
    "            all_preds.append(out.cpu())\n",
    "            all_meta.extend(batch['meta'])  # Áî®‰æÜÂ≠òÂÑ≤Ê∏¨Ë©¶ÈõÜÁöÑmeta‰ø°ÊÅØ\n",
    "\n",
    "    return torch.cat(all_preds).numpy(), all_meta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b268",
   "metadata": {},
   "source": [
    "# callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6e2ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None or val_loss < self.best_score:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(val_losses, label=\"Val Loss\", marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.show()\n",
    "# Êî∂ÈõÜË≥áÊñô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8788f8e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Found indices in 'edge_index' that are larger than 31 (got 3985). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 32) in your node feature matrix and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:271\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Ë®≠ÂÆö evaluation Ê®°Âºè\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 40\u001b[0m     output_GraphSAGEModel \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphSAGE Model Output Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_GraphSAGEModel\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m, in \u001b[0;36mGraphSAGEModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# data ÁÇ∫ PyG ÁöÑ Data Áâ©‰ª∂ÔºåÂåÖÂê´ x Âíå edge_index\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraphsage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# GraphSAGE Ë®àÁÆóÁØÄÈªûË°®Á§∫\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# ‰ΩøÁî®ÂÖ®Â±ÄÊ±†ÂåñÂ∞áÊØèÂÄãÂúñ‰∏≠ÁöÑÁØÄÈªûË°®Á§∫ËÅöÂêàÊàê‰∏ÄÂÄãÂúñË°®Á§∫\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m global_mean_pool(x, data\u001b[38;5;241m.\u001b[39mbatch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/nn/models/basic_gnn.py:256\u001b[0m, in \u001b[0;36mBasicGNN.forward\u001b[0;34m(self, x, edge_index, edge_weight, edge_attr, batch, batch_size, num_sampled_nodes_per_hop, num_sampled_edges_per_hop)\u001b[0m\n\u001b[1;32m    254\u001b[0m     x \u001b[38;5;241m=\u001b[39m conv(x, edge_index, edge_attr\u001b[38;5;241m=\u001b[39medge_attr)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjk_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_first:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/nn/conv/sage_conv.py:134\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    131\u001b[0m     x \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mrelu(), x[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_l(out)\n\u001b[1;32m    137\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/torch_geometric.nn.conv.sage_conv_SAGEConv_propagate_6m6vnx5m.py:173\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, size)\u001b[0m\n\u001b[1;32m    167\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    168\u001b[0m         out,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutable_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# Begin Message Forward Pre Hook #######################################\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/torch_geometric.nn.conv.sage_conv_SAGEConv_propagate_6m6vnx5m.py:83\u001b[0m, in \u001b[0;36mcollect\u001b[0;34m(self, edge_index, x, size)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_x_0, Tensor):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_size(size, \u001b[38;5;241m0\u001b[39m, _x_0)\n\u001b[0;32m---> 83\u001b[0m     x_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     x_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:267\u001b[0m, in \u001b[0;36mMessagePassing._index_select\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_select_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:282\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound negative indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)):\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m that are larger \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mIndexError\u001b[0m: Found indices in 'edge_index' that are larger than 31 (got 3985). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 32) in your node feature matrix and try again."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GraphSAGE, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        # ÂàùÂßãÂåñ GraphSAGE Â±§\n",
    "        self.graphsage = GraphSAGE(input_dim, hidden_dim, num_layers, aggr='mean')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # Áî®ÊñºÈ†êÊ∏¨ 35 ÂÄãÁ¥∞ËÉûÈ°ûÂûã\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data ÁÇ∫ PyG ÁöÑ Data Áâ©‰ª∂ÔºåÂåÖÂê´ x Âíå edge_index\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.graphsage(x, edge_index)  # GraphSAGE Ë®àÁÆóÁØÄÈªûË°®Á§∫\n",
    "        # ‰ΩøÁî®ÂÖ®Â±ÄÊ±†ÂåñÂ∞áÊØèÂÄãÂúñ‰∏≠ÁöÑÁØÄÈªûË°®Á§∫ËÅöÂêàÊàê‰∏ÄÂÄãÂúñË°®Á§∫\n",
    "        x = global_mean_pool(x, data.batch) if hasattr(data, 'batch') else x.mean(dim=0, keepdim=True)\n",
    "        return self.fc(x)  # Á∂ìÈÅéÂÖ®ÈÄ£Êé•Â±§Ëº∏Âá∫ÊúÄÁµÇÈ†êÊ∏¨\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ÂâµÂª∫ MultiScaleCNN Ê®°ÂûãÂØ¶‰æã\n",
    "model = MultiScaleCNN().to(device)\n",
    "\n",
    "# ÊßãÈÄ†ÂÅáÊï∏Êìö\n",
    "# ÂÅáË®≠ÊØèÂÄã tile ÁöÑÂ∞∫ÂØ∏ÊòØ 64 x 64Ôºå‰∏¶‰∏îÁÇ∫ RGBÔºà3 ÈÄöÈÅìÔºâ\n",
    "# ÊàëÂÄëÂèØ‰ª•ÊßãÈÄ†‰∏ÄÂÄã batchÔºåÂÅáË®≠ batch_size = 8\n",
    "\n",
    "edge_index = batch[1]['edge_index'].to(device)\n",
    "normal_coords = batch[0]['normal_coord'].to(device)\n",
    "data = Data(x=output, edge_index=edge_index, pos=normal_coords)\n",
    "input_dim = 128  # ÊØèÂÄãÁØÄÈªûÁöÑÁâπÂæµÁ∂≠Â∫¶\n",
    "hidden_dim = 64\n",
    "output_dim = 35  # ÊúÄÁµÇÈ†êÊ∏¨ 35 ÂÄãÁ¥∞ËÉûÈ°ûÂûã\n",
    "model = GraphSAGEModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=1).to(device)\n",
    "\n",
    "model.eval()  # Ë®≠ÂÆö evaluation Ê®°Âºè\n",
    "with torch.no_grad():\n",
    "    output_GraphSAGEModel = model(data)\n",
    "\n",
    "print(\"GraphSAGE Model Output Shape:\", output_GraphSAGEModel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "483d11f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 128, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c9b4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 118] at entry 0 and [2, 270] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 41\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     val_loss, val_spearman \u001b[38;5;241m=\u001b[39m evaluate(model, combined_val_loader, loss_fn, device)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# ‚úÖ ÂÑ≤Â≠òÊúÄÂ•ΩÁöÑÊ®°Âûã\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     14\u001b[0m     S_tile, M_tile, L_tile \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS_tile\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM_tile\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL_tile\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m     label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py:43\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Mapping):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[38;5;28mself\u001b[39m([data[key] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py:43\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Mapping):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py:33\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(\n\u001b[1;32m     28\u001b[0m         batch,\n\u001b[1;32m     29\u001b[0m         follow_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_batch,\n\u001b[1;32m     30\u001b[0m         exclude_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_keys,\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, TensorFrame):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_frame\u001b[38;5;241m.\u001b[39mcat(batch, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 118] at entry 0 and [2, 270] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# üîß Ë®≠ÂÆöË£ùÁΩÆ\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "# üîß ÂàùÂßãÂåñÊ®°Âûã & ÂÑ™ÂåñÂô®\n",
    "# üîß ÂàùÂßãÂåñÊ®°Âûã & ÂÑ™ÂåñÂô®\n",
    "cnn_backbone = MultiScaleCNN()  # ÈÄôÊòØ‰Ω†ÂÆöÁæ©ÁöÑÂ§öÂ∞∫Â∫¶CNN\n",
    "graphsage_model = GraphSAGEModel(input_dim=1024, hidden_dim=512, output_dim=35, num_layers=1)  # ÂÖ©Â±§ GraphSAGE\n",
    "attention_head = MultiLayerAttentionHead(input_dim=256, output_dim=35)  # ÂÅáË®≠ÁöÑ AttentionHead\n",
    "\n",
    "model = HEVisium(cnn_backbone, graphsage_model, attention_head).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()  # ÊàñËÄÖ‰ΩøÁî®ÂÖ∂‰ªñÊêçÂ§±ÂáΩÊï∏Ôºå‰æãÂ¶Ç spearman_loss\n",
    "#loss_fn = spearman_loss\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# üîß ÂÑ≤Â≠ò log ÁöÑË®≠ÂÆö\n",
    "log_file = open(\"training_log.csv\", mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# üîß Áî®‰æÜÁï´Âúñ\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "# üîÅ ÈñãÂßãË®ìÁ∑¥\n",
    "num_epochs = 150\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, combined_train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_spearman = evaluate(model, combined_val_loader, loss_fn, device)\n",
    "\n",
    "    # ‚úÖ ÂÑ≤Â≠òÊúÄÂ•ΩÁöÑÊ®°Âûã\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"‚úÖ Saved best model!\")\n",
    "\n",
    "    # ‚úÖ Ë™øÊï¥Â≠∏ÁøíÁéá\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # ‚úÖ ÂØ´ÂÖ• CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch+1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # ‚úÖ Âç∞ epoch ÁµêÊûú\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | œÅ: {val_spearman:.4f} | lr: {lr:.2e}\")\n",
    "\n",
    "    # ‚úÖ Êõ¥Êñ∞ loss list ‰∏¶Áï´Âúñ\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    plot_losses(train_losses, val_losses)\n",
    "\n",
    "    # ‚úÖ Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"‚õî Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# ‚úÖ ÈóúÈñâ log Ê™îÊ°à\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1478ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'abc.DataBatch'>\n",
      "‚úÖ This batch is a Data object!\n",
      "Batch keys: ['L_tile', 'label', 'S_tile', 'ptr', 'M_tile', 'edge_index', 'meta', 'normal_coord', 'batch']\n",
      "S_tile shape: torch.Size([96, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Ê∏¨Ë©¶ train_loader ÊòØÂê¶ËøîÂõûÊ≠£Á¢∫Ê†ºÂºèÁöÑÊï∏Êìö\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch type: {type(batch)}\")\n",
    "    if isinstance(batch, Data):\n",
    "        print(\"‚úÖ This batch is a Data object!\")\n",
    "        print(f\"Batch keys: {batch.keys()}\")\n",
    "        \n",
    "        # ÊâìÂç∞ S_tile ÁöÑ shape ‰æÜÊ™¢Êü•\n",
    "        if 'S_tile' in batch:\n",
    "            print(f\"S_tile shape: {batch['S_tile'].shape}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è S_tile not found in batch!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è This batch is not a Data object\")\n",
    "    break  # Âè™Ê™¢Êü•Á¨¨‰∏ÄÂÄãÊâπÊ¨°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f55274",
   "metadata": {},
   "source": [
    "ÔºÉ# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7220265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/3135847424.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionMLPModelWithCoord(\n",
       "  (encoder_spot): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_subtiles): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_neighbors): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): MLPDecoder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=194, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=35, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== ÈúÄË¶ÅÁöÑ Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "\n",
    "# ===== ËºâÂÖ•Ë®ìÁ∑¥Â•ΩÁöÑÊ®°ÂûãÊ¨äÈáç =====\n",
    "from hevisum_model import HEVisumModel\n",
    "from hevisum_dataset import importDataset\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "model = VisionMLPModelWithCoord().to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77d22647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/1343659088.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"test_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "# Ê≠£Á¢∫ÊñπÂºè\n",
    "test_data = torch.load(\"test_dataset.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "# ====== Step 2: Âª∫Á´ã Dataset ======\n",
    "test_dataset = importDataset(\n",
    "    center_tile=test_data['tiles'],\n",
    "    subtiles=test_data['subtiles'],\n",
    "    neighbor_tiles=test_data['neighbor_tiles'],\n",
    "    label=np.zeros((len(test_data['tiles']), 35)),  # dummy label\n",
    "    meta=normalized_coords\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea4d8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "test_preds, test_meta = predict(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ddbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.25503415, 0.13337179, 0.1524582 , ..., 0.04139816, 0.01808124,\n",
       "        0.03039141],\n",
       "       ...,\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.23385419, 0.13244098, 0.13941698, ..., 0.04153137, 0.01910294,\n",
       "        0.03005139]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f26de3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== ËÆÄÂèñ test spot index Áî®ÊñºÂ∞çÊáâ ID ====\n",
    "with h5py.File(\"./elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "\n",
    "ensemble_df = pd.DataFrame(test_preds, columns=[f\"C{i+1}\" for i in range(test_preds.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"‚úÖ Saved submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
