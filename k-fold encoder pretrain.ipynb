{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable / total params = 76,451 / 1,130,787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from python_scripts.pretrain_model import PretrainedEncoderRegressor\n",
    "import torch.nn as nn\n",
    "\n",
    "name = 'AE_Center_noaug'\n",
    "\n",
    "checkpoint_path = f\"AE_model/{name}/best.pt\"\n",
    "\n",
    "# 1) ÂÆû‰æãÂåñÔºà‰ºöËá™Âä®Âä†ËΩΩÂπ∂ÂÜªÁªì encoderÔºâ\n",
    "model = PretrainedEncoderRegressor(\n",
    "    ae_checkpoint=checkpoint_path,\n",
    "    ae_type=\"center\",\n",
    "    center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "    tile_size=26, output_dim=35,\n",
    "    freeze_encoder = True\n",
    ")\n",
    "\n",
    "# 2) monkey‚Äêpatch ‰∏Ä‰∏™Êñ∞ÁöÑ head\n",
    "model.decoder  = nn.Sequential(\n",
    "    nn.Linear(64+64, 256),\n",
    "    nn.SiLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.SiLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.SiLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(64, 35)\n",
    "    \n",
    ")\n",
    "\n",
    "# 3) ÂÜçÊ¨°Ê£ÄÊü•Âè™ËÆ≠ÁªÉ head\n",
    "# for name, p in model.named_parameters():\n",
    "#     print(name, p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# ‚Äî‚Äî 5) Á°Æ‰øùÂè™Êúâ decoder ÂèØËÆ≠ÁªÉ ‚Äî‚Äî  \n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable / total params = {trainable:,} / {total:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same in multiple .pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/import_data.py:285: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(os.path.join(folder_path, fname), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded keys: dict_keys(['tile', 'label', 'subtiles', 'source_idx', 'slide_idx'])\n",
      "Samples: 8348\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import inspect\n",
    "from python_scripts.import_data import load_all_tile_data\n",
    "\n",
    "# Áî®Ê≥ïÁØÑ‰æã\n",
    "#folder = \"dataset/spot-rank/version-3/only_tile_sub/original_train\"\n",
    "folder = \"dataset/spot-rank/filtered_directly_rank/masked/realign/Macenko_masked/filtered/train_data/\"\n",
    "\n",
    "grouped_data = load_all_tile_data( \n",
    "        folder_path=folder,\n",
    "        model=model,\n",
    "        fraction=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # grouped_data ÁèæÂú®Âè™ÊúÉÊúâ model.forward() ÈúÄË¶ÅÁöÑ keyÔºå\n",
    "    # ÂÉè ['tile','subtiles','neighbors','norm_coord','node_feat','adj_list','edge_feat','label','source_idx']\n",
    "print(\"Loaded keys:\", grouped_data.keys())\n",
    "print(\"Samples:\", len(next(iter(grouped_data.values()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from python_scripts.operate_model import train_one_epoch, evaluate, predict, plot_losses, plot_per_cell_metrics,spear_EarlyStopping\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------------------\n",
    "# ÊåáÂÆöÂÑ≤Â≠òË≥áÊñôÂ§æ\n",
    "# ---------------------------\n",
    "save_folder = f\"/Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/{name}/filtered_directly_rank/k-fold/realign_all/Macenko_masked/\"  # ‰øÆÊîπÁÇ∫‰Ω†ÊÉ≥Ë¶ÅÁöÑË≥áÊñôÂ§æÂêçÁ®±\n",
    "if not os.path.exists(save_folder):   \n",
    "    os.makedirs(save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 2) Âæû grouped_data ÂèñÂá∫ slide_idxÔºåËΩâÊàê numpy\n",
    "# --------------------------------------------\n",
    "import numpy as np\n",
    "slide_idx = np.array(grouped_data['slide_idx'])   # shape (N,)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 3) Âª∫Á´ã LOGOÔºàÊàñÊîπÊàê GroupKFoldÔºâ\n",
    "# --------------------------------------------\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# X ÂèØ‰ª•Áµ¶ËôõÊì¨Áü©Èô£ÔºåÂõ†ÁÇ∫ÂàÜÁµÑÂè™Èù† groups\n",
    "X_dummy = np.zeros(len(slide_idx))\n",
    "X_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.6 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ device: mps\n",
      "Starting fold 0\n",
      "Starting subseting...\n",
      "Starting augmenting...\n",
      "Starting importDataset...\n",
      "Starting DataLoader...\n",
      "Starting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "*** Unfreezing encoder at epoch 0 and adding to optimizer ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|‚ñà‚ñè        | 72/577 [00:13<00:55,  9.16it/s, avg=221, loss=106] "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.aug         import augment_grouped_data, identity, subset_grouped_data\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "# ---------------- Âü∫Êú¨ÂèÉÊï∏ ----------------\n",
    "start_fold = 0   # ‰ªéÁ¨¨Âá†‰∏™ fold ÂºÄÂßãËÆ≠ÁªÉÔºà0-basedÔºâÔºåÂ∞è‰∫éÊ≠§ÂÄºÁöÑ fold ‰ºöË∑≥Ëøá\n",
    "\n",
    "BATCH_SIZE   = 64\n",
    "num_epochs   = 150\n",
    "repeats      = 5\n",
    "# ---------------- ÂèÉÊï∏Ë®≠ÂÆö ----------------\n",
    "save_root    = save_folder          # ÊúÄÂ§ñÂ±§\n",
    "os.makedirs(save_root, exist_ok=True)\n",
    "\n",
    "# Seed once at the very top for reproducibility\n",
    "\n",
    "logo     = LeaveOneGroupOut()\n",
    "device   = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"‚úÖ device:\", device)\n",
    "\n",
    "overall_best = []\n",
    "# Âú®‰Ω†ÁöÑËÑöÊú¨Êñá‰ª∂Â§¥ÈÉ®\n",
    "\n",
    "\n",
    "\n",
    "# ‚ûã LOGO / GroupKFold ÂàáÂàÜÁ¥¢ÂºïÔºåÂ∞ç Dataset Âª∫ Subset\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "        logo.split(X=np.zeros(len(slide_idx)), y=None, groups=slide_idx)):\n",
    "    \n",
    "        # Â¶ÇÊûúÂΩìÂâç fold < start_foldÔºåÂ∞±Ë∑≥Ëøá\n",
    "    if fold_id < start_fold:\n",
    "        print(f\"‚è≠Ô∏è Skipping fold {fold_id}\")\n",
    "        continue\n",
    "    \n",
    "    print(\"Starting fold\", fold_id)\n",
    "    print(\"Starting subseting...\")\n",
    "\n",
    "    train_base = subset_grouped_data(grouped_data, tr_idx)\n",
    "    print(\"Starting augmenting...\")\n",
    "\n",
    "    train_ds = augment_grouped_data(\n",
    "                    grouped_data=train_base,\n",
    "                    image_keys=['tile','subtiles'],\n",
    "                    repeats=repeats   # ÊØîÂ¶ÇÂØπÊØèÂº†ÂÅö 2 Ê¨°Â¢ûÂº∫\n",
    "                )\n",
    "    print(\"Starting importDataset...\")\n",
    "    # 1) ÂéüÂßã dataset\n",
    "    train_ds = importDataset(train_ds, model,\n",
    "                            image_keys=['tile','subtiles'],\n",
    "                            transform=identity)\n",
    "\n",
    "\n",
    "    # 3) validation raw\n",
    "    val_ds     = subset_grouped_data(grouped_data, va_idx)\n",
    "    val_ds     = importDataset(val_ds, model,\n",
    "                            image_keys=['tile','subtiles'],\n",
    "                            transform=identity)\n",
    "\n",
    "    print(\"Starting DataLoader...\")\n",
    "    # 4) DataLoader ‰∏çÂÜçÂãïÊÖãÂ¢ûÂº∑\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=0, pin_memory=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "    # 5) ‰∏ÄÂàáÂÖ∂È§òÊ≠•È©üÂêå‰πãÂâçÔºöÂÆöÊ®°Âûã„ÄÅoptimizer„ÄÅtrain_one_epoch„ÄÅevaluate‚Ä¶‚Ä¶\n",
    "\n",
    "    print(\"Starting model...\")\n",
    "    # ----- Êñ∞Âª∫Ê®°Âûã / ÂÑ™ÂåñÂô® -----\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"center\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = True\n",
    "    )\n",
    "\n",
    "    # 2) monkey‚Äêpatch ‰∏Ä‰∏™Êñ∞ÁöÑ head\n",
    "    net.decoder  = nn.Sequential(\n",
    "        nn.Linear(64+64, 256),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "        \n",
    "    )\n",
    "    net = net.to(device)\n",
    "\n",
    "    # Êàë‰ª¨ÊääÂéüÊù• Adam Êç¢Êàê AdamWÔºåÁ®çÂæÆÂä†‰∏ÄÁÇπ weight decay\n",
    "    peak_lr       = 5e-4\n",
    "    min_lr        = 1e-6\n",
    "    warmup_epochs = 0\n",
    "    total_epochs  = 30\n",
    "\n",
    "    unfreeze_epoch = 0  # ÊØîÂ¶ÇÂú®Á¨¨ 50 ‰∏™ epoch Ëß£ÂÜª encoder\n",
    "    encoder_lr     = 1e-5\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, net.parameters()),\n",
    "        lr=peak_lr, weight_decay=1e-3\n",
    "    )\n",
    "\n",
    "    # ÂÆö‰πâ lr_lambda\n",
    "    def lr_lambda(cur_epoch):\n",
    "        if cur_epoch < warmup_epochs:\n",
    "            # Á∫øÊÄß warm-up: ‰ªé 0 ‚Üí 1\n",
    "            return float(cur_epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            # ‰ΩôÂº¶ÈÄÄÁÅ´Ôºö‰ªé 1 ‚Üí min_lr/peak_lr\n",
    "            progress = (cur_epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "            # cos(0)=1 ‚Üí cos(pi)=‚àí1, remap to [min_ratio,1]\n",
    "            min_ratio = min_lr / peak_lr\n",
    "            return min_ratio + 0.5 * (1 - min_ratio) * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    stopper = spear_EarlyStopping(patience=15)\n",
    "\n",
    "    # ----- fold Â∞àÂ±¨Ëº∏Âá∫Ë∑ØÂæë -----\n",
    "    fold_dir  = os.path.join(save_folder, f\"fold{fold_id}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(fold_dir, \"best_model.pt\")\n",
    "    loss_plot_path  = os.path.join(fold_dir, \"loss_curve.png\")\n",
    "    csv_path        = os.path.join(fold_dir, \"training_log.csv\")\n",
    "\n",
    "    # ----- CSV log -----\n",
    "    log_f = open(csv_path, \"w\", newline=\"\")\n",
    "    csv_w = csv.writer(log_f)\n",
    "    csv_w.writerow([\"Epoch\",\"TrainLoss\",\"ValLoss\",\"ValSpearman\",\"LR\"])\n",
    "\n",
    "    # ----- ÂúñÂΩ¢ -----\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "    display(fig)\n",
    "\n",
    "    train_losses = []; val_losses = []\n",
    "    train_rhos   = []; val_rhos   = []\n",
    "\n",
    "    best_rho = -1.0\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch == unfreeze_epoch:\n",
    "            print(f\"*** Unfreezing encoder at epoch {epoch} and adding to optimizer ***\")\n",
    "        # Êää encoder ÁöÑÂèÇÊï∞Ëß£ÂÜªÂπ∂Âä†ÂÖ• optimizer\n",
    "            net.unfreeze_encoder(lr=encoder_lr, optimizer=optimizer)\n",
    "\n",
    "        tloss, trho = train_one_epoch(\n",
    "            net, train_loader, optimizer, device,\n",
    "            current_epoch=epoch, initial_alpha=0, final_alpha=0, target_epoch=20 )\n",
    "\n",
    "        vloss, vrho, mse_cell, rho_cell = evaluate(\n",
    "            net, val_loader, device,\n",
    "            current_epoch=epoch, initial_alpha=0, final_alpha=0, target_epoch=20 )\n",
    "\n",
    "        clear_output(wait=True)  # Ê∏ÖÈô§‰πãÂâçÁöÑËº∏Âá∫\n",
    "        axes[0][0].clear()\n",
    "        axes[0][1].clear()\n",
    "        axes[1][0].clear()\n",
    "        axes[1][1].clear()\n",
    "        # --- save best ---\n",
    "        if vrho > best_rho:\n",
    "            best_rho = vrho\n",
    "            torch.save(net.state_dict(), best_model_path)\n",
    "            print(f\"‚úÖ Saved best model in {best_model_path}!\")\n",
    "\n",
    "        # --- scheduler / early stop ---\n",
    "        scheduler.step()\n",
    "        stopper(vrho)\n",
    "\n",
    "\n",
    "    \n",
    "        # --- logging ---\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        csv_w.writerow([epoch+1, tloss, vloss, vrho, lr])\n",
    "\n",
    "        train_losses.append(tloss); val_losses.append(vloss)\n",
    "        train_rhos.append(trho);   val_rhos.append(vrho)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # Âç∞Âá∫ Epoch ÁµêÊûú\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"[Epoch {epoch+1}]  lr={lr:.2e}, train_loss={tloss:.4f}, val_loss={vloss:.4f}\")\n",
    "        print(f\"train spearman: {trho:.4f} | Val spearman: {vrho:.4f} | best: {best_rho:.4f}\")\n",
    "        # --- update plots ---\n",
    "        plot_losses(train_losses, val_losses, axes[0][0], \"MSE Loss\")\n",
    "        plot_losses(train_rhos,   val_rhos,   axes[0][1], \"Spearman\")\n",
    "        cell_names = [f\"C{i+1}\" for i in range(35)]\n",
    "        plot_per_cell_metrics(mse_cell, rho_cell, cell_names,\n",
    "                              ax_mse=axes[1][0], ax_spearman=axes[1][1])\n",
    "        plt.tight_layout(); display(fig); plt.pause(0.1)\n",
    "        fig.savefig(loss_plot_path)\n",
    "        print(f\"Êõ≤Á∑öÂúñÂ∑≤ÂÑ≤Â≠òËá≥ {loss_plot_path}\")\n",
    "        if stopper.early_stop:\n",
    "            print(\"‚õî early stop\"); break\n",
    "\n",
    "    log_f.close(); plt.close(fig)\n",
    "    overall_best.append(best_rho) \n",
    "    print(f\"üìà Fold {fold_id} best œÅ = {best_rho:.4f}\")\n",
    "\n",
    "# ========= Êï¥È´îÁµêÊûú =========\n",
    "overall_best = np.array(overall_best)\n",
    "print(\"\\n=========== CV summary ===========\")\n",
    "for i, r in enumerate(overall_best):\n",
    "    print(f\"fold {i}: best œÅ = {r:.4f}\")\n",
    "print(f\"overall best (mean) œÅ = {overall_best.mean():.4f} ¬± {overall_best.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = importDataset(grouped_data, model,\n",
    "                             image_keys=['tile','subtiles'],\n",
    "                             transform=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/fold0/best_model.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_9016/1117153455.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: OOF preds shape (2197, 35)\n",
      "Loading model from /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/fold1/best_model.pt...\n",
      "Fold 1: OOF preds shape (2269, 35)\n",
      "Loading model from /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/fold2/best_model.pt...\n",
      "Fold 2: OOF preds shape (690, 35)\n",
      "Loading model from /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/fold3/best_model.pt...\n",
      "Fold 3: OOF preds shape (1187, 35)\n",
      "Loading model from /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/fold4/best_model.pt...\n",
      "Fold 4: OOF preds shape (1677, 35)\n",
      "Loading model from /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/fold5/best_model.pt...\n",
      "Fold 5: OOF preds shape (328, 35)\n",
      "Meta feature shape: (6678, 35)\n",
      "Feature std (min/max): 1.309504 8.708129\n",
      "Training LightGBM on OOF meta-features with early stopping...\n",
      "Training target 0...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 5.47029\n",
      "[200]\tvalid_0's rmse: 5.32908\n",
      "[300]\tvalid_0's rmse: 5.31855\n",
      "[400]\tvalid_0's rmse: 5.31383\n",
      "[500]\tvalid_0's rmse: 5.31135\n",
      "Early stopping, best iteration is:\n",
      "[483]\tvalid_0's rmse: 5.30736\n",
      "Training target 1...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 2.66173\n",
      "[200]\tvalid_0's rmse: 2.61135\n",
      "[300]\tvalid_0's rmse: 2.60291\n",
      "[400]\tvalid_0's rmse: 2.59721\n",
      "[500]\tvalid_0's rmse: 2.5952\n",
      "[600]\tvalid_0's rmse: 2.59277\n",
      "Early stopping, best iteration is:\n",
      "[532]\tvalid_0's rmse: 2.5924\n",
      "Training target 2...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 3.74502\n",
      "[200]\tvalid_0's rmse: 3.62813\n",
      "[300]\tvalid_0's rmse: 3.60572\n",
      "[400]\tvalid_0's rmse: 3.60183\n",
      "[500]\tvalid_0's rmse: 3.59874\n",
      "Early stopping, best iteration is:\n",
      "[473]\tvalid_0's rmse: 3.59646\n",
      "Training target 3...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.58123\n",
      "[200]\tvalid_0's rmse: 7.28106\n",
      "[300]\tvalid_0's rmse: 7.21505\n",
      "[400]\tvalid_0's rmse: 7.19595\n",
      "[500]\tvalid_0's rmse: 7.18169\n",
      "[600]\tvalid_0's rmse: 7.17412\n",
      "[700]\tvalid_0's rmse: 7.16727\n",
      "[800]\tvalid_0's rmse: 7.16538\n",
      "Early stopping, best iteration is:\n",
      "[784]\tvalid_0's rmse: 7.16109\n",
      "Training target 4...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.15958\n",
      "[200]\tvalid_0's rmse: 6.9889\n",
      "[300]\tvalid_0's rmse: 6.97651\n",
      "[400]\tvalid_0's rmse: 6.9731\n",
      "Early stopping, best iteration is:\n",
      "[374]\tvalid_0's rmse: 6.963\n",
      "Training target 5...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 8.24307\n",
      "[200]\tvalid_0's rmse: 8.07254\n",
      "[300]\tvalid_0's rmse: 8.04747\n",
      "[400]\tvalid_0's rmse: 8.0384\n",
      "Early stopping, best iteration is:\n",
      "[353]\tvalid_0's rmse: 8.03076\n",
      "Training target 6...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 3.8176\n",
      "[200]\tvalid_0's rmse: 3.7714\n",
      "[300]\tvalid_0's rmse: 3.7622\n",
      "[400]\tvalid_0's rmse: 3.75354\n",
      "[500]\tvalid_0's rmse: 3.75724\n",
      "Early stopping, best iteration is:\n",
      "[432]\tvalid_0's rmse: 3.75332\n",
      "Training target 7...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 5.55319\n",
      "[200]\tvalid_0's rmse: 5.55866\n",
      "Early stopping, best iteration is:\n",
      "[149]\tvalid_0's rmse: 5.54754\n",
      "Training target 8...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 8.75568\n",
      "[200]\tvalid_0's rmse: 8.48067\n",
      "[300]\tvalid_0's rmse: 8.41817\n",
      "[400]\tvalid_0's rmse: 8.38604\n",
      "[500]\tvalid_0's rmse: 8.36558\n",
      "[600]\tvalid_0's rmse: 8.34994\n",
      "[700]\tvalid_0's rmse: 8.33538\n",
      "[800]\tvalid_0's rmse: 8.33859\n",
      "Early stopping, best iteration is:\n",
      "[753]\tvalid_0's rmse: 8.33068\n",
      "Training target 9...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 6.96195\n",
      "[200]\tvalid_0's rmse: 6.8303\n",
      "[300]\tvalid_0's rmse: 6.81286\n",
      "[400]\tvalid_0's rmse: 6.80742\n",
      "[500]\tvalid_0's rmse: 6.79882\n",
      "[600]\tvalid_0's rmse: 6.78813\n",
      "[700]\tvalid_0's rmse: 6.78978\n",
      "Early stopping, best iteration is:\n",
      "[607]\tvalid_0's rmse: 6.78653\n",
      "Training target 10...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.28081\n",
      "[200]\tvalid_0's rmse: 7.16607\n",
      "[300]\tvalid_0's rmse: 7.16418\n",
      "Early stopping, best iteration is:\n",
      "[249]\tvalid_0's rmse: 7.15558\n",
      "Training target 11...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 4.05218\n",
      "[200]\tvalid_0's rmse: 3.93869\n",
      "[300]\tvalid_0's rmse: 3.91978\n",
      "[400]\tvalid_0's rmse: 3.91772\n",
      "[500]\tvalid_0's rmse: 3.91529\n",
      "Early stopping, best iteration is:\n",
      "[427]\tvalid_0's rmse: 3.9129\n",
      "Training target 12...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 6.15755\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid_0's rmse: 6.15604\n",
      "Training target 13...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 5.24359\n",
      "[200]\tvalid_0's rmse: 5.21742\n",
      "[300]\tvalid_0's rmse: 5.22388\n",
      "Early stopping, best iteration is:\n",
      "[229]\tvalid_0's rmse: 5.21436\n",
      "Training target 14...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 5.98774\n",
      "[200]\tvalid_0's rmse: 5.81374\n",
      "[300]\tvalid_0's rmse: 5.78525\n",
      "[400]\tvalid_0's rmse: 5.7736\n",
      "[500]\tvalid_0's rmse: 5.78015\n",
      "Early stopping, best iteration is:\n",
      "[412]\tvalid_0's rmse: 5.77214\n",
      "Training target 15...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 6.08664\n",
      "[200]\tvalid_0's rmse: 5.99144\n",
      "[300]\tvalid_0's rmse: 5.98546\n",
      "Early stopping, best iteration is:\n",
      "[265]\tvalid_0's rmse: 5.97808\n",
      "Training target 16...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.12795\n",
      "[200]\tvalid_0's rmse: 6.94002\n",
      "[300]\tvalid_0's rmse: 6.90504\n",
      "[400]\tvalid_0's rmse: 6.89366\n",
      "[500]\tvalid_0's rmse: 6.88885\n",
      "[600]\tvalid_0's rmse: 6.88114\n",
      "Early stopping, best iteration is:\n",
      "[597]\tvalid_0's rmse: 6.88046\n",
      "Training target 17...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 3.54541\n",
      "[200]\tvalid_0's rmse: 3.44198\n",
      "[300]\tvalid_0's rmse: 3.43035\n",
      "[400]\tvalid_0's rmse: 3.43131\n",
      "Early stopping, best iteration is:\n",
      "[359]\tvalid_0's rmse: 3.42851\n",
      "Training target 18...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.42544\n",
      "[200]\tvalid_0's rmse: 7.3516\n",
      "[300]\tvalid_0's rmse: 7.344\n",
      "Early stopping, best iteration is:\n",
      "[247]\tvalid_0's rmse: 7.34069\n",
      "Training target 19...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 5.03285\n",
      "[200]\tvalid_0's rmse: 4.9722\n",
      "[300]\tvalid_0's rmse: 4.96485\n",
      "[400]\tvalid_0's rmse: 4.9606\n",
      "[500]\tvalid_0's rmse: 4.95886\n",
      "[600]\tvalid_0's rmse: 4.95648\n",
      "Early stopping, best iteration is:\n",
      "[526]\tvalid_0's rmse: 4.95442\n",
      "Training target 20...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 6.74323\n",
      "[200]\tvalid_0's rmse: 6.6815\n",
      "[300]\tvalid_0's rmse: 6.68225\n",
      "[400]\tvalid_0's rmse: 6.67348\n",
      "[500]\tvalid_0's rmse: 6.67095\n",
      "Early stopping, best iteration is:\n",
      "[490]\tvalid_0's rmse: 6.66786\n",
      "Training target 21...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.67521\n",
      "[200]\tvalid_0's rmse: 7.60654\n",
      "[300]\tvalid_0's rmse: 7.59925\n",
      "[400]\tvalid_0's rmse: 7.60353\n",
      "Early stopping, best iteration is:\n",
      "[319]\tvalid_0's rmse: 7.59126\n",
      "Training target 22...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 2.94514\n",
      "[200]\tvalid_0's rmse: 2.93136\n",
      "[300]\tvalid_0's rmse: 2.93332\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's rmse: 2.93087\n",
      "Training target 23...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 5.96583\n",
      "[200]\tvalid_0's rmse: 5.86261\n",
      "[300]\tvalid_0's rmse: 5.85678\n",
      "Early stopping, best iteration is:\n",
      "[245]\tvalid_0's rmse: 5.85388\n",
      "Training target 24...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.13498\n",
      "[200]\tvalid_0's rmse: 6.9959\n",
      "[300]\tvalid_0's rmse: 6.97547\n",
      "[400]\tvalid_0's rmse: 6.96517\n",
      "Early stopping, best iteration is:\n",
      "[373]\tvalid_0's rmse: 6.96385\n",
      "Training target 25...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 6.88594\n",
      "[200]\tvalid_0's rmse: 6.85229\n",
      "Early stopping, best iteration is:\n",
      "[196]\tvalid_0's rmse: 6.85017\n",
      "Training target 26...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.94733\n",
      "[200]\tvalid_0's rmse: 7.66581\n",
      "[300]\tvalid_0's rmse: 7.59829\n",
      "[400]\tvalid_0's rmse: 7.57077\n",
      "[500]\tvalid_0's rmse: 7.56468\n",
      "Early stopping, best iteration is:\n",
      "[475]\tvalid_0's rmse: 7.5605\n",
      "Training target 27...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 5.96757\n",
      "[200]\tvalid_0's rmse: 5.93996\n",
      "[300]\tvalid_0's rmse: 5.93979\n",
      "Early stopping, best iteration is:\n",
      "[212]\tvalid_0's rmse: 5.93761\n",
      "Training target 28...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 6.71991\n",
      "[200]\tvalid_0's rmse: 6.66194\n",
      "[300]\tvalid_0's rmse: 6.65243\n",
      "Early stopping, best iteration is:\n",
      "[264]\tvalid_0's rmse: 6.65108\n",
      "Training target 29...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 6.52369\n",
      "[200]\tvalid_0's rmse: 6.35127\n",
      "[300]\tvalid_0's rmse: 6.32458\n",
      "[400]\tvalid_0's rmse: 6.31304\n",
      "[500]\tvalid_0's rmse: 6.3124\n",
      "[600]\tvalid_0's rmse: 6.31082\n",
      "Early stopping, best iteration is:\n",
      "[548]\tvalid_0's rmse: 6.30553\n",
      "Training target 30...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 2.94502\n",
      "[200]\tvalid_0's rmse: 2.90561\n",
      "[300]\tvalid_0's rmse: 2.90171\n",
      "[400]\tvalid_0's rmse: 2.90226\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's rmse: 2.89967\n",
      "Training target 31...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 2.46011\n",
      "[200]\tvalid_0's rmse: 2.45683\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's rmse: 2.45408\n",
      "Training target 32...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.3444\n",
      "[200]\tvalid_0's rmse: 7.33257\n",
      "Early stopping, best iteration is:\n",
      "[162]\tvalid_0's rmse: 7.32807\n",
      "Training target 33...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 5.53992\n",
      "[200]\tvalid_0's rmse: 5.49866\n",
      "Early stopping, best iteration is:\n",
      "[193]\tvalid_0's rmse: 5.49777\n",
      "Training target 34...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 2.94004\n",
      "[200]\tvalid_0's rmse: 2.92548\n",
      "[300]\tvalid_0's rmse: 2.92511\n",
      "Early stopping, best iteration is:\n",
      "[295]\tvalid_0's rmse: 2.92489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_9016/1117153455.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved stacked submission.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import rankdata\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.operate_model import predict\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "\n",
    "# ---------------- Settings ----------------\n",
    "save_root  = save_folder  # your save_folder path\n",
    "n_folds    = len([d for d in os.listdir(save_root) if d.startswith('fold')])\n",
    "n_samples  = len(full_dataset)\n",
    "C          = 35  # num cell types\n",
    "start_fold = 0\n",
    "BATCH_SIZE = 64\n",
    "# If optimizing Spearman, convert labels to ranks\n",
    "use_rank   = False\n",
    "\n",
    "# --- 1) Prepare OOF meta-features ---\n",
    "# Initialize matrix for OOF predictions\n",
    "n_samples = len(full_dataset)\n",
    "oof_preds = np.zeros((n_samples, C), dtype=np.float32)\n",
    "# True labels (raw or rank)\n",
    "# importDataset returns a dict-like sample, so label is under key 'label'\n",
    "y_true = np.vstack([ full_dataset[i]['label'].cpu().numpy() for i in range(n_samples) ])\n",
    "if use_rank:\n",
    "    y_meta = np.apply_along_axis(rankdata, 1, y_true)\n",
    "else:\n",
    "    y_meta = y_true\n",
    "\n",
    "# Build CV splitter (must match first stage splits)\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over folds, load best model, predict on validation indices\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "        logo.split(X=np.zeros(n_samples), y=None, groups=slide_idx)):\n",
    "    # Load model\n",
    "    # if fold_id > start_fold:\n",
    "    #     print(f\"‚è≠Ô∏è Skipping fold {fold_id}\")\n",
    "    #     continue\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    print(f\"Loading model from {ckpt_path}...\")\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"all\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = True\n",
    "    )\n",
    "\n",
    "    # 2) monkey‚Äêpatch ‰∏Ä‰∏™Êñ∞ÁöÑ head\n",
    "    net.decoder  = nn.Sequential(\n",
    "        nn.Linear(64+64, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "        \n",
    "    )\n",
    "    net = net.to(device)    # Alternatively, if your model requires specific args, replace with:\n",
    "    # net = VisionMLP_MultiTask(tile_dim=64, subtile_dim=64, output_dim=35).to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.to(device).eval()\n",
    "\n",
    "    # Predict on validation set\n",
    "    val_ds = Subset(full_dataset, va_idx)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = predict(net, val_loader, device)  # (n_val, C)\n",
    "    oof_preds[va_idx] = preds\n",
    "    print(f\"Fold {fold_id}: OOF preds shape {preds.shape}\")\n",
    "\n",
    "# --- 2) Train LightGBM meta-model ---\n",
    "# Choose objective: regression on rank (for Spearman) or raw (for MSE)\n",
    "# Â∞á meta features ÊãÜÊàêË®ìÁ∑¥ÈõÜËàá early stopping Áî®ÁöÑÈ©óË≠âÈõÜ\n",
    "X_train, X_val, y_train, y_val = train_test_split(oof_preds, y_meta, test_size=0.2, random_state=42)\n",
    "print(\"Meta feature shape:\", X_train.shape)\n",
    "print(\"Feature std (min/max):\", np.min(np.std(X_train, axis=0)), np.max(np.std(X_train, axis=0)))\n",
    "\n",
    "\n",
    "# # Base model\n",
    "# lgb_base = lgb.LGBMRegressor(\n",
    "#     objective='l2',\n",
    "#     metric='rmse',\n",
    "#     n_estimators=12000,\n",
    "#     max_depth=15,\n",
    "#     learning_rate=0.008,\n",
    "#     num_leaves=32,\n",
    "#     colsample_bytree=0.25\n",
    "# )\n",
    "lgb_base = lgb.LGBMRegressor(\n",
    "    objective='l2',\n",
    "    metric='rmse',\n",
    "    n_estimators=3000,         # ÈÖçÂêà early stoppingÔºåË®≠Á®çÂæÆÈ´ò‰∏ÄÈªû\n",
    "    max_depth=15,               # ËºÉÂÆâÂÖ®ÁöÑÊ∑±Â∫¶ÔºåÈÅøÂÖçÊ≤íÊÑèÁæ©ÁöÑÊ∑±Ê®π\n",
    "    learning_rate=0.02,        # Á©©ÂÆöÂ≠∏ÁøíÁéáÔºåÊê≠ÈÖç early stopping\n",
    "    num_leaves=32,             # ÈÅ©ÂêàÁâπÂæµÊï∏ÈáèÁÇ∫ 35 ÁöÑÊÉÖÊ≥Å\n",
    "    colsample_bytree=0.9,      # ‰ΩøÁî®ËºÉÂ§öÁâπÂæµ\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    min_data_in_leaf=20,       # Èò≤Ê≠¢Â∞è leafÔºåÊèêÂçáÊ≥õÂåñ\n",
    "#    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "# Â∞áÊØèÂÄã target ÂàÜÂà• early stopping\n",
    "meta_model = MultiOutputRegressor(lgb_base)\n",
    "\n",
    "print(\"Training LightGBM on OOF meta-features with early stopping...\")\n",
    "meta_model.estimators_ = []\n",
    "\n",
    "for i in range(y_train.shape[1]):\n",
    "    print(f\"Training target {i}...\")\n",
    "    model  = lgb.LGBMRegressor(\n",
    "        objective='l2',\n",
    "        metric='rmse',\n",
    "        n_estimators=2000,         # ÈÖçÂêà early stoppingÔºåË®≠Á®çÂæÆÈ´ò‰∏ÄÈªû\n",
    "        max_depth=7,               # ËºÉÂÆâÂÖ®ÁöÑÊ∑±Â∫¶ÔºåÈÅøÂÖçÊ≤íÊÑèÁæ©ÁöÑÊ∑±Ê®π\n",
    "        learning_rate=0.02,        # Á©©ÂÆöÂ≠∏ÁøíÁéáÔºåÊê≠ÈÖç early stopping\n",
    "        num_leaves=32,             # ÈÅ©ÂêàÁâπÂæµÊï∏ÈáèÁÇ∫ 35 ÁöÑÊÉÖÊ≥Å\n",
    "        colsample_bytree=0.9,      # ‰ΩøÁî®ËºÉÂ§öÁâπÂæµ\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        min_data_in_leaf=20,       # Èò≤Ê≠¢Â∞è leafÔºåÊèêÂçáÊ≥õÂåñ\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train[:, i],\n",
    "        eval_set=[(X_val, y_val[:, i])],\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=100),\n",
    "            log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    meta_model.estimators_.append(model)\n",
    "\n",
    "# ‰øùÂ≠òÊ®°Âûã\n",
    "joblib.dump(meta_model, os.path.join(save_root, 'meta_model.pkl'))\n",
    "\n",
    "\n",
    "# --- 3) Prepare test meta-features ---\n",
    "n_test = len(test_dataset)\n",
    "test_meta = np.zeros((n_test, C), dtype=np.float32)\n",
    "for fold_id in range(n_folds):\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"all\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = True\n",
    "    )\n",
    "\n",
    "    # 2) monkey‚Äêpatch ‰∏Ä‰∏™Êñ∞ÁöÑ head\n",
    "    net.decoder  = nn.Sequential(\n",
    "        nn.Linear(64+64, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "        \n",
    "    )\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.to(device).eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = predict(net, test_loader, device)\n",
    "    test_meta += preds\n",
    "# Average across folds\n",
    "test_meta /= n_folds\n",
    "\n",
    "# --- 4) Meta-model predict ---\n",
    "if use_rank:\n",
    "    final_preds = meta_model.predict(test_meta) / (C + 1)\n",
    "else:\n",
    "    final_preds = meta_model.predict(test_meta)\n",
    "\n",
    "# --- Save submission ---\n",
    "import h5py\n",
    "import pandas as pd\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spot_ids = pd.DataFrame(np.array(f[\"spots/Test\"][\"S_7\"]))\n",
    "sub = pd.DataFrame(final_preds, columns=[f\"C{i+1}\" for i in range(C)])\n",
    "sub.insert(0, 'ID', test_spot_ids.index)\n",
    "sub.to_csv(os.path.join(save_root, 'submission_stacked.csv'), index=False)\n",
    "print(\"‚úÖ Saved stacked submission.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'slide_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m logo \u001b[38;5;241m=\u001b[39m LeaveOneGroupOut()\n\u001b[1;32m     35\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_id, (tr_idx, va_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[0;32m---> 38\u001b[0m         logo\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(n_samples), y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[43mslide_idx\u001b[49m)):\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Â¶ÇÊûúÂΩìÂâç fold ‰∏çÂú®Êàë‰ª¨ÊÉ≥Ë¶ÅÁöÑ meta_folds ÂàóË°®ÈáåÔºåÂ∞±Ë∑≥Ëøá\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fold_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m meta_folds:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚è≠Ô∏è Skipping OOF for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'slide_idx' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import rankdata\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.operate_model import predict\n",
    "\n",
    "# --- ÈÖçÁΩÆ: Âè™Áî®Âì™‰∫õ fold ÁöÑÁªìÊûúÊù•ËÆ≠ÁªÉ/È¢ÑÊµã meta-model ---\n",
    "meta_folds = [0]  # ‰æãÂ¶ÇÂè™Áî® fold0, fold2, fold4\n",
    "\n",
    "# 1) ÂáÜÂ§á full_dataset, slide_idx, test_dataset Á≠â\n",
    "full_dataset = importDataset(\n",
    "    grouped_data, model,\n",
    "    image_keys=['tile','subtiles'],\n",
    "    transform=lambda x: x\n",
    ")\n",
    "n_samples = len(full_dataset)\n",
    "C = 35  # Á±ªÂà´Êï∞\n",
    "\n",
    "# 2) È¢ÑÁïô oof_preds Âíå fold_ids\n",
    "oof_preds    = np.zeros((n_samples, C), dtype=np.float32)\n",
    "oof_fold_ids = np.full(n_samples, -1, dtype=int)\n",
    "\n",
    "# ÁúüÊ†áÁ≠æ\n",
    "y_true = np.vstack([ full_dataset[i]['label'].cpu().numpy() for i in range(n_samples) ])\n",
    "y_meta = y_true.copy()  # ‰∏çÂÅö rank Êó∂Áõ¥Êé•Áî® raw\n",
    "\n",
    "# 3) ÁîüÊàê OOF È¢ÑÊµãÂπ∂ËÆ∞ÂΩï fold id\n",
    "logo = LeaveOneGroupOut()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "        logo.split(X=np.zeros(n_samples), y=None, groups=slide_idx)):\n",
    "\n",
    "    # Â¶ÇÊûúÂΩìÂâç fold ‰∏çÂú®Êàë‰ª¨ÊÉ≥Ë¶ÅÁöÑ meta_folds ÂàóË°®ÈáåÔºåÂ∞±Ë∑≥Ëøá\n",
    "    if fold_id not in meta_folds:\n",
    "        print(f\"‚è≠Ô∏è Skipping OOF for fold {fold_id}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n>>> Generating OOF for fold {fold_id}\")\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"all\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = True\n",
    "    )\n",
    "\n",
    "    # 2) monkey‚Äêpatch ‰∏Ä‰∏™Êñ∞ÁöÑ head\n",
    "    net.decoder  = nn.Sequential(\n",
    "        nn.Linear(64+64, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "        \n",
    "    )\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    val_loader = DataLoader(Subset(full_dataset, va_idx), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = predict(net, val_loader, device)  # (n_val, C)\n",
    "\n",
    "    oof_preds[va_idx]    = preds\n",
    "    oof_fold_ids[va_idx] = fold_id\n",
    "\n",
    "    print(f\"  ‚Üí Fold {fold_id} OOF preds shape: {preds.shape}\")\n",
    "# 4) Âè™ÈÄâÂèñ meta_folds ÁöÑË°åÊù•ËÆ≠ÁªÉ meta-model\n",
    "mask = np.isin(oof_fold_ids, meta_folds)\n",
    "X_meta = oof_preds[mask]\n",
    "y_meta_sub = y_meta[mask]\n",
    "\n",
    "print(f\"\\nTraining meta-model on folds {meta_folds}:\")\n",
    "print(f\"  ‰ΩøÁî®Ê†∑Êú¨Êï∞Ôºö{X_meta.shape[0]} / {n_samples}\")\n",
    "\n",
    "lgb_base = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=1000,\n",
    "    num_leaves=31,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    n_jobs=-1,\n",
    "    force_col_wise=True\n",
    ")\n",
    "meta_model = MultiOutputRegressor(lgb_base)\n",
    "meta_model.fit(X_meta, y_meta_sub)\n",
    "joblib.dump(meta_model, os.path.join(save_root, 'meta_model.pkl'))\n",
    "\n",
    "# 5) ÂáÜÂ§á test_metaÔºåÂè™Âπ≥Âùá meta_folds ‰∏≠ÁöÑÈ¢ÑÊµã\n",
    "n_folds = len([d for d in os.listdir(save_root) if d.startswith('fold')])\n",
    "n_test  = len(test_dataset)\n",
    "test_meta = np.zeros((n_test, C), dtype=np.float32)\n",
    "\n",
    "for fold_id in range(n_folds):\n",
    "    if fold_id not in meta_folds:\n",
    "        continue\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"all\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = True\n",
    "    )\n",
    "\n",
    "    # 2) monkey‚Äêpatch ‰∏Ä‰∏™Êñ∞ÁöÑ head\n",
    "    net.decoder  = nn.Sequential(\n",
    "        nn.Linear(64+64, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "        \n",
    "    )\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = predict(net, loader, device)\n",
    "    test_meta += preds\n",
    "\n",
    "# Âπ≥ÂùáÊó∂Èô§‰ª•ÂèÇ‰∏éÁöÑ folds Êï∞ÁõÆ\n",
    "test_meta /= len(meta_folds)\n",
    "\n",
    "# 6) Áî® meta-model ÂÅöÊúÄÁªàÈ¢ÑÊµã\n",
    "final_preds = meta_model.predict(test_meta)\n",
    "\n",
    "# --- Save submission ---\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spot_ids = pd.DataFrame(np.array(f[\"spots/Test\"][\"S_7\"]))\n",
    "\n",
    "sub = pd.DataFrame(final_preds, columns=[f\"C{i+1}\" for i in range(C)])\n",
    "sub.insert(0, 'ID', test_spot_ids.index)\n",
    "sub.to_csv(os.path.join(save_root, 'submission_stacked.csv'), index=False)\n",
    "print(\"‚úÖ Saved stacked submission.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 292180, number of used features: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRanker was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved stacked submission.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from scipy.stats import rankdata\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.operate_model import predict\n",
    "\n",
    "\n",
    "# --- ÈÖçÁΩÆ ---\n",
    "meta_folds = [0,1,2,3,4,5]  # ‰æãÂ¶ÇÂè™Áî® fold0, fold2, fold4\n",
    "C          = 35       # cell type Êï∞Èáè\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 1) ÂØºÂÖ• full_dataset, slide_idx, test_dataset\n",
    "full_dataset = importDataset(grouped_data, model,\n",
    "                             image_keys=['tile','subtiles'],\n",
    "                             transform=lambda x: x)\n",
    "n_spots = len(full_dataset)\n",
    "\n",
    "# 2) ÁîüÊàê OOF preds & ËÆ∞ÂΩï fold id (Âêå‰Ω†‰πãÂâçÂÅöÊ≥ï)\n",
    "oof_preds    = np.zeros((n_spots, C), dtype=np.float32)\n",
    "oof_fold_ids = np.full(n_spots, -1, dtype=int)\n",
    "logo = LeaveOneGroupOut()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "        logo.split(X=np.zeros(n_spots), y=None, groups=slide_idx)):\n",
    "    if fold_id not in meta_folds:\n",
    "        continue\n",
    "    # load CNN model & predict\n",
    "    ckpt = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net  = model.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "    net.eval()\n",
    "    loader = DataLoader(Subset(full_dataset, va_idx),\n",
    "                        batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = predict(net, loader, device)  # (n_val, C)\n",
    "    oof_preds[va_idx]    = preds\n",
    "    oof_fold_ids[va_idx] = fold_id\n",
    "\n",
    "# ÁúüÊ†áÁ≠æÁü©Èòµ (n_spots, C)\n",
    "y_true = np.vstack([ full_dataset[i]['label'].cpu().numpy()\n",
    "                     for i in range(n_spots) ])\n",
    "\n",
    "# 3) Á≠õÂá∫ meta_folds ÁöÑË°å\n",
    "mask       = np.isin(oof_fold_ids, meta_folds)\n",
    "X_meta     = oof_preds[mask]    # (N_meta, C)\n",
    "y_meta     = y_true[mask]       # (N_meta, C)\n",
    "slides_sub = slide_idx[mask]    # (N_meta,)\n",
    "\n",
    "N_meta = X_meta.shape[0]\n",
    "\n",
    "docs_feat = X_meta.reshape(-1, 1)   # ÂèòÊàê (N_meta * 35, 1)\n",
    "\n",
    "# ÂØπÂ∫îÁöÑÁúüÂÆû abundanceÔºå‰πüË¶Å flatten\n",
    "docs_true = y_meta.reshape(-1,)     # (N_meta * 35,)\n",
    "\n",
    "# ÊØè‰∏™ spot ÈáåÂØπ 35 ‰∏™ abundance ÂÅö ordinal ÊéíÔºåÂç≥ relevance label\n",
    "docs_rel = np.zeros_like(docs_true, dtype=int)\n",
    "for i in range(X_meta.shape[0]):\n",
    "    start = i * C\n",
    "    end   = start + C\n",
    "    docs_rel[start:end] = rankdata(\n",
    "        docs_true[start:end],\n",
    "        method='ordinal'\n",
    "    ) - 1          # ÂèòÊàê 0‚Äêbased\n",
    "\n",
    "# Flatten ‰πãÂêéÁöÑ groupÔºöÊØè‰∏™ spot ‰∏ãÂõ∫ÂÆö 35 Êù° doc\n",
    "train_group = [C] * X_meta.shape[0]  # e.g. [35,35,35, ...] ÈïøÂ∫¶ = N_meta\n",
    "\n",
    "# ------------------------ 2) ËÆ≠ÁªÉ‰∏Ä‰∏™ LGBMRanker ------------------------\n",
    "ranker = lgb.LGBMRanker(\n",
    "    objective='lambdarank',\n",
    "    metric='ndcg@35',      # ‰πüÂèØ‰ª•Âè™ÂÖ≥Ê≥® ndcg@5„ÄÅ@10\n",
    "    label_gain=list(range(docs_rel.max()+1)),\n",
    "    learning_rate=1e-3,\n",
    "    n_estimators=1000,\n",
    "    num_leaves=31,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    n_jobs=-1\n",
    ")\n",
    "ranker.fit(\n",
    "    docs_feat,   # (N_meta*35, 1)\n",
    "    docs_rel,    # (N_meta*35,)\n",
    "    group=train_group\n",
    ")\n",
    "\n",
    "# ------------------------ 3) ÊµãËØïÊó∂ÂêåÊ†∑ Flatten + Predict ------------------------\n",
    "# 5) ÂáÜÂ§á test_metaÔºåÂè™Âπ≥Âùá meta_folds ‰∏≠ÁöÑÈ¢ÑÊµã\n",
    "n_folds = len([d for d in os.listdir(save_root) if d.startswith('fold')])\n",
    "n_test  = len(test_dataset)\n",
    "test_meta = np.zeros((n_test, C), dtype=np.float32)\n",
    "\n",
    "for fold_id in range(n_folds):\n",
    "    if fold_id not in meta_folds:\n",
    "        continue\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = model.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = predict(net, loader, device)\n",
    "    test_meta += preds\n",
    "\n",
    "# Âπ≥ÂùáÊó∂Èô§‰ª•ÂèÇ‰∏éÁöÑ folds Êï∞ÁõÆ\n",
    "test_meta /= len(meta_folds)\n",
    "\n",
    "docs_test_feat = test_meta.reshape(-1, 1)     # (n_test*35, 1)\n",
    "test_group     = [C] * test_meta.shape[0]    # [35, 35, ..., 35]\n",
    "\n",
    "docs_test_score = ranker.predict(docs_test_feat, group=test_group)  # (n_test*35,)\n",
    "\n",
    "# ÊääÂàÜÊï∞ reshape Âõû (n_test, 35)\n",
    "scores = docs_test_score.reshape(test_meta.shape[0], C)  \n",
    "\n",
    "\n",
    "final_scores = test_meta*scores\n",
    "# 6) ÂÜôÂÖ• submission\n",
    "import pandas as pd, h5py\n",
    "\n",
    "\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spot_ids = pd.DataFrame(np.array(f[\"spots/Test\"][\"S_7\"]))\n",
    "\n",
    "sub = pd.DataFrame(final_scores, columns=[f\"C{i+1}\" for i in range(C)])\n",
    "sub.insert(0, 'ID', test_spot_ids.index)\n",
    "sub.to_csv(os.path.join(save_root, 'submission_LGBMRanker.csv'), index=False)\n",
    "print(\"‚úÖ Saved stacked submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_9016/3498125875.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  raw = torch.load(pt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è ‰ªé 'tile' Êé®Êñ≠Ê†∑Êú¨Êï∞Èáè: 2088\n",
      "Model forward signature: (tile, subtiles)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "\n",
    "def load_node_feature_data(pt_path: str, model, num_cells: int = 35) -> dict:\n",
    "    \"\"\"\n",
    "    Ê†πÊçÆ model.forward ÁöÑÂèÇÊï∞Ëá™Âä®Âä†ËΩΩ .pt ÈáåÂØπÂ∫îÁöÑÂ≠óÊÆµÔºå\n",
    "    Âπ∂‰∏îÂ¶ÇÊûúÊ≤°Êúâ labelÔºåÂ∞±Ëá™Âä®ÂàõÂª∫‰∏Ä‰∏™ÂÖ® 0 ÁöÑ label Âº†ÈáèÔºå\n",
    "    ÂÖ∂Â∞∫ÂØ∏‰∏∫ (Ê†∑Êú¨Êï∞, num_cells)ÔºåÊ†∑Êú¨Êï∞‰ªéÁ¨¨‰∏Ä‰∏™Êúâ __len__ ÁöÑËæìÂÖ•Êé®Êñ≠„ÄÇ\n",
    "\n",
    "    ÂèÇÊï∞Ôºö\n",
    "      pt_path:     strÔºå.pt Êñá‰ª∂Ë∑ØÂæÑ\n",
    "      model:       Â∑≤ÂÆû‰æãÂåñÁöÑ PyTorch Ê®°Âûã\n",
    "      num_cells:   intÔºålabel ÁöÑÂàóÊï∞ÔºàÈªòËÆ§ 35Ôºâ\n",
    "\n",
    "    ËøîÂõûÔºö\n",
    "      dict: key ÂØπÂ∫îÊ®°Âûã forward ‰∏≠ÁöÑÂèÇÊï∞ÂêçÔºà‰∏çÂê´ selfÔºâÔºå\n",
    "            value ÊòØÂØπÂ∫îÁöÑ Tensor/ndarrayÔºå\n",
    "            Âπ∂È¢ùÂ§ñ‰øùËØÅÊúâ 'label' Â≠óÊÆµ„ÄÇ\n",
    "    \"\"\"\n",
    "    # 1) ËΩΩÂÖ•ÂéüÂßãÊï∞ÊçÆ\n",
    "    raw = torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "    # 2) ÂèñÊ®°Âûã forward ÂÖ•ÂèÇÁ≠æÂêçÔºà‰∏çÂê´ selfÔºâ\n",
    "    sig = inspect.signature(model.forward)\n",
    "    param_names = [p for p in sig.parameters if p != \"self\"]\n",
    "\n",
    "    out = {}\n",
    "    for name in param_names:\n",
    "        # a) Áõ¥Êé•ÂêåÂêç\n",
    "        if name in raw:\n",
    "            out[name] = raw[name]\n",
    "            continue\n",
    "        # b) Â§çÊï∞ÂΩ¢Âºè\n",
    "        if name + \"s\" in raw:\n",
    "            out[name] = raw[name + \"s\"]\n",
    "            continue\n",
    "        # c) Ê®°Á≥äÂåπÈÖçÔºà‰∏ãÂàíÁ∫ø„ÄÅÂ§çÊï∞ÊàñÂâçÂêéÁºÄÔºâ\n",
    "        cands = [k for k in raw if name in k or k in name]\n",
    "        if len(cands) == 1:\n",
    "            out[name] = raw[cands[0]]\n",
    "            continue\n",
    "        raise KeyError(f\"Êó†Ê≥ïÊâæÂà∞ '{name}' Âú® pt Êñá‰ª∂‰∏≠ÁöÑÂØπÂ∫îÂ≠óÊÆµÔºåraw keys: {list(raw.keys())}\")\n",
    "\n",
    "    # 3) Áî®Á¨¨‰∏Ä‰∏™ÊîØÊåÅ len() ÁöÑËæìÂÖ•Êé®Êñ≠Ê†∑Êú¨Êï∞\n",
    "    dataset_size = None\n",
    "    for v in out.keys():\n",
    "        if hasattr(out[v], \"__len__\"):\n",
    "            dataset_size = len(out[v])\n",
    "            print(f\"‚ö†Ô∏è ‰ªé '{v}' Êé®Êñ≠Ê†∑Êú¨Êï∞Èáè: {dataset_size}\")\n",
    "            break\n",
    "    if dataset_size is None:\n",
    "        raise RuntimeError(\"Êó†Ê≥ï‰ªé‰ªª‰ΩïËæìÂÖ•‰∏≠Êé®Êñ≠Ê†∑Êú¨Êï∞ÈáèÔºåËØ∑Ê£ÄÊü• pt Êñá‰ª∂ÂÜÖÂÆπ„ÄÇ\")\n",
    "\n",
    "    # 4) Ëá™Âä®Ë°• label\n",
    "\n",
    "    out[\"label\"] = torch.zeros((dataset_size, num_cells), dtype=torch.float32)\n",
    "    return out\n",
    "\n",
    "\n",
    "image_keys = [ 'tile', 'subtiles']\n",
    "\n",
    "\n",
    "# Áî®Ê≥ïÁ§∫‰æã\n",
    "from python_scripts.import_data import importDataset\n",
    "# ÂÅáËÆæ‰Ω†ÁöÑ model Â∑≤ÁªèÂÆö‰πâÂ•ΩÂπ∂ÂÆû‰æãÂåñ‰∏∫ `model`\n",
    "test_dataset = load_node_feature_data(\"dataset/spot-rank/test/original_masked/test_dataset.pt\", model)\n",
    "test_dataset = importDataset(\n",
    "        data_dict=test_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking dataset sample: 1000\n",
      "üìè tile shape: torch.Size([3, 78, 78]) | dtype: torch.float32 | min: 0.200, max: 1.000, mean: 0.729, std: 0.152\n",
      "üìè subtiles shape: torch.Size([9, 3, 26, 26]) | dtype: torch.float32 | min: 0.200, max: 1.000, mean: 0.729, std: 0.152\n",
      "üìè label shape: torch.Size([35]) | dtype: torch.float32 | min: 0.000, max: 0.000, mean: 0.000, std: 0.000\n",
      "--- label head (Ââç 10 ÂÄãÂÖÉÁ¥†):\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "‚úÖ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_9016/566374972.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt, map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved fold 0 predictions to /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/submission_fold0.csv\n",
      "‚úÖ Saved fold 1 predictions to /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/submission_fold1.csv\n",
      "‚úÖ Saved fold 2 predictions to /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/submission_fold2.csv\n",
      "‚úÖ Saved fold 3 predictions to /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/submission_fold3.csv\n",
      "‚úÖ Saved fold 4 predictions to /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/submission_fold4.csv\n",
      "‚úÖ Saved fold 5 predictions to /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/submission_fold5.csv\n",
      "‚úÖ Saved rank‚Äêensemble submission to /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/realign/pretrain/AE_Center_noaug/filtered_directly_rank/k-fold/realign_all/original_image/submission_rank_ensemble.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ËÆÄ test spot index\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spots     = f[\"spots/Test\"]\n",
    "    test_spot_table= pd.DataFrame(np.array(test_spots['S_7']))\n",
    "\n",
    "fold_ckpts = sorted(glob.glob(os.path.join(save_folder, \"fold*\", \"best_model.pt\")))\n",
    "models = []\n",
    "for ckpt in fold_ckpts:\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"all\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = False\n",
    "    )\n",
    "\n",
    "    # 2) monkey‚Äêpatch ‰∏Ä‰∏™Êñ∞ÁöÑ head\n",
    "    net.decoder  = nn.Sequential(\n",
    "        nn.Linear(64+64, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "        \n",
    "    )\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt, map_location=\"cpu\"))\n",
    "    net.to(device).eval()\n",
    "    models.append(net)\n",
    "\n",
    "all_fold_preds = []\n",
    "for fold_id, net in enumerate(models):\n",
    "    # Êé®Ë´ñ\n",
    "    with torch.no_grad():\n",
    "        preds = predict(net, test_loader, device)  # (N_test,35) numpy array\n",
    "\n",
    "    # 1) Â≠òÊØè‰∏ÄÊäòÁöÑÂéüÂßãÈ†êÊ∏¨\n",
    "    df_fold = pd.DataFrame(preds, columns=[f\"C{i+1}\" for i in range(preds.shape[1])])\n",
    "    df_fold.insert(0, \"ID\", test_spot_table.index)\n",
    "    path_fold = os.path.join(save_folder, f\"submission_fold{fold_id}.csv\")\n",
    "    df_fold.to_csv(path_fold, index=False)\n",
    "    print(f\"‚úÖ Saved fold {fold_id} predictions to {path_fold}\")\n",
    "\n",
    "    all_fold_preds.append(preds)\n",
    "\n",
    "# 2) ÂÅö rank‚Äêaverage ensemble\n",
    "all_fold_preds = np.stack(all_fold_preds, axis=0)       # (K, N_test, 35)\n",
    "ranks          = all_fold_preds.argsort(axis=2).argsort(axis=2).astype(float)\n",
    "mean_rank      = ranks.mean(axis=0)                    # (N_test,35)\n",
    "\n",
    "# 3) Â≠ò final ensemble\n",
    "df_ens = pd.DataFrame(mean_rank, columns=[f\"C{i+1}\" for i in range(mean_rank.shape[1])])\n",
    "df_ens.insert(0, \"ID\", test_spot_table.index)\n",
    "path_ens = os.path.join(save_folder, \"submission_rank_ensemble.csv\")\n",
    "df_ens.to_csv(path_ens, index=False)\n",
    "print(f\"‚úÖ Saved rank‚Äêensemble submission to {path_ens}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
