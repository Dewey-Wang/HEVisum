{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, out_dim, in_channels=3, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(64, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiTaskDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=35, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(negative_slope)\n",
    "        )\n",
    "        self.heads = nn.ModuleList([nn.Linear(64, 1) for _ in range(output_dim)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        outs = [head(shared) for head in self.heads]\n",
    "        return torch.cat(outs, dim=1)  # (B, 35)\n",
    "\n",
    "\n",
    "class VisionMLP_MultiTask(nn.Module):\n",
    "    def __init__(self, cnn_out_dim=64, output_dim=35, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.encoder_tile = CNNEncoder(cnn_out_dim, in_channels=3, negative_slope=negative_slope)\n",
    "        self.encoder_neighbors = CNNEncoder(cnn_out_dim, in_channels=3, negative_slope=negative_slope)\n",
    "        self.encoder_subtiles = CNNEncoder(cnn_out_dim, in_channels=3, negative_slope=negative_slope)\n",
    "\n",
    "        self.decoder = MultiTaskDecoder(input_dim=cnn_out_dim * 3 + 2,\n",
    "                                        output_dim=output_dim,\n",
    "                                        negative_slope=negative_slope)\n",
    "\n",
    "    def forward(self, tile, subtiles, neighbors, coords):\n",
    "        B = tile.size(0)\n",
    "\n",
    "        # tile: [B, 3, 78, 78]\n",
    "        f_tile = self.encoder_tile(tile)\n",
    "\n",
    "        # subtiles: [B, 9, 3, 26, 26]\n",
    "        B, N, C, H, W = subtiles.shape\n",
    "        subtiles_reshaped = subtiles.contiguous().reshape(B * N, C, H, W)  # ✅ 強制 contiguous\n",
    "        f_sub = self.encoder_subtiles(subtiles_reshaped).reshape(B, N, -1).mean(dim=1)  # ✅ reshape\n",
    "\n",
    "        # neighbors: [B, 8, 3, 78, 78]\n",
    "        B, N, C, H, W = neighbors.shape\n",
    "        neighbors_reshaped = neighbors.contiguous().reshape(B * N, C, H, W)  # ✅ 強制 contiguous\n",
    "        f_neigh = self.encoder_neighbors(neighbors_reshaped).reshape(B, N, -1).mean(dim=1)  # ✅ reshape\n",
    "\n",
    "        x = torch.cat([f_tile, f_sub, f_neigh, coords], dim=1)\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = VisionMLP_MultiTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入的 class 名稱: ['CNNEncoder', 'MLPDecoder', 'VisionMLPModelWithCoord']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from python_scripts.import_model import load_model_classes\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "\n",
    "# ==============================================\n",
    "# 範例使用\n",
    "# ==============================================\n",
    "folder = \"./output_folder/CNN+MLP/\"  # 替換成實際的資料夾路徑，該路徑下應有 model.py\n",
    "try:\n",
    "    loaded_classes = load_model_classes(folder)\n",
    "    print(\"載入的 class 名稱:\", list(loaded_classes.keys()))\n",
    "except Exception as e:\n",
    "    print(\"載入模型 class 發生錯誤:\", e)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Signature (center_tile, subtiles, neighbor_tiles, coords)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假設 loaded_classes 是你已經從 model.py 載入的 class 字典\n",
    "name_of_class = 'VisionMLPModelWithCoord'\n",
    "ModelClass = loaded_classes.get(name_of_class)\n",
    "if ModelClass is None:\n",
    "    raise ValueError(f\"找不到 {name_of_class} 這個 class\")\n",
    "# 這裡呼叫 ModelClass() 建立實例，注意不能直接用 name_of_class() (因為它是一個字串)\n",
    "model = ModelClass()  # 如果需要參數，請在此處傳入\n",
    "get_model_inputs(model)\n",
    "#print(\"建立的模型實例:\", model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same in multiple .pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_29090/3115120493.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def load_grouped_tile_data(folder_path):\n",
    "    neighbors_list = []\n",
    "    tiles_list = []\n",
    "    subtiles_list = []\n",
    "    labels_list = []\n",
    "    coords_list = []\n",
    "    #norm_coords_list = []\n",
    "\n",
    "    pt_files = sorted([\n",
    "        f for f in os.listdir(folder_path)\n",
    "        if f.endswith(\".pt\") and not f.startswith(\"original\")\n",
    "    ])\n",
    "\n",
    "    for fname in pt_files:\n",
    "        file_path = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            data = torch.load(file_path, map_location=\"cpu\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {fname}: {e}\")\n",
    "            continue  # skip this file\n",
    "\n",
    "        neighbors_list.append(data[\"neighbors\"])\n",
    "        tiles_list.append(data[\"tile\"])\n",
    "        subtiles_list.append(data[\"subtiles\"])\n",
    "        labels_list.append(data[\"label\"])\n",
    "        coords_list.append(data[\"coord\"])\n",
    "        #norm_coords_list.append(data[\"norm_coord\"])\n",
    "    \n",
    "    return {\n",
    "        \"neighbors\": neighbors_list,\n",
    "        \"tile\": tiles_list,\n",
    "        \"subtiles\": subtiles_list,\n",
    "        \"label\": labels_list,\n",
    "        \"coords\": coords_list,\n",
    "        #\"norm_coord\": norm_coords_list\n",
    "    }\n",
    "\n",
    "\n",
    "# 🧪 用法\n",
    "grouped_data = load_grouped_tile_data(\"./dataset/3-channel/78-3sizes-3*3/train/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (tile, subtiles, neighbors, coords)\n"
     ]
    }
   ],
   "source": [
    "from python_scripts.import_data import importDataset\n",
    "\n",
    "image_keys = ['neighbors', 'tile', 'subtiles']\n",
    "\n",
    "train_dataset = importDataset(\n",
    "        data_dict=grouped_data,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking dataset sample: 1000\n",
      "📏 tile shape: torch.Size([3, 78, 78]) | dtype: torch.float32 | min: 0.291, max: 0.992, mean: 0.860, std: 0.139\n",
      "📏 subtiles shape: torch.Size([9, 3, 26, 26]) | dtype: torch.float32 | min: 0.291, max: 0.992, mean: 0.860, std: 0.139\n",
      "📏 neighbors shape: torch.Size([8, 3, 78, 78]) | dtype: torch.float32 | min: 0.330, max: 1.000, mean: 0.855, std: 0.146\n",
      "📏 coords shape: torch.Size([2]) | dtype: torch.float32 | min: 1527.000, max: 1709.000, mean: 1618.000, std: 128.693\n",
      "--- coords head (前 10 個元素):\n",
      "tensor([1527., 1709.])\n",
      "📏 label shape: torch.Size([35]) | dtype: torch.float32 | min: -0.744, max: 2.278, mean: -0.254, std: 0.572\n",
      "--- label head (前 10 個元素):\n",
      "tensor([-0.6705,  0.0176, -0.6396, -0.6044,  2.2777, -0.6090,  0.4683, -0.3074,\n",
      "        -0.5801, -0.2711])\n",
      "✅ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "train_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 26179 samples\n",
      "✅ Val: 6545 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 設定比例\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# 拆分 Dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "print(f\"✅ Train: {len(train_set)} samples\")\n",
    "print(f\"✅ Val: {len(val_set)} samples\")\n",
    "\n",
    "# 🔹 將其包成 DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save in one pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_19297/1962845458.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tile = torch.load(\"./dataset/final_data/M_tiles.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_19297/1962845458.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  label = torch.load(\"./dataset/final_data/gu_log2_labels.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_19297/1962845458.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta = torch.load(\"./dataset/final_data/meta_info.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (tile, coords)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from python_scripts.import_data import importDataset, preprocess_data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 載入資料\n",
    "#tile = torch.load(\"./dataset/final_data/M_tiles.pt\")\n",
    "#subtiles = torch.load(\"./train_dataset_sep_v2/subtiles.pt\")\n",
    "#neighbor_tiles = torch.load(\"./train_dataset_sep_v2/neighbor_tiles.pt\")\n",
    "#label = torch.load(\"./dataset/final_data/gu_log2_labels.pt\")\n",
    "#meta = torch.load(\"./dataset/final_data/meta_info.pt\")\n",
    "\n",
    "\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in grouped_data['coords']:\n",
    "    if _meta is not None:\n",
    "         x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "train_dataset = {\n",
    "        'tile': tile,\n",
    "        #'subtiles': subtiles,\n",
    "        #'neighbor_tiles': neighbor_tiles,\n",
    "        'coords': normalized_coords,\n",
    "        'label': label\n",
    "    }\n",
    "\n",
    "my_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "\n",
    "image_keys = ['tile']\n",
    "\n",
    "#processed_data = preprocess_data(train_dataset, image_keys, my_transform)\n",
    "\n",
    "train_dataset = importDataset(\n",
    "        data_dict=train_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m importDataset(\n\u001b[0;32m----> 2\u001b[0m         data_dict\u001b[38;5;241m=\u001b[39m\u001b[43mprocessed_data\u001b[49m,\n\u001b[1;32m      3\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      4\u001b[0m         image_keys\u001b[38;5;241m=\u001b[39mimage_keys,\n\u001b[1;32m      5\u001b[0m         transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x,  \u001b[38;5;66;03m# identity transform\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         print_sig\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = importDataset(\n",
    "        data_dict=processed_data,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "轉換欄位 'tile' 的資料為 tensor 時出錯，資料內容: [[[0.9678768  0.954182   0.9747549 ]\n  [0.9704657  0.95502454 0.9764706 ]\n  [0.9704657  0.95502454 0.9764706 ]\n  ...\n  [0.9767846  0.95091146 0.9703432 ]\n  [0.9713082  0.9556219  0.96738666]\n  [0.9646906  0.95493263 0.9654565 ]]\n\n [[0.9689032  0.95465684 0.97550553]\n  [0.97071075 0.95477945 0.9764706 ]\n  [0.96967673 0.9537454  0.9754366 ]\n  ...\n  [0.97511876 0.9502413  0.97131205]\n  [0.9694317  0.95374537 0.96551013]\n  [0.9623774  0.9550245  0.9647059 ]]\n\n [[0.9686274  0.95686275 0.9764706 ]\n  [0.9676509  0.9558862  0.975494  ]\n  [0.96568245 0.95391774 0.97352564]\n  ...\n  [0.9730354  0.9502413  0.9730354 ]\n  [0.9664216  0.9529412  0.9647059 ]\n  [0.96134347 0.9550245  0.9647059 ]]\n\n ...\n\n [[0.8678539  0.6751915  0.78875613]\n  [0.89483    0.7065947  0.8041437 ]\n  [0.91975725 0.7323836  0.8442517 ]\n  ...\n  [0.68689877 0.554902   0.77635956]\n  [0.68630135 0.5452283  0.7674058 ]\n  [0.7217984  0.57002914 0.78608304]]\n\n [[0.8391927  0.6464614  0.7745404 ]\n  [0.89067096 0.6948759  0.81308216]\n  [0.9337929  0.73981315 0.86159235]\n  ...\n  [0.70375305 0.578508   0.7847044 ]\n  [0.70224416 0.5752834  0.77957267]\n  [0.7377298  0.60462624 0.80364585]]\n\n [[0.7855086  0.59163606 0.73481923]\n  [0.8534467  0.65076596 0.78898597]\n  [0.91767    0.7181602  0.8506357 ]\n  ...\n  [0.7173024  0.59690565 0.7941943 ]\n  [0.7302696  0.6088082  0.8036765 ]\n  [0.7664675  0.6419577  0.8292126 ]]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/GitHub/HEVisum/python_scripts/import_data.py:192\u001b[0m, in \u001b[0;36mimportDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_keys:\n\u001b[0;32m--> 192\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/GitHub/HEVisum/python_scripts/import_data.py:123\u001b[0m, in \u001b[0;36mconvert_item\u001b[0;34m(item, is_image)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_item\u001b[39m(item, is_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_image:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GitHub/HEVisum/python_scripts/import_data.py:230\u001b[0m, in \u001b[0;36mimportDataset.check_item\u001b[0;34m(self, idx, num_lines)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m檢查第 idx 筆資料中每個欄位的詳細資訊。\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m對每個欄位（依據 forward_keys 加上 'label'）印出：\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m  - 對於非圖片資料，印出該 tensor 前 num_lines 列/元素的內容。\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m expected_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_keys \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 230\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔍 Checking dataset sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n",
      "File \u001b[0;32m~/Desktop/GitHub/HEVisum/python_scripts/import_data.py:196\u001b[0m, in \u001b[0;36mimportDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    194\u001b[0m         value \u001b[38;5;241m=\u001b[39m convert_item(value, is_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m轉換欄位 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 的資料為 tensor 時出錯，資料內容: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# 轉換成 float32\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[0;31mValueError\u001b[0m: 轉換欄位 'tile' 的資料為 tensor 時出錯，資料內容: [[[0.9678768  0.954182   0.9747549 ]\n  [0.9704657  0.95502454 0.9764706 ]\n  [0.9704657  0.95502454 0.9764706 ]\n  ...\n  [0.9767846  0.95091146 0.9703432 ]\n  [0.9713082  0.9556219  0.96738666]\n  [0.9646906  0.95493263 0.9654565 ]]\n\n [[0.9689032  0.95465684 0.97550553]\n  [0.97071075 0.95477945 0.9764706 ]\n  [0.96967673 0.9537454  0.9754366 ]\n  ...\n  [0.97511876 0.9502413  0.97131205]\n  [0.9694317  0.95374537 0.96551013]\n  [0.9623774  0.9550245  0.9647059 ]]\n\n [[0.9686274  0.95686275 0.9764706 ]\n  [0.9676509  0.9558862  0.975494  ]\n  [0.96568245 0.95391774 0.97352564]\n  ...\n  [0.9730354  0.9502413  0.9730354 ]\n  [0.9664216  0.9529412  0.9647059 ]\n  [0.96134347 0.9550245  0.9647059 ]]\n\n ...\n\n [[0.8678539  0.6751915  0.78875613]\n  [0.89483    0.7065947  0.8041437 ]\n  [0.91975725 0.7323836  0.8442517 ]\n  ...\n  [0.68689877 0.554902   0.77635956]\n  [0.68630135 0.5452283  0.7674058 ]\n  [0.7217984  0.57002914 0.78608304]]\n\n [[0.8391927  0.6464614  0.7745404 ]\n  [0.89067096 0.6948759  0.81308216]\n  [0.9337929  0.73981315 0.86159235]\n  ...\n  [0.70375305 0.578508   0.7847044 ]\n  [0.70224416 0.5752834  0.77957267]\n  [0.7377298  0.60462624 0.80364585]]\n\n [[0.7855086  0.59163606 0.73481923]\n  [0.8534467  0.65076596 0.78898597]\n  [0.91767    0.7181602  0.8506357 ]\n  ...\n  [0.7173024  0.59690565 0.7941943 ]\n  [0.7302696  0.6088082  0.8036765 ]\n  [0.7664675  0.6419577  0.8292126 ]]]"
     ]
    }
   ],
   "source": [
    "train_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 26179 samples\n",
      "✅ Val: 6545 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 設定比例\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# 拆分 Dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "print(f\"✅ Train: {len(train_set)} samples\")\n",
    "print(f\"✅ Val: {len(val_set)} samples\")\n",
    "\n",
    "# 🔹 將其包成 DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from python_scripts.operate_model import get_model_inputs, train_one_epoch, evaluate, predict, EarlyStopping, plot_losses, plot_per_cell_metrics\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------------------\n",
    "# 指定儲存資料夾\n",
    "# ---------------------------\n",
    "save_folder = \"output_folder/with_all_preprocess_log2/GU+CNN+MTD_3size_78/\"  # 修改為你想要的資料夾名稱\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_fn(loss_type=\"mse\", cell_weights=None):\n",
    "    \"\"\"\n",
    "    支援多種 loss function，包含:\n",
    "    - \"mse\": Mean Squared Error (預設)\n",
    "    - \"weighted_mse\": 根據 cell_weights 做加權的 MSE\n",
    "    - \"mae\": Mean Absolute Error\n",
    "    - \"spearman\": Spearman loss (非 differentiable，但可以實驗)\n",
    "\n",
    "    回傳對應的 loss function。\n",
    "    \"\"\"\n",
    "    loss_type = loss_type.lower()\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        return nn.MSELoss()\n",
    "    \n",
    "    elif loss_type == \"mae\":\n",
    "        return nn.L1Loss()\n",
    "\n",
    "    elif loss_type == \"weighted_mse\":\n",
    "        if cell_weights is None:\n",
    "            raise ValueError(\"需要提供 cell_weights 才能使用 weighted MSE\")\n",
    "\n",
    "        def weighted_mse(pred, target):\n",
    "            loss = (pred - target) ** 2  # (B, C)\n",
    "            loss = loss.mean(dim=0)      # 對 batch 平均 → (C,)\n",
    "            weighted_loss = (loss * cell_weights.to(pred.device)).mean()\n",
    "            return weighted_loss\n",
    "\n",
    "        return weighted_mse\n",
    "\n",
    "    elif loss_type == \"spearman\":\n",
    "        from scipy.stats import spearmanr\n",
    "\n",
    "        def spearman_loss(pred, target):\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            target = target.detach().cpu().numpy()\n",
    "            rho = np.mean([spearmanr(pred[:, i], target[:, i])[0] for i in range(pred.shape[1])])\n",
    "            return 1 - rho  # 模擬 loss 越小越好（非 differentiable）\n",
    "        \n",
    "        return spearman_loss\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"不支援的 loss_type: {loss_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2118, 1.4699, 1.2259, 1.2525, 1.2928, 1.3897, 1.4408, 1.8691, 1.2647,\n",
       "        1.4540, 1.6404, 1.3991, 2.0000, 1.7003, 1.2916, 1.2448, 1.5875, 1.3241,\n",
       "        1.8716, 1.6964, 1.7266, 1.9636, 1.4970, 1.4547, 1.5564, 1.8343, 1.2534,\n",
       "        1.7102, 1.6177, 1.4917, 1.4073, 1.4435, 1.9684, 1.7002, 1.4398])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_weights = torch.tensor(mse_per_cell)\n",
    "cell_weights = cell_weights / cell_weights.max()   # scale 到 [0, 1]\n",
    "cell_weights = cell_weights + 1.0                  # shift 到 [1, 2]\n",
    "cell_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3966, 0.8798, 0.4230, 0.4727, 0.5482, 0.7296, 0.8255, 1.6273, 0.4956,\n",
       "        0.8500, 1.1992, 0.7474, 1.8725, 1.3113, 0.5461, 0.4584, 1.1001, 0.6069,\n",
       "        1.6320, 1.3040, 1.3606, 1.8043, 0.9306, 0.8514, 1.0419, 1.5623, 0.4745,\n",
       "        1.3299, 1.1567, 0.9208, 0.7626, 0.8305, 1.8133, 1.3111, 0.8236])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_weights = torch.tensor(mse_per_cell)\n",
    "cell_weights = cell_weights / cell_weights.mean()  # 均值為 1，總和為 35 左右\n",
    "cell_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 13/819 [00:03<02:17,  5.86it/s, avg=19.8, loss=0.773]"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 🔧 設定裝置\n",
    "# ---------------------------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 初始化模型 & 優化器\n",
    "# ---------------------------\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "# loss_fn = spearman_loss\n",
    "#LOSS_TYPE = \"weighted_mse\"  # 或 \"mse\", \"mae\", \"spearman\"\n",
    "\n",
    "\n",
    "#loss_fn = get_loss_fn(loss_type=LOSS_TYPE, cell_weights=cell_weights)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# ---------------------------\n",
    "# 儲存 log 的設定\n",
    "# ---------------------------\n",
    "log_path = os.path.join(save_folder, \"training_log.csv\")\n",
    "log_file = open(log_path, mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# ---------------------------\n",
    "# 用來畫圖的變數\n",
    "# ---------------------------\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_spearmanes = []\n",
    "val_spearmanes = []\n",
    "\n",
    "# ---------------------------\n",
    "# 指定最佳模型儲存路徑\n",
    "# ---------------------------\n",
    "best_model_path = os.path.join(save_folder, \"best_model.pt\")\n",
    "loss_plot_path = os.path.join(save_folder, \"loss_curve.png\")\n",
    "\n",
    "# ---------------------------\n",
    "# 開始訓練\n",
    "# ---------------------------\n",
    "num_epochs = 500\n",
    "best_val_loss = float('inf')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "display(fig)  # 初始顯示圖形\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_spearman = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_spearman, mse_per_cell, spearman_per_cell = evaluate(model, val_loader, loss_fn, device)\n",
    "    \n",
    "    axes[0][0].clear()\n",
    "    axes[0][1].clear()\n",
    "    axes[1][0].clear()\n",
    "    axes[1][1].clear()\n",
    "    # 儲存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\"✅ Saved best model!\")\n",
    "\n",
    "    # 調整學習率\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 寫入 CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch + 1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # 印出 Epoch 結果\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | ρ: {val_spearman:.4f} | lr: {lr:.2e}\")\n",
    "\n",
    "    # 更新 log 列表並畫圖\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_spearmanes.append(train_spearman)\n",
    "    val_spearmanes.append(val_spearman)\n",
    "    \n",
    "\n",
    "\n",
    "    # plot loss\n",
    "    plot_losses(train_losses, val_losses, ax=axes[0][0], title=\"MSE Loss\")\n",
    "    plot_losses(train_spearmanes, val_spearmanes, ax=axes[0][1], title=\"Spearman Loss\")\n",
    "\n",
    "    cell_names = [f\"C{i+1}\" for i in range(35)]\n",
    "    # plot per-cell stats\n",
    "    plot_per_cell_metrics(mse_per_cell, spearman_per_cell,cell_names,\n",
    "                        ax_mse=axes[1][0], ax_spearman=axes[1][1])\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    clear_output(wait=True)  # 清除之前的輸出\n",
    "    display(fig)\n",
    "    plt.pause(0.1)  # 暫停以便更新畫面\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"⛔ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# 關閉 CSV log 檔案\n",
    "log_file.close()\n",
    "\n",
    "# 儲存最終圖形\n",
    "fig.savefig(loss_plot_path)\n",
    "plt.close(fig)\n",
    "print(f\"訓練結束，loss 曲線圖已儲存至 {loss_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_1688/3109279275.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionWithCoord_MTL(\n",
       "  (encoder): SimpleCNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (12): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU()\n",
       "      (15): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): MultiTaskDecoder(\n",
       "    (shared): Sequential(\n",
       "      (0): Linear(in_features=66, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (7): ReLU()\n",
       "    )\n",
       "    (heads): ModuleList(\n",
       "      (0-34): 35 x Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 需要的 Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "import os\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "best_model_path = os.path.join(save_folder, \"best_model.pt\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_1688/1705579048.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"./dataset/final_data/test_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (tile, coords)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import inspect\n",
    "from python_scripts.import_data import importDataset, preprocess_data\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# 正確方式\n",
    "\n",
    "test_data = torch.load(\"./dataset/final_data/test_dataset.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in test_data['coords']:\n",
    "    if _meta is not None:\n",
    "        x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "test_dataset = {\n",
    "        'tile': torch.stack([torch.tensor(img).permute(2, 0, 1) for img in test_data['M_tiles']]),\n",
    "        #'subtiles': test_data['subtiles'],\n",
    "        #'neighbor_tiles': test_data['neighbor_tiles'],\n",
    "        'coords': normalized_coords,\n",
    "        'label': np.zeros((len(test_data['M_tiles']), 35))\n",
    "    }\n",
    "my_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "\n",
    "image_keys = ['tile']\n",
    "\n",
    "\n",
    "\n",
    "#processed_data = preprocess_data(test_dataset, image_keys, my_transform)\n",
    "\n",
    "test_dataset = importDataset(\n",
    "        data_dict=test_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking dataset sample: 1000\n",
      "📏 tile shape: torch.Size([4, 112, 112]) | dtype: torch.float32 | min: 0.000, max: 1.000, mean: 0.779, std: 0.203\n",
      "📏 coords shape: torch.Size([2]) | dtype: torch.float32 | min: -0.393, max: 0.416, mean: 0.012, std: 0.572\n",
      "--- coords head (前 10 個元素):\n",
      "tensor([-0.3929,  0.4160])\n",
      "📏 label shape: torch.Size([35]) | dtype: torch.float32 | min: 0.000, max: 0.000, mean: 0.000, std: 0.000\n",
      "--- label head (前 10 個元素):\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "✅ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "test_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82993376, -0.35686785,  0.8224873 , ..., -0.41686007,\n",
       "         0.14185855, -0.09103155],\n",
       "       [ 0.14049199, -0.4052957 ,  0.12465326, ..., -0.28855395,\n",
       "        -0.24106702, -0.4477588 ],\n",
       "       [ 0.14049199, -0.4052957 ,  0.12465326, ..., -0.28855395,\n",
       "        -0.24106702, -0.4477588 ],\n",
       "       ...,\n",
       "       [ 0.14049199, -0.4052957 ,  0.12465326, ..., -0.28855395,\n",
       "        -0.24106702, -0.4477588 ],\n",
       "       [-0.0380049 , -0.59372497, -0.06408934, ..., -0.32383734,\n",
       "        -0.3725784 , -0.52910906],\n",
       "       [-0.38289717, -0.2602251 , -0.4121772 , ...,  0.10326526,\n",
       "         0.01796564, -0.05405343]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = predict(model, test_loader, device)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 測試 revert_log2_predictions\n",
      "[[ 1.01396717e+00 -8.07002121e-03  1.80511435e+00  1.13151159e-01\n",
      "   2.00707840e+00  7.59623606e-02  6.96093327e-02  7.63244836e-03\n",
      "  -4.43450703e-02  4.99186394e-04  1.08874982e-02 -2.43959104e-02\n",
      "   4.05517143e-03  2.29893147e-02  6.26831059e-02  8.12215671e-02\n",
      "   2.64344670e-01  2.31829931e-02  1.36808254e-02 -1.33865438e-03\n",
      "  -2.55863969e-02  1.04127442e-02  1.75559047e-03 -3.42817691e-02\n",
      "   8.95906936e-02  2.71020152e-03  7.19174871e-02  1.14335168e-02\n",
      "  -2.35451263e-02  5.75979741e-03  1.10113500e-01  8.37440865e-02\n",
      "   1.95191196e-02  1.52221790e-02 -7.13266282e-02]\n",
      " [ 3.28478151e+00  3.26228103e-01 -1.51059413e-01  2.10342036e-01\n",
      "  -3.70963169e-01  9.46231454e-02  1.08288662e-02  4.10010213e-03\n",
      "   3.99134882e-01  1.60557299e-02 -6.11376033e-03  1.01965609e-03\n",
      "   7.99558069e-03  1.75983651e-02  1.73605903e-01  1.38104129e+00\n",
      "   1.07006572e-02 -5.08118852e-02  1.18068944e-03  6.97391458e-02\n",
      "  -2.20179089e-03  2.30087185e-02 -5.02672957e-03  1.61468807e-01\n",
      "   4.75605425e-02  9.87347349e-03 -3.15146508e-02 -6.39880534e-03\n",
      "   7.97088743e-03  2.97967609e-03  1.01989491e-01  5.37938927e-02\n",
      "   5.04197225e-03 -2.38961325e-03  7.05009362e-02]]\n",
      "🔁 測試 revert_boxcox_predictions\n",
      "⚠️ C1: 3 NaNs filled with median\n",
      "⚠️ C3: 1 NaNs filled with median\n",
      "⚠️ C5: 1 NaNs filled with median\n",
      "⚠️ C15: 1 NaNs filled with median\n",
      "⚠️ C18: 1 NaNs filled with median\n",
      "⚠️ C21: 1 NaNs filled with median\n",
      "[[5.69941215e-01 1.44401062e-02 2.16613543e-01 6.92434560e-02\n",
      "  1.52222846e+00 7.35393786e-02 5.17280355e-02 7.45352587e-03\n",
      "  1.00000241e-06 5.03471033e-04 1.07043069e-02 1.00001083e-06\n",
      "  3.68039741e-03 1.77321683e-02 5.42891212e-02 9.05469999e-02\n",
      "  2.29843179e-01 2.03432510e-02 1.03645722e-02 5.64149836e-03\n",
      "  1.00000957e-06 7.89466761e-03 1.77324432e-02 1.00000313e-06\n",
      "  1.04105599e-01 2.45926188e-03 5.81362032e-02 1.06301747e-02\n",
      "  1.00000794e-06 5.75564067e-03 8.02693318e-02 6.09578730e-02\n",
      "  1.91569223e-02 1.16400356e-02 1.00000711e-06]\n",
      " [8.43054539e-02 5.75388366e-01 1.00000164e-06 1.33292774e-01\n",
      "  1.00000117e-06 9.89988965e-02 1.17600325e-02 3.70104409e-03\n",
      "  3.94138868e-01 1.56028871e-02 1.00000730e-06 2.48993407e-03\n",
      "  8.29558963e-03 1.37160833e-02 2.09324024e-01 9.28974183e-01\n",
      "  1.47278636e-02 1.00000466e-06 2.89474341e-03 5.27115733e-02\n",
      "  1.00000957e-06 1.92639808e-02 1.44715060e-02 9.72692180e-02\n",
      "  4.52473814e-02 9.35846123e-03 1.00000310e-06 1.00001765e-06\n",
      "  6.97255594e-03 2.91593392e-03 7.42650631e-02 4.12930919e-02\n",
      "  4.26394940e-03 1.00001483e-06 5.04165762e-02]]\n"
     ]
    }
   ],
   "source": [
    "from python_scripts.revert_utils import (\n",
    "    load_json_params,\n",
    "    revert_log2_predictions,\n",
    "    revert_boxcox_predictions\n",
    ")\n",
    "\n",
    "# 還原 Box-Cox\n",
    "params = load_json_params(\"./dataset/boxcox_zscore_params.json\")\n",
    "restored = revert_boxcox_predictions(test_preds, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1864083 , 0.0703001 , 0.7612741 , ..., 0.00406682, 0.01734674,\n",
       "        0.04675364],\n",
       "       [0.54058456, 0.06550384, 0.3668126 , ..., 0.00607252, 0.00961936,\n",
       "        0.02623594],\n",
       "       [0.54058456, 0.06550384, 0.3668126 , ..., 0.00607252, 0.00961936,\n",
       "        0.02623594],\n",
       "       ...,\n",
       "       [0.54058456, 0.06550384, 0.3668126 , ..., 0.00607252, 0.00961936,\n",
       "        0.02623594],\n",
       "       [0.4070865 , 0.04704511, 0.27621973, ..., 0.00552046, 0.00697911,\n",
       "        0.0216136 ],\n",
       "       [0.18102789, 0.07993639, 0.12459803, ..., 0.01222217, 0.01484013,\n",
       "        0.04890382]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from python_scripts.revert_utils import (\n",
    "    load_json_params,\n",
    "    revert_log2_predictions,\n",
    ")\n",
    "params = load_json_params(json_path=\"./dataset/zscore_params.json\")\n",
    "\n",
    "# 或還原 log2\n",
    "params = load_json_params(\"./dataset/zscore_params.json\")\n",
    "restored = revert_log2_predictions(test_preds, params, add_constant=1)\n",
    "\n",
    "restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved submission.csv in output_folder/with_all_preprocess_log2/GU+CNN4+MTD4/submission.csv\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== 讀取 test spot index 用於對應 ID ====\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "submission_path = os.path.join(save_folder, \"submission.csv\")\n",
    "\n",
    "ensemble_df = pd.DataFrame(restored, columns=[f\"C{i+1}\" for i in range(restored.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(submission_path, index=False)\n",
    "print(f\"✅ Saved submission.csv in {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
