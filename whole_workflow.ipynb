{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入的 class 名稱: ['CNNEncoder', 'MLPDecoder', 'VisionMLPModelWithCoord']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from python_scripts.import_model import load_model_classes\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "\n",
    "# ==============================================\n",
    "# 範例使用\n",
    "# ==============================================\n",
    "folder = \"./output_folder/CNN+MLP/\"  # 替換成實際的資料夾路徑，該路徑下應有 model.py\n",
    "try:\n",
    "    loaded_classes = load_model_classes(folder)\n",
    "    print(\"載入的 class 名稱:\", list(loaded_classes.keys()))\n",
    "except Exception as e:\n",
    "    print(\"載入模型 class 發生錯誤:\", e)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Signature (center_tile, subtiles, neighbor_tiles, coords)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假設 loaded_classes 是你已經從 model.py 載入的 class 字典\n",
    "name_of_class = 'VisionMLPModelWithCoord'\n",
    "ModelClass = loaded_classes.get(name_of_class)\n",
    "if ModelClass is None:\n",
    "    raise ValueError(f\"找不到 {name_of_class} 這個 class\")\n",
    "# 這裡呼叫 ModelClass() 建立實例，注意不能直接用 name_of_class() (因為它是一個字串)\n",
    "model = ModelClass()  # 如果需要參數，請在此處傳入\n",
    "get_model_inputs(model)\n",
    "#print(\"建立的模型實例:\", model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_95032/1475077652.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  center_tile = torch.load(\"./train_dataset_sep_v2/tiles.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_95032/1475077652.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  subtiles = torch.load(\"./train_dataset_sep_v2/subtiles.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_95032/1475077652.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  neighbor_tiles = torch.load(\"./train_dataset_sep_v2/neighbor_tiles.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_95032/1475077652.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  label = torch.load(\"./train_dataset_sep_v2/labels.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_95032/1475077652.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta = torch.load(\"./train_dataset_sep_v2/meta_info.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method importDataset.check_item of <import_data.importDataset object at 0x103af4eb0>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from python_scripts.import_data import importDataset, preprocess_data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 載入資料\n",
    "center_tile = torch.load(\"./train_dataset_sep_v2/tiles.pt\")\n",
    "subtiles = torch.load(\"./train_dataset_sep_v2/subtiles.pt\")\n",
    "neighbor_tiles = torch.load(\"./train_dataset_sep_v2/neighbor_tiles.pt\")\n",
    "label = torch.load(\"./train_dataset_sep_v2/labels.pt\")\n",
    "meta = torch.load(\"./train_dataset_sep_v2/meta_info.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in meta:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "train_dataset = {\n",
    "        'center_tile': center_tile,\n",
    "        'subtiles': subtiles,\n",
    "        'neighbor_tiles': neighbor_tiles,\n",
    "        'coords': normalized_coords,\n",
    "        'label': label\n",
    "    }\n",
    "\n",
    "my_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "\n",
    "image_keys = ['center_tile', 'subtiles', 'neighbor_tiles']\n",
    "\n",
    "processed_data = preprocess_data(train_dataset, image_keys, my_transform)\n",
    "\n",
    "train_dataset = importDataset(\n",
    "        data_dict=processed_data,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = importDataset(\n",
    "        data_dict=processed_data,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking dataset sample: 1000\n",
      "📏 center_tile shape: torch.Size([3, 78, 78]) | dtype: torch.float32 | min: -0.208, max: 1.000, mean: 0.488, std: 0.232\n",
      "📏 subtiles shape: torch.Size([9, 3, 26, 26]) | dtype: torch.float32 | min: -0.208, max: 1.000, mean: 0.488, std: 0.232\n",
      "📏 neighbor_tiles shape: torch.Size([8, 3, 78, 78]) | dtype: torch.float32 | min: -0.631, max: 1.000, mean: 0.497, std: 0.246\n",
      "📏 coords shape: torch.Size([2]) | dtype: torch.float32 | min: -1.243, max: 1.148, mean: -0.047, std: 1.691\n",
      "--- coords head (前 10 個元素):\n",
      "tensor([ 1.1484, -1.2428])\n",
      "📏 label shape: torch.Size([35]) | dtype: torch.float32 | min: -0.678, max: 3.737, mean: -0.151, std: 0.945\n",
      "--- label head (前 10 個元素):\n",
      "tensor([-0.5575, -0.4912, -0.5658, -0.4039,  0.8988, -0.3776, -0.4374, -0.4394,\n",
      "        -0.4366, -0.3719])\n",
      "✅ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "train_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 6679 samples\n",
      "✅ Val: 1670 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 設定比例\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# 拆分 Dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "print(f\"✅ Train: {len(train_set)} samples\")\n",
    "print(f\"✅ Val: {len(val_set)} samples\")\n",
    "\n",
    "# 🔹 將其包成 DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from python_scripts.operate_model import get_model_inputs, train_one_epoch, evaluate, predict, EarlyStopping, plot_losses\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------------------\n",
    "# 指定儲存資料夾\n",
    "# ---------------------------\n",
    "save_folder = \"output_folder/try\"  # 修改為你想要的資料夾名稱\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0UAAAFlCAYAAAAktEOqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfBUlEQVR4nO3db2yd5Xk/8MuxYxvY7IqkGIcE1+mgTRuVLraSxllUlYFRQFSROuGKigADqVb/hMSDNWkmaCIkq52KVloS2pKAKgXmlX/ihUfjF2swJFsbz6mqJhIVSXHS2kQ2wg7QOSR5fi9YvJ/xMeQc/wu5Px/pvDh37tvnOrec59L3OY/PU5RlWRYAAACJmjXTBQAAAMwkoQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKQJRQAAQNKEIgAAIGlCEQAAkDShCAAASFreoeiFF16IG2+8MebNmxdFRUXx7LPPfuCa3bt3R11dXZSXl8fChQvj4YcfLqRWABhDXwJgovIORW+99VZcddVV8aMf/eis5h8+fDiuv/76WLlyZXR3d8e3v/3tWLt2bTz11FN5FwsA76UvATBRRVmWZQUvLiqKZ555JlavXj3unG9961vx3HPPxcGDB0fGmpub4ze/+U3s3bu30JcGgDH0JQAKUTLVL7B3795obGwcNXbdddfF9u3b45133onZs2ePWTM8PBzDw8Mjz0+fPh2vv/56zJkzJ4qKiqa6ZAD+V5Zlcfz48Zg3b17MmnV+/BlqIX0pQm8COFdMRW+a8lDU19cXVVVVo8aqqqri5MmT0d/fH9XV1WPWtLa2xubNm6e6NADO0pEjR2L+/PkzXcakKKQvRehNAOeayexNUx6KImLMGbQzV+yNd2Zt48aN0dLSMvJ8cHAwLr/88jhy5EhUVFRMXaEAjDI0NBQLFiyIv/zLv5zpUiZVvn0pQm8COFdMRW+a8lB06aWXRl9f36ixY8eORUlJScyZMyfnmrKysigrKxszXlFRofEAzIDz6fKwQvpShN4EcK6ZzN405ReIL1++PDo6OkaN7dq1K+rr68e9bhsApoq+BMB75R2K3nzzzdi/f3/s378/It79atP9+/dHT09PRLx7ecGaNWtG5jc3N8err74aLS0tcfDgwdixY0ds37497r777sl5BwAkTV8CYKLyvnxu37598YUvfGHk+Znrq2+99dZ47LHHore3d6QRRUTU1tZGe3t7rF+/Ph566KGYN29ePPjgg/GlL31pEsoHIHX6EgATNaH7FE2XoaGhqKysjMHBQddtA0wjx9/x2RuAmTEVx9/z46YTAAAABRKKAACApAlFAABA0oQiAAAgaUIRAACQNKEIAABImlAEAAAkTSgCAACSJhQBAABJE4oAAICkCUUAAEDShCIAACBpQhEAAJA0oQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKQJRQAAQNKEIgAAIGlCEQAAkDShCAAASJpQBAAAJE0oAgAAkiYUAQAASROKAACApAlFAABA0oQiAAAgaUIRAACQNKEIAABImlAEAAAkTSgCAACSJhQBAABJE4oAAICkCUUAAEDShCIAACBpQhEAAJA0oQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKQJRQAAQNKEIgAAIGlCEQAAkDShCAAASJpQBAAAJE0oAgAAkiYUAQAASROKAACApAlFAABA0goKRVu3bo3a2tooLy+Purq66OzsfN/5O3fujKuuuiouvPDCqK6ujttvvz0GBgYKKhgActGbAChU3qGora0t1q1bF5s2bYru7u5YuXJlrFq1Knp6enLOf/HFF2PNmjVxxx13xO9+97v4+c9/Hr/+9a/jzjvvnHDxABChNwEwMXmHogceeCDuuOOOuPPOO2PRokXxL//yL7FgwYLYtm1bzvn/+Z//GR/72Mdi7dq1UVtbG3/zN38TX/3qV2Pfvn0TLh4AIvQmACYmr1B04sSJ6OrqisbGxlHjjY2NsWfPnpxrGhoa4ujRo9He3h5ZlsVrr70WTz75ZNxwww3jvs7w8HAMDQ2NegBALnoTABOVVyjq7++PU6dORVVV1ajxqqqq6Ovry7mmoaEhdu7cGU1NTVFaWhqXXnppfOQjH4kf/vCH475Oa2trVFZWjjwWLFiQT5kAJERvAmCiCvqihaKiolHPsywbM3bGgQMHYu3atXHvvfdGV1dXPP/883H48OFobm4e9+dv3LgxBgcHRx5HjhwppEwAEqI3AVCoknwmz507N4qLi8eceTt27NiYM3RntLa2xooVK+Kee+6JiIjPfOYzcdFFF8XKlSvj/vvvj+rq6jFrysrKoqysLJ/SAEiU3gTAROX1SVFpaWnU1dVFR0fHqPGOjo5oaGjIuebtt9+OWbNGv0xxcXFEvHsWDwAmQm8CYKLyvnyupaUlHnnkkdixY0ccPHgw1q9fHz09PSOXHGzcuDHWrFkzMv/GG2+Mp59+OrZt2xaHDh2Kl156KdauXRtLly6NefPmTd47ASBZehMAE5HX5XMREU1NTTEwMBBbtmyJ3t7eWLx4cbS3t0dNTU1ERPT29o66L8Rtt90Wx48fjx/96EfxD//wD/GRj3wkrr766vjud787ee8CgKTpTQBMRFH2IbhOYGhoKCorK2NwcDAqKipmuhyAZDj+js/eAMyMqTj+FvTtcwAAAOcLoQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKQJRQAAQNKEIgAAIGlCEQAAkDShCAAASJpQBAAAJE0oAgAAkiYUAQAASROKAACApAlFAABA0oQiAAAgaUIRAACQNKEIAABImlAEAAAkTSgCAACSJhQBAABJE4oAAICkCUUAAEDShCIAACBpQhEAAJA0oQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKQJRQAAQNKEIgAAIGlCEQAAkDShCAAASJpQBAAAJE0oAgAAkiYUAQAASROKAACApAlFAABA0oQiAAAgaUIRAACQNKEIAABImlAEAAAkTSgCAACSJhQBAABJE4oAAICkCUUAAEDShCIAACBpQhEAAJA0oQgAAEhaQaFo69atUVtbG+Xl5VFXVxednZ3vO394eDg2bdoUNTU1UVZWFh//+Mdjx44dBRUMALnoTQAUqiTfBW1tbbFu3brYunVrrFixIn784x/HqlWr4sCBA3H55ZfnXHPTTTfFa6+9Ftu3b4+/+qu/imPHjsXJkycnXDwAROhNAExMUZZlWT4Lli1bFkuWLIlt27aNjC1atChWr14dra2tY+Y///zz8eUvfzkOHToUF198cUFFDg0NRWVlZQwODkZFRUVBPwOA/H1Yjr96E0A6puL4m9flcydOnIiurq5obGwcNd7Y2Bh79uzJuea5556L+vr6+N73vheXXXZZXHnllXH33XfHn//853FfZ3h4OIaGhkY9ACAXvQmAicrr8rn+/v44depUVFVVjRqvqqqKvr6+nGsOHToUL774YpSXl8czzzwT/f398bWvfS1ef/31ca/dbm1tjc2bN+dTGgCJ0psAmKiCvmihqKho1PMsy8aMnXH69OkoKiqKnTt3xtKlS+P666+PBx54IB577LFxz8ht3LgxBgcHRx5HjhwppEwAEqI3AVCovD4pmjt3bhQXF48583bs2LExZ+jOqK6ujssuuywqKytHxhYtWhRZlsXRo0fjiiuuGLOmrKwsysrK8ikNgETpTQBMVF6fFJWWlkZdXV10dHSMGu/o6IiGhoaca1asWBF/+tOf4s033xwZe/nll2PWrFkxf/78AkoGgP+jNwEwUXlfPtfS0hKPPPJI7NixIw4ePBjr16+Pnp6eaG5ujoh3Ly9Ys2bNyPybb7455syZE7fffnscOHAgXnjhhbjnnnvi7//+7+OCCy6YvHcCQLL0JgAmIu/7FDU1NcXAwEBs2bIlent7Y/HixdHe3h41NTUREdHb2xs9PT0j8//iL/4iOjo64pvf/GbU19fHnDlz4qabbor7779/8t4FAEnTmwCYiLzvUzQT3AsCYGY4/o7P3gDMjBm/TxEAAMD5RigCAACSJhQBAABJE4oAAICkCUUAAEDShCIAACBpQhEAAJA0oQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKQJRQAAQNKEIgAAIGlCEQAAkDShCAAASJpQBAAAJE0oAgAAkiYUAQAASROKAACApAlFAABA0oQiAAAgaUIRAACQNKEIAABImlAEAAAkTSgCAACSJhQBAABJE4oAAICkCUUAAEDShCIAACBpQhEAAJA0oQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKQJRQAAQNKEIgAAIGlCEQAAkDShCAAASJpQBAAAJE0oAgAAkiYUAQAASROKAACApAlFAABA0oQiAAAgaUIRAACQNKEIAABImlAEAAAkTSgCAACSJhQBAABJKygUbd26NWpra6O8vDzq6uqis7PzrNa99NJLUVJSEp/97GcLeVkAGJfeBECh8g5FbW1tsW7duti0aVN0d3fHypUrY9WqVdHT0/O+6wYHB2PNmjXxt3/7twUXCwC56E0ATERRlmVZPguWLVsWS5YsiW3bto2MLVq0KFavXh2tra3jrvvyl78cV1xxRRQXF8ezzz4b+/fvP+vXHBoaisrKyhgcHIyKiop8ygVgAj4sx1+9CSAdU3H8zeuTohMnTkRXV1c0NjaOGm9sbIw9e/aMu+7RRx+NV155Je67776zep3h4eEYGhoa9QCAXPQmACYqr1DU398fp06diqqqqlHjVVVV0dfXl3PN73//+9iwYUPs3LkzSkpKzup1Wltbo7KycuSxYMGCfMoEICF6EwATVdAXLRQVFY16nmXZmLGIiFOnTsXNN98cmzdvjiuvvPKsf/7GjRtjcHBw5HHkyJFCygQgIXoTAIU6u9Nj/2vu3LlRXFw85szbsWPHxpyhi4g4fvx47Nu3L7q7u+Mb3/hGREScPn06siyLkpKS2LVrV1x99dVj1pWVlUVZWVk+pQGQKL0JgInK65Oi0tLSqKuri46OjlHjHR0d0dDQMGZ+RUVF/Pa3v439+/ePPJqbm+MTn/hE7N+/P5YtWzax6gFInt4EwETl9UlRRERLS0vccsstUV9fH8uXL4+f/OQn0dPTE83NzRHx7uUFf/zjH+NnP/tZzJo1KxYvXjxq/SWXXBLl5eVjxgGgUHoTABORdyhqamqKgYGB2LJlS/T29sbixYujvb09ampqIiKit7f3A+8LAQCTSW8CYCLyvk/RTHAvCICZ4fg7PnsDMDNm/D5FAAAA5xuhCAAASJpQBAAAJE0oAgAAkiYUAQAASROKAACApAlFAABA0oQiAAAgaUIRAACQNKEIAABImlAEAAAkTSgCAACSJhQBAABJE4oAAICkCUUAAEDShCIAACBpQhEAAJA0oQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKQJRQAAQNKEIgAAIGlCEQAAkDShCAAASJpQBAAAJE0oAgAAkiYUAQAASROKAACApAlFAABA0oQiAAAgaUIRAACQNKEIAABImlAEAAAkTSgCAACSJhQBAABJE4oAAICkCUUAAEDShCIAACBpQhEAAJA0oQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKQJRQAAQNKEIgAAIGlCEQAAkDShCAAASFpBoWjr1q1RW1sb5eXlUVdXF52dnePOffrpp+Paa6+Nj370o1FRURHLly+PX/ziFwUXDAC56E0AFCrvUNTW1hbr1q2LTZs2RXd3d6xcuTJWrVoVPT09Oee/8MILce2110Z7e3t0dXXFF77whbjxxhuju7t7wsUDQITeBMDEFGVZluWzYNmyZbFkyZLYtm3byNiiRYti9erV0draelY/49Of/nQ0NTXFvffee1bzh4aGorKyMgYHB6OioiKfcgGYgA/L8VdvAkjHVBx/8/qk6MSJE9HV1RWNjY2jxhsbG2PPnj1n9TNOnz4dx48fj4svvjiflwaAnPQmACaqJJ/J/f39cerUqaiqqho1XlVVFX19fWf1M77//e/HW2+9FTfddNO4c4aHh2N4eHjk+dDQUD5lApAQvQmAiSroixaKiopGPc+ybMxYLk888UR85zvfiba2trjkkkvGndfa2hqVlZUjjwULFhRSJgAJ0ZsAKFReoWju3LlRXFw85szbsWPHxpyhe6+2tra444474t/+7d/immuued+5GzdujMHBwZHHkSNH8ikTgIToTQBMVF6hqLS0NOrq6qKjo2PUeEdHRzQ0NIy77oknnojbbrstHn/88bjhhhs+8HXKysqioqJi1AMActGbAJiovP6mKCKipaUlbrnllqivr4/ly5fHT37yk+jp6Ynm5uaIePdM2h//+Mf42c9+FhHvNp01a9bED37wg/jc5z43cibvggsuiMrKykl8KwCkSm8CYCLyDkVNTU0xMDAQW7Zsid7e3li8eHG0t7dHTU1NRET09vaOui/Ej3/84zh58mR8/etfj69//esj47feems89thjE38HACRPbwJgIvK+T9FMcC8IgJnh+Ds+ewMwM2b8PkUAAADnG6EIAABImlAEAAAkTSgCAACSJhQBAABJE4oAAICkCUUAAEDShCIAACBpQhEAAJA0oQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKQJRQAAQNKEIgAAIGlCEQAAkDShCAAASJpQBAAAJE0oAgAAkiYUAQAASROKAACApAlFAABA0oQiAAAgaUIRAACQNKEIAABImlAEAAAkTSgCAACSJhQBAABJE4oAAICkCUUAAEDShCIAACBpQhEAAJA0oQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKQJRQAAQNKEIgAAIGlCEQAAkDShCAAASJpQBAAAJE0oAgAAkiYUAQAASROKAACApAlFAABA0oQiAAAgaUIRAACQNKEIAABImlAEAAAkraBQtHXr1qitrY3y8vKoq6uLzs7O952/e/fuqKuri/Ly8li4cGE8/PDDBRULAOPRmwAoVN6hqK2tLdatWxebNm2K7u7uWLlyZaxatSp6enpyzj98+HBcf/31sXLlyuju7o5vf/vbsXbt2njqqacmXDwAROhNAExMUZZlWT4Lli1bFkuWLIlt27aNjC1atChWr14dra2tY+Z/61vfiueeey4OHjw4Mtbc3By/+c1vYu/evWf1mkNDQ1FZWRmDg4NRUVGRT7kATMCH5firNwGkYyqOvyX5TD5x4kR0dXXFhg0bRo03NjbGnj17cq7Zu3dvNDY2jhq77rrrYvv27fHOO+/E7Nmzx6wZHh6O4eHhkeeDg4MR8e4GADB9zhx38zx/Nq30JoC0TEVvyisU9ff3x6lTp6KqqmrUeFVVVfT19eVc09fXl3P+yZMno7+/P6qrq8esaW1tjc2bN48ZX7BgQT7lAjBJBgYGorKycqbLyElvAkjTZPamvELRGUVFRaOeZ1k2ZuyD5ucaP2Pjxo3R0tIy8vyNN96Impqa6OnpOWeb8kwYGhqKBQsWxJEjR1y68R72Jjf7Mj57k9vg4GBcfvnlcfHFF890KR9Ibzo3+L+Um30Zn73Jzb6Mbyp6U16haO7cuVFcXDzmzNuxY8fGnHE749JLL805v6SkJObMmZNzTVlZWZSVlY0Zr6ys9EuRQ0VFhX0Zh73Jzb6Mz97kNmvWuXsHB73p3OT/Um72ZXz2Jjf7Mr7J7E15/aTS0tKoq6uLjo6OUeMdHR3R0NCQc83y5cvHzN+1a1fU19fnvGYbAPKhNwEwUXnHq5aWlnjkkUdix44dcfDgwVi/fn309PREc3NzRLx7ecGaNWtG5jc3N8err74aLS0tcfDgwdixY0ds37497r777sl7FwAkTW8CYCLy/puipqamGBgYiC1btkRvb28sXrw42tvbo6amJiIient7R90Xora2Ntrb22P9+vXx0EMPxbx58+LBBx+ML33pS2f9mmVlZXHfffflvGwhZfZlfPYmN/syPnuT24dlX/Smc4d9yc2+jM/e5GZfxjcVe5P3fYoAAADOJ+fuX84CAABMA6EIAABImlAEAAAkTSgCAACSds6Eoq1bt0ZtbW2Ul5dHXV1ddHZ2vu/83bt3R11dXZSXl8fChQvj4YcfnqZKp1c++/L000/HtddeGx/96EejoqIili9fHr/4xS+msdrple/vzBkvvfRSlJSUxGc/+9mpLXCG5Lsvw8PDsWnTpqipqYmysrL4+Mc/Hjt27JimaqdPvvuyc+fOuOqqq+LCCy+M6urquP3222NgYGCaqp0+L7zwQtx4440xb968KCoqimefffYD1zj+5pbKvkToTePRl8anN+WmN401Y30pOwf867/+azZ79uzspz/9aXbgwIHsrrvuyi666KLs1VdfzTn/0KFD2YUXXpjddddd2YEDB7Kf/vSn2ezZs7Mnn3xymiufWvnuy1133ZV997vfzX71q19lL7/8crZx48Zs9uzZ2X//939Pc+VTL9+9OeONN97IFi5cmDU2NmZXXXXV9BQ7jQrZly9+8YvZsmXLso6Ojuzw4cPZf/3Xf2UvvfTSNFY99fLdl87OzmzWrFnZD37wg+zQoUNZZ2dn9ulPfzpbvXr1NFc+9drb27NNmzZlTz31VBYR2TPPPPO+8x1/0+5LWaY3jUdfGp/elJvelNtM9aVzIhQtXbo0a25uHjX2yU9+MtuwYUPO+f/4j/+YffKTnxw19tWvfjX73Oc+N2U1zoR89yWXT33qU9nmzZsnu7QZV+jeNDU1Zf/0T/+U3Xfffedl88l3X/793/89q6yszAYGBqajvBmT77788z//c7Zw4cJRYw8++GA2f/78KavxXHA2zcfxN+2+lGV603j0pfHpTbnpTR9sOvvSjF8+d+LEiejq6orGxsZR442NjbFnz56ca/bu3Ttm/nXXXRf79u2Ld955Z8pqnU6F7Mt7nT59Oo4fPx4XX3zxVJQ4Ywrdm0cffTReeeWVuO+++6a6xBlRyL4899xzUV9fH9/73vfisssuiyuvvDLuvvvu+POf/zwdJU+LQvaloaEhjh49Gu3t7ZFlWbz22mvx5JNPxg033DAdJZ/THH/T7UsRetN49KXx6U256U2TZ7KOvyWTXVi++vv749SpU1FVVTVqvKqqKvr6+nKu6evryzn/5MmT0d/fH9XV1VNW73QpZF/e6/vf/3689dZbcdNNN01FiTOmkL35/e9/Hxs2bIjOzs4oKZnxX/spUci+HDp0KF588cUoLy+PZ555Jvr7++NrX/tavP766+fNtduF7EtDQ0Ps3Lkzmpqa4n/+53/i5MmT8cUvfjF++MMfTkfJ5zTH33T7UoTeNB59aXx6U2560+SZrOPvjH9SdEZRUdGo51mWjRn7oPm5xj/s8t2XM5544on4zne+E21tbXHJJZdMVXkz6mz35tSpU3HzzTfH5s2b48orr5yu8mZMPr8zp0+fjqKioti5c2csXbo0rr/++njggQfiscceO6/OyEXkty8HDhyItWvXxr333htdXV3x/PPPx+HDh6O5uXk6Sj3nOf6e/fxc4+cDvSk3fWl8elNuetPkmIzj74yfmpg7d24UFxePScXHjh0bk/rOuPTSS3POLykpiTlz5kxZrdOpkH05o62tLe644474+c9/Htdcc81Uljkj8t2b48ePx759+6K7uzu+8Y1vRMS7B9wsy6KkpCR27doVV1999bTUPpUK+Z2prq6Oyy67LCorK0fGFi1aFFmWxdGjR+OKK66Y0pqnQyH70traGitWrIh77rknIiI+85nPxEUXXRQrV66M+++//7w5618Ix990+1KE3jQefWl8elNuetPkmazj74x/UlRaWhp1dXXR0dExaryjoyMaGhpyrlm+fPmY+bt27Yr6+vqYPXv2lNU6nQrZl4h3z8Lddttt8fjjj5+315jmuzcVFRXx29/+Nvbv3z/yaG5ujk984hOxf//+WLZs2XSVPqUK+Z1ZsWJF/OlPf4o333xzZOzll1+OWbNmxfz586e03ulSyL68/fbbMWvW6MNjcXFxRPzf2adUOf6m25ci9Kbx6Evj05ty05smz6Qdf/P6WoYpcuYrCbdv354dOHAgW7duXXbRRRdlf/jDH7Isy7INGzZkt9xyy8j8M1+9t379+uzAgQPZ9u3bz8uvPs13Xx5//PGspKQke+ihh7Le3t6RxxtvvDFTb2HK5Ls373W+fstPvvty/PjxbP78+dnf/d3fZb/73e+y3bt3Z1dccUV25513ztRbmBL57sujjz6alZSUZFu3bs1eeeWV7MUXX8zq6+uzpUuXztRbmDLHjx/Puru7s+7u7iwisgceeCDr7u4e+UpYx1996b30ptz0pfHpTbnpTbnNVF86J0JRlmXZQw89lNXU1GSlpaXZkiVLst27d4/826233pp9/vOfHzX/l7/8ZfbXf/3XWWlpafaxj30s27Zt2zRXPD3y2ZfPf/7zWUSMedx6663TX/g0yPd35v93PjeffPfl4MGD2TXXXJNdcMEF2fz587OWlpbs7bffnuaqp16++/Lggw9mn/rUp7ILLrggq66uzr7yla9kR48eneaqp95//Md/vO9xw/FXX8pFb8pNXxqf3pSb3jTWTPWloixL+PM2AAAgeTP+N0UAAAAzSSgCAACSJhQBAABJE4oAAICkCUUAAEDShCIAACBpQhEAAJA0oQgAAEiaUAQAACRNKAIAAJImFAEAAEkTigAAgKT9P0oDfHCbQv3BAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m display(fig)  \u001b[38;5;66;03m# 初始顯示圖形\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 49\u001b[0m     train_loss, train_spearman \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     val_loss, val_spearman \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, loss_fn, device)\n\u001b[1;32m     52\u001b[0m     axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/Desktop/GitHub/HEVisum/operate_model.py:54\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, loss_fn, device, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m all_preds, all_targets \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     52\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     55\u001b[0m     inputs, label \u001b[38;5;241m=\u001b[39m make_input_to_device(model, batch, device)\n\u001b[1;32m     56\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[63], line 91\u001b[0m, in \u001b[0;36mimportDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_keys:\n\u001b[0;32m---> 91\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m         value \u001b[38;5;241m=\u001b[39m convert_item(value, is_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[63], line 26\u001b[0m, in \u001b[0;36mconvert_item\u001b[0;34m(item, is_image)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     converted_list \u001b[38;5;241m=\u001b[39m [convert_item(elem, is_image\u001b[38;5;241m=\u001b[39mis_image) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m item]\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m轉換列表中的元素失敗，列表內容: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 🔧 設定裝置\n",
    "# ---------------------------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 初始化模型 & 優化器\n",
    "# ---------------------------\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "# loss_fn = spearman_loss\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# ---------------------------\n",
    "# 儲存 log 的設定\n",
    "# ---------------------------\n",
    "log_path = os.path.join(save_folder, \"training_log.csv\")\n",
    "log_file = open(log_path, mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# ---------------------------\n",
    "# 用來畫圖的變數\n",
    "# ---------------------------\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_spearmanes = []\n",
    "val_spearmanes = []\n",
    "\n",
    "# ---------------------------\n",
    "# 指定最佳模型儲存路徑\n",
    "# ---------------------------\n",
    "best_model_path = os.path.join(save_folder, \"best_model.pt\")\n",
    "loss_plot_path = os.path.join(save_folder, \"loss_curve.png\")\n",
    "\n",
    "# ---------------------------\n",
    "# 開始訓練\n",
    "# ---------------------------\n",
    "num_epochs = 500\n",
    "best_val_loss = float('inf')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "display(fig)  # 初始顯示圖形\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_spearman = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_spearman = evaluate(model, val_loader, loss_fn, device)\n",
    "    \n",
    "    axes[0].clear()\n",
    "    axes[1].clear()\n",
    "\n",
    "    # 儲存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\"✅ Saved best model!\")\n",
    "\n",
    "    # 調整學習率\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 寫入 CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch + 1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # 印出 Epoch 結果\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | ρ: {val_spearman:.4f} | lr: {lr:.2e}\")\n",
    "\n",
    "    # 更新 log 列表並畫圖\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_spearmanes.append(train_spearman)\n",
    "    val_spearmanes.append(val_spearman)\n",
    "\n",
    "    # 更新 Loss 圖\n",
    "    plot_losses(train_losses, val_losses, ax=axes[0], title=\"MSE Loss\")\n",
    "    # 更新 Spearman 圖\n",
    "    plot_losses(train_spearmanes, val_spearmanes, ax=axes[1], title=\"Spearman Loss\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    clear_output(wait=True)  # 清除之前的輸出\n",
    "    display(fig)\n",
    "    plt.pause(0.1)  # 暫停以便更新畫面\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"⛔ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# 關閉 CSV log 檔案\n",
    "log_file.close()\n",
    "\n",
    "# 儲存最終圖形\n",
    "fig.savefig(loss_plot_path)\n",
    "plt.close(fig)\n",
    "print(f\"訓練結束，loss 曲線圖已儲存至 {loss_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 需要的 Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_87169/3739314886.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"./test_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import inspect\n",
    "from python_scripts.import_data import importDataset, preprocess_data\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# 正確方式\n",
    "\n",
    "test_data = torch.load(\"./test_dataset.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in test_data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "test_dataset = {\n",
    "        'center_tile': test_data['tiles'],\n",
    "        'subtiles': test_data['subtiles'],\n",
    "        'neighbor_tiles': test_data['neighbor_tiles'],\n",
    "        'coords': normalized_coords,\n",
    "        'label': np.zeros((len(test_data['tiles']), 35))\n",
    "    }\n",
    "\n",
    "\n",
    "processed_data = preprocess_data(test_dataset, image_keys, my_transform)\n",
    "\n",
    "test_dataset = importDataset(\n",
    "        data_dict=test_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset.check_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = predict(model, test_loader, device)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from revert_predict_data import revert_prediction_array\n",
    "\n",
    "test_preds＿re = revert_prediction_array(test_preds)\n",
    "test_preds＿re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== 讀取 test spot index 用於對應 ID ====\n",
    "with h5py.File(\"./elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "submission_path = os.path.join(save_folder, \"submission.csv\")\n",
    "\n",
    "ensemble_df = pd.DataFrame(test_preds＿re, columns=[f\"C{i+1}\" for i in range(test_preds＿re.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(submission_path, index=False)\n",
    "print(f\"✅ Saved submission.csv in {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
