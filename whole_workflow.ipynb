{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, out_dim, in_channels=3, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(64, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiTaskDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=35, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(negative_slope)\n",
    "        )\n",
    "        self.heads = nn.ModuleList([nn.Linear(64, 1) for _ in range(output_dim)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        outs = [head(shared) for head in self.heads]\n",
    "        return torch.cat(outs, dim=1)  # (B, 35)\n",
    "\n",
    "\n",
    "class VisionMLP_MultiTask(nn.Module):\n",
    "    def __init__(self, cnn_out_dim=64, output_dim=35, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.encoder_tile = CNNEncoder(cnn_out_dim, in_channels=3, negative_slope=negative_slope)\n",
    "        self.encoder_neighbors = CNNEncoder(cnn_out_dim, in_channels=3, negative_slope=negative_slope)\n",
    "        self.encoder_subtiles = CNNEncoder(cnn_out_dim, in_channels=3, negative_slope=negative_slope)\n",
    "\n",
    "        self.decoder = MultiTaskDecoder(input_dim=cnn_out_dim * 3 + 2,\n",
    "                                        output_dim=output_dim,\n",
    "                                        negative_slope=negative_slope)\n",
    "\n",
    "    def forward(self, tile, subtiles, neighbors, coords):\n",
    "        B = tile.size(0)\n",
    "\n",
    "        # tile: [B, 3, 78, 78]\n",
    "        f_tile = self.encoder_tile(tile)\n",
    "\n",
    "        # subtiles: [B, 9, 3, 26, 26]\n",
    "        B, N, C, H, W = subtiles.shape\n",
    "        subtiles_reshaped = subtiles.contiguous().reshape(B * N, C, H, W)  # ‚úÖ Âº∑Âà∂ contiguous\n",
    "        f_sub = self.encoder_subtiles(subtiles_reshaped).reshape(B, N, -1).mean(dim=1)  # ‚úÖ reshape\n",
    "\n",
    "        # neighbors: [B, 8, 3, 78, 78]\n",
    "        B, N, C, H, W = neighbors.shape\n",
    "        neighbors_reshaped = neighbors.contiguous().reshape(B * N, C, H, W)  # ‚úÖ Âº∑Âà∂ contiguous\n",
    "        f_neigh = self.encoder_neighbors(neighbors_reshaped).reshape(B, N, -1).mean(dim=1)  # ‚úÖ reshape\n",
    "\n",
    "        x = torch.cat([f_tile, f_sub, f_neigh, coords], dim=1)\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = VisionMLP_MultiTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ËºâÂÖ•ÁöÑ class ÂêçÁ®±: ['CNNEncoder', 'MLPDecoder', 'VisionMLPModelWithCoord']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from python_scripts.import_model import load_model_classes\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "\n",
    "# ==============================================\n",
    "# ÁØÑ‰æã‰ΩøÁî®\n",
    "# ==============================================\n",
    "folder = \"./output_folder/CNN+MLP/\"  # ÊõøÊèõÊàêÂØ¶ÈöõÁöÑË≥áÊñôÂ§æË∑ØÂæëÔºåË©≤Ë∑ØÂæë‰∏ãÊáâÊúâ model.py\n",
    "try:\n",
    "    loaded_classes = load_model_classes(folder)\n",
    "    print(\"ËºâÂÖ•ÁöÑ class ÂêçÁ®±:\", list(loaded_classes.keys()))\n",
    "except Exception as e:\n",
    "    print(\"ËºâÂÖ•Ê®°Âûã class ÁôºÁîüÈåØË™§:\", e)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Signature (center_tile, subtiles, neighbor_tiles, coords)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÂÅáË®≠ loaded_classes ÊòØ‰Ω†Â∑≤Á∂ìÂæû model.py ËºâÂÖ•ÁöÑ class Â≠óÂÖ∏\n",
    "name_of_class = 'VisionMLPModelWithCoord'\n",
    "ModelClass = loaded_classes.get(name_of_class)\n",
    "if ModelClass is None:\n",
    "    raise ValueError(f\"Êâæ‰∏çÂà∞ {name_of_class} ÈÄôÂÄã class\")\n",
    "# ÈÄôË£°ÂëºÂè´ ModelClass() Âª∫Á´ãÂØ¶‰æãÔºåÊ≥®ÊÑè‰∏çËÉΩÁõ¥Êé•Áî® name_of_class() (Âõ†ÁÇ∫ÂÆÉÊòØ‰∏ÄÂÄãÂ≠ó‰∏≤)\n",
    "model = ModelClass()  # Â¶ÇÊûúÈúÄË¶ÅÂèÉÊï∏ÔºåË´ãÂú®Ê≠§ËôïÂÇ≥ÂÖ•\n",
    "get_model_inputs(model)\n",
    "#print(\"Âª∫Á´ãÁöÑÊ®°ÂûãÂØ¶‰æã:\", model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same in multiple .pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_29090/3115120493.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def load_grouped_tile_data(folder_path):\n",
    "    neighbors_list = []\n",
    "    tiles_list = []\n",
    "    subtiles_list = []\n",
    "    labels_list = []\n",
    "    coords_list = []\n",
    "    #norm_coords_list = []\n",
    "\n",
    "    pt_files = sorted([\n",
    "        f for f in os.listdir(folder_path)\n",
    "        if f.endswith(\".pt\") and not f.startswith(\"original\")\n",
    "    ])\n",
    "\n",
    "    for fname in pt_files:\n",
    "        file_path = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            data = torch.load(file_path, map_location=\"cpu\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {fname}: {e}\")\n",
    "            continue  # skip this file\n",
    "\n",
    "        neighbors_list.append(data[\"neighbors\"])\n",
    "        tiles_list.append(data[\"tile\"])\n",
    "        subtiles_list.append(data[\"subtiles\"])\n",
    "        labels_list.append(data[\"label\"])\n",
    "        coords_list.append(data[\"coord\"])\n",
    "        #norm_coords_list.append(data[\"norm_coord\"])\n",
    "    \n",
    "    return {\n",
    "        \"neighbors\": neighbors_list,\n",
    "        \"tile\": tiles_list,\n",
    "        \"subtiles\": subtiles_list,\n",
    "        \"label\": labels_list,\n",
    "        \"coords\": coords_list,\n",
    "        #\"norm_coord\": norm_coords_list\n",
    "    }\n",
    "\n",
    "\n",
    "# üß™ Áî®Ê≥ï\n",
    "grouped_data = load_grouped_tile_data(\"./dataset/3-channel/78-3sizes-3*3/train/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (tile, subtiles, neighbors, coords)\n"
     ]
    }
   ],
   "source": [
    "from python_scripts.import_data import importDataset\n",
    "\n",
    "image_keys = ['neighbors', 'tile', 'subtiles']\n",
    "\n",
    "train_dataset = importDataset(\n",
    "        data_dict=grouped_data,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking dataset sample: 1000\n",
      "üìè tile shape: torch.Size([3, 78, 78]) | dtype: torch.float32 | min: 0.291, max: 0.992, mean: 0.860, std: 0.139\n",
      "üìè subtiles shape: torch.Size([9, 3, 26, 26]) | dtype: torch.float32 | min: 0.291, max: 0.992, mean: 0.860, std: 0.139\n",
      "üìè neighbors shape: torch.Size([8, 3, 78, 78]) | dtype: torch.float32 | min: 0.330, max: 1.000, mean: 0.855, std: 0.146\n",
      "üìè coords shape: torch.Size([2]) | dtype: torch.float32 | min: 1527.000, max: 1709.000, mean: 1618.000, std: 128.693\n",
      "--- coords head (Ââç 10 ÂÄãÂÖÉÁ¥†):\n",
      "tensor([1527., 1709.])\n",
      "üìè label shape: torch.Size([35]) | dtype: torch.float32 | min: -0.744, max: 2.278, mean: -0.254, std: 0.572\n",
      "--- label head (Ââç 10 ÂÄãÂÖÉÁ¥†):\n",
      "tensor([-0.6705,  0.0176, -0.6396, -0.6044,  2.2777, -0.6090,  0.4683, -0.3074,\n",
      "        -0.5801, -0.2711])\n",
      "‚úÖ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "train_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train: 26179 samples\n",
      "‚úÖ Val: 6545 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Ë®≠ÂÆöÊØî‰æã\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# ÊãÜÂàÜ Dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_set)} samples\")\n",
    "print(f\"‚úÖ Val: {len(val_set)} samples\")\n",
    "\n",
    "# üîπ Â∞áÂÖ∂ÂåÖÊàê DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save in one pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_19297/1962845458.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tile = torch.load(\"./dataset/final_data/M_tiles.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_19297/1962845458.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  label = torch.load(\"./dataset/final_data/gu_log2_labels.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_19297/1962845458.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta = torch.load(\"./dataset/final_data/meta_info.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (tile, coords)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from python_scripts.import_data import importDataset, preprocess_data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# ËºâÂÖ•Ë≥áÊñô\n",
    "#tile = torch.load(\"./dataset/final_data/M_tiles.pt\")\n",
    "#subtiles = torch.load(\"./train_dataset_sep_v2/subtiles.pt\")\n",
    "#neighbor_tiles = torch.load(\"./train_dataset_sep_v2/neighbor_tiles.pt\")\n",
    "#label = torch.load(\"./dataset/final_data/gu_log2_labels.pt\")\n",
    "#meta = torch.load(\"./dataset/final_data/meta_info.pt\")\n",
    "\n",
    "\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in grouped_data['coords']:\n",
    "    if _meta is not None:\n",
    "         x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "train_dataset = {\n",
    "        'tile': tile,\n",
    "        #'subtiles': subtiles,\n",
    "        #'neighbor_tiles': neighbor_tiles,\n",
    "        'coords': normalized_coords,\n",
    "        'label': label\n",
    "    }\n",
    "\n",
    "my_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "\n",
    "image_keys = ['tile']\n",
    "\n",
    "#processed_data = preprocess_data(train_dataset, image_keys, my_transform)\n",
    "\n",
    "train_dataset = importDataset(\n",
    "        data_dict=train_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m importDataset(\n\u001b[0;32m----> 2\u001b[0m         data_dict\u001b[38;5;241m=\u001b[39m\u001b[43mprocessed_data\u001b[49m,\n\u001b[1;32m      3\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      4\u001b[0m         image_keys\u001b[38;5;241m=\u001b[39mimage_keys,\n\u001b[1;32m      5\u001b[0m         transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x,  \u001b[38;5;66;03m# identity transform\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         print_sig\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = importDataset(\n",
    "        data_dict=processed_data,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ËΩâÊèõÊ¨Ñ‰Ωç 'tile' ÁöÑË≥áÊñôÁÇ∫ tensor ÊôÇÂá∫ÈåØÔºåË≥áÊñôÂÖßÂÆπ: [[[0.9678768  0.954182   0.9747549 ]\n  [0.9704657  0.95502454 0.9764706 ]\n  [0.9704657  0.95502454 0.9764706 ]\n  ...\n  [0.9767846  0.95091146 0.9703432 ]\n  [0.9713082  0.9556219  0.96738666]\n  [0.9646906  0.95493263 0.9654565 ]]\n\n [[0.9689032  0.95465684 0.97550553]\n  [0.97071075 0.95477945 0.9764706 ]\n  [0.96967673 0.9537454  0.9754366 ]\n  ...\n  [0.97511876 0.9502413  0.97131205]\n  [0.9694317  0.95374537 0.96551013]\n  [0.9623774  0.9550245  0.9647059 ]]\n\n [[0.9686274  0.95686275 0.9764706 ]\n  [0.9676509  0.9558862  0.975494  ]\n  [0.96568245 0.95391774 0.97352564]\n  ...\n  [0.9730354  0.9502413  0.9730354 ]\n  [0.9664216  0.9529412  0.9647059 ]\n  [0.96134347 0.9550245  0.9647059 ]]\n\n ...\n\n [[0.8678539  0.6751915  0.78875613]\n  [0.89483    0.7065947  0.8041437 ]\n  [0.91975725 0.7323836  0.8442517 ]\n  ...\n  [0.68689877 0.554902   0.77635956]\n  [0.68630135 0.5452283  0.7674058 ]\n  [0.7217984  0.57002914 0.78608304]]\n\n [[0.8391927  0.6464614  0.7745404 ]\n  [0.89067096 0.6948759  0.81308216]\n  [0.9337929  0.73981315 0.86159235]\n  ...\n  [0.70375305 0.578508   0.7847044 ]\n  [0.70224416 0.5752834  0.77957267]\n  [0.7377298  0.60462624 0.80364585]]\n\n [[0.7855086  0.59163606 0.73481923]\n  [0.8534467  0.65076596 0.78898597]\n  [0.91767    0.7181602  0.8506357 ]\n  ...\n  [0.7173024  0.59690565 0.7941943 ]\n  [0.7302696  0.6088082  0.8036765 ]\n  [0.7664675  0.6419577  0.8292126 ]]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/GitHub/HEVisum/python_scripts/import_data.py:192\u001b[0m, in \u001b[0;36mimportDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_keys:\n\u001b[0;32m--> 192\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/GitHub/HEVisum/python_scripts/import_data.py:123\u001b[0m, in \u001b[0;36mconvert_item\u001b[0;34m(item, is_image)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_item\u001b[39m(item, is_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_image:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GitHub/HEVisum/python_scripts/import_data.py:230\u001b[0m, in \u001b[0;36mimportDataset.check_item\u001b[0;34m(self, idx, num_lines)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03mÊ™¢Êü•Á¨¨ idx Á≠ÜË≥áÊñô‰∏≠ÊØèÂÄãÊ¨Ñ‰ΩçÁöÑË©≥Á¥∞Ë≥áË®ä„ÄÇ\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03mÂ∞çÊØèÂÄãÊ¨Ñ‰ΩçÔºà‰æùÊìö forward_keys Âä†‰∏ä 'label'ÔºâÂç∞Âá∫Ôºö\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m  - Â∞çÊñºÈùûÂúñÁâáË≥áÊñôÔºåÂç∞Âá∫Ë©≤ tensor Ââç num_lines Âàó/ÂÖÉÁ¥†ÁöÑÂÖßÂÆπ„ÄÇ\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m expected_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_keys \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 230\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç Checking dataset sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n",
      "File \u001b[0;32m~/Desktop/GitHub/HEVisum/python_scripts/import_data.py:196\u001b[0m, in \u001b[0;36mimportDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    194\u001b[0m         value \u001b[38;5;241m=\u001b[39m convert_item(value, is_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mËΩâÊèõÊ¨Ñ‰Ωç \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ÁöÑË≥áÊñôÁÇ∫ tensor ÊôÇÂá∫ÈåØÔºåË≥áÊñôÂÖßÂÆπ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# ËΩâÊèõÊàê float32\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[0;31mValueError\u001b[0m: ËΩâÊèõÊ¨Ñ‰Ωç 'tile' ÁöÑË≥áÊñôÁÇ∫ tensor ÊôÇÂá∫ÈåØÔºåË≥áÊñôÂÖßÂÆπ: [[[0.9678768  0.954182   0.9747549 ]\n  [0.9704657  0.95502454 0.9764706 ]\n  [0.9704657  0.95502454 0.9764706 ]\n  ...\n  [0.9767846  0.95091146 0.9703432 ]\n  [0.9713082  0.9556219  0.96738666]\n  [0.9646906  0.95493263 0.9654565 ]]\n\n [[0.9689032  0.95465684 0.97550553]\n  [0.97071075 0.95477945 0.9764706 ]\n  [0.96967673 0.9537454  0.9754366 ]\n  ...\n  [0.97511876 0.9502413  0.97131205]\n  [0.9694317  0.95374537 0.96551013]\n  [0.9623774  0.9550245  0.9647059 ]]\n\n [[0.9686274  0.95686275 0.9764706 ]\n  [0.9676509  0.9558862  0.975494  ]\n  [0.96568245 0.95391774 0.97352564]\n  ...\n  [0.9730354  0.9502413  0.9730354 ]\n  [0.9664216  0.9529412  0.9647059 ]\n  [0.96134347 0.9550245  0.9647059 ]]\n\n ...\n\n [[0.8678539  0.6751915  0.78875613]\n  [0.89483    0.7065947  0.8041437 ]\n  [0.91975725 0.7323836  0.8442517 ]\n  ...\n  [0.68689877 0.554902   0.77635956]\n  [0.68630135 0.5452283  0.7674058 ]\n  [0.7217984  0.57002914 0.78608304]]\n\n [[0.8391927  0.6464614  0.7745404 ]\n  [0.89067096 0.6948759  0.81308216]\n  [0.9337929  0.73981315 0.86159235]\n  ...\n  [0.70375305 0.578508   0.7847044 ]\n  [0.70224416 0.5752834  0.77957267]\n  [0.7377298  0.60462624 0.80364585]]\n\n [[0.7855086  0.59163606 0.73481923]\n  [0.8534467  0.65076596 0.78898597]\n  [0.91767    0.7181602  0.8506357 ]\n  ...\n  [0.7173024  0.59690565 0.7941943 ]\n  [0.7302696  0.6088082  0.8036765 ]\n  [0.7664675  0.6419577  0.8292126 ]]]"
     ]
    }
   ],
   "source": [
    "train_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train: 26179 samples\n",
      "‚úÖ Val: 6545 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Ë®≠ÂÆöÊØî‰æã\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# ÊãÜÂàÜ Dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_set)} samples\")\n",
    "print(f\"‚úÖ Val: {len(val_set)} samples\")\n",
    "\n",
    "# üîπ Â∞áÂÖ∂ÂåÖÊàê DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from python_scripts.operate_model import get_model_inputs, train_one_epoch, evaluate, predict, EarlyStopping, plot_losses, plot_per_cell_metrics\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------------------\n",
    "# ÊåáÂÆöÂÑ≤Â≠òË≥áÊñôÂ§æ\n",
    "# ---------------------------\n",
    "save_folder = \"output_folder/with_all_preprocess_log2/GU+CNN+MTD_3size_78/\"  # ‰øÆÊîπÁÇ∫‰Ω†ÊÉ≥Ë¶ÅÁöÑË≥áÊñôÂ§æÂêçÁ®±\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_fn(loss_type=\"mse\", cell_weights=None):\n",
    "    \"\"\"\n",
    "    ÊîØÊè¥Â§öÁ®Æ loss functionÔºåÂåÖÂê´:\n",
    "    - \"mse\": Mean Squared Error (È†êË®≠)\n",
    "    - \"weighted_mse\": Ê†πÊìö cell_weights ÂÅöÂä†Ê¨äÁöÑ MSE\n",
    "    - \"mae\": Mean Absolute Error\n",
    "    - \"spearman\": Spearman loss (Èùû differentiableÔºå‰ΩÜÂèØ‰ª•ÂØ¶È©ó)\n",
    "\n",
    "    ÂõûÂÇ≥Â∞çÊáâÁöÑ loss function„ÄÇ\n",
    "    \"\"\"\n",
    "    loss_type = loss_type.lower()\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        return nn.MSELoss()\n",
    "    \n",
    "    elif loss_type == \"mae\":\n",
    "        return nn.L1Loss()\n",
    "\n",
    "    elif loss_type == \"weighted_mse\":\n",
    "        if cell_weights is None:\n",
    "            raise ValueError(\"ÈúÄË¶ÅÊèê‰æõ cell_weights ÊâçËÉΩ‰ΩøÁî® weighted MSE\")\n",
    "\n",
    "        def weighted_mse(pred, target):\n",
    "            loss = (pred - target) ** 2  # (B, C)\n",
    "            loss = loss.mean(dim=0)      # Â∞ç batch Âπ≥Âùá ‚Üí (C,)\n",
    "            weighted_loss = (loss * cell_weights.to(pred.device)).mean()\n",
    "            return weighted_loss\n",
    "\n",
    "        return weighted_mse\n",
    "\n",
    "    elif loss_type == \"spearman\":\n",
    "        from scipy.stats import spearmanr\n",
    "\n",
    "        def spearman_loss(pred, target):\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            target = target.detach().cpu().numpy()\n",
    "            rho = np.mean([spearmanr(pred[:, i], target[:, i])[0] for i in range(pred.shape[1])])\n",
    "            return 1 - rho  # Ê®°Êì¨ loss Ë∂äÂ∞èË∂äÂ•ΩÔºàÈùû differentiableÔºâ\n",
    "        \n",
    "        return spearman_loss\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"‰∏çÊîØÊè¥ÁöÑ loss_type: {loss_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2118, 1.4699, 1.2259, 1.2525, 1.2928, 1.3897, 1.4408, 1.8691, 1.2647,\n",
       "        1.4540, 1.6404, 1.3991, 2.0000, 1.7003, 1.2916, 1.2448, 1.5875, 1.3241,\n",
       "        1.8716, 1.6964, 1.7266, 1.9636, 1.4970, 1.4547, 1.5564, 1.8343, 1.2534,\n",
       "        1.7102, 1.6177, 1.4917, 1.4073, 1.4435, 1.9684, 1.7002, 1.4398])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_weights = torch.tensor(mse_per_cell)\n",
    "cell_weights = cell_weights / cell_weights.max()   # scale Âà∞ [0, 1]\n",
    "cell_weights = cell_weights + 1.0                  # shift Âà∞ [1, 2]\n",
    "cell_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3966, 0.8798, 0.4230, 0.4727, 0.5482, 0.7296, 0.8255, 1.6273, 0.4956,\n",
       "        0.8500, 1.1992, 0.7474, 1.8725, 1.3113, 0.5461, 0.4584, 1.1001, 0.6069,\n",
       "        1.6320, 1.3040, 1.3606, 1.8043, 0.9306, 0.8514, 1.0419, 1.5623, 0.4745,\n",
       "        1.3299, 1.1567, 0.9208, 0.7626, 0.8305, 1.8133, 1.3111, 0.8236])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_weights = torch.tensor(mse_per_cell)\n",
    "cell_weights = cell_weights / cell_weights.mean()  # ÂùáÂÄºÁÇ∫ 1ÔºåÁ∏ΩÂíåÁÇ∫ 35 Â∑¶Âè≥\n",
    "cell_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: mps\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|‚ñè         | 13/819 [00:03<02:17,  5.86it/s, avg=19.8, loss=0.773]"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# üîß Ë®≠ÂÆöË£ùÁΩÆ\n",
    "# ---------------------------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "# ---------------------------\n",
    "# ÂàùÂßãÂåñÊ®°Âûã & ÂÑ™ÂåñÂô®\n",
    "# ---------------------------\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "# loss_fn = spearman_loss\n",
    "#LOSS_TYPE = \"weighted_mse\"  # Êàñ \"mse\", \"mae\", \"spearman\"\n",
    "\n",
    "\n",
    "#loss_fn = get_loss_fn(loss_type=LOSS_TYPE, cell_weights=cell_weights)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# ---------------------------\n",
    "# ÂÑ≤Â≠ò log ÁöÑË®≠ÂÆö\n",
    "# ---------------------------\n",
    "log_path = os.path.join(save_folder, \"training_log.csv\")\n",
    "log_file = open(log_path, mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# ---------------------------\n",
    "# Áî®‰æÜÁï´ÂúñÁöÑËÆäÊï∏\n",
    "# ---------------------------\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_spearmanes = []\n",
    "val_spearmanes = []\n",
    "\n",
    "# ---------------------------\n",
    "# ÊåáÂÆöÊúÄ‰Ω≥Ê®°ÂûãÂÑ≤Â≠òË∑ØÂæë\n",
    "# ---------------------------\n",
    "best_model_path = os.path.join(save_folder, \"best_model.pt\")\n",
    "loss_plot_path = os.path.join(save_folder, \"loss_curve.png\")\n",
    "\n",
    "# ---------------------------\n",
    "# ÈñãÂßãË®ìÁ∑¥\n",
    "# ---------------------------\n",
    "num_epochs = 500\n",
    "best_val_loss = float('inf')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "display(fig)  # ÂàùÂßãÈ°ØÁ§∫ÂúñÂΩ¢\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_spearman = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_spearman, mse_per_cell, spearman_per_cell = evaluate(model, val_loader, loss_fn, device)\n",
    "    \n",
    "    axes[0][0].clear()\n",
    "    axes[0][1].clear()\n",
    "    axes[1][0].clear()\n",
    "    axes[1][1].clear()\n",
    "    # ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°Âûã\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\"‚úÖ Saved best model!\")\n",
    "\n",
    "    # Ë™øÊï¥Â≠∏ÁøíÁéá\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # ÂØ´ÂÖ• CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch + 1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # Âç∞Âá∫ Epoch ÁµêÊûú\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | œÅ: {val_spearman:.4f} | lr: {lr:.2e}\")\n",
    "\n",
    "    # Êõ¥Êñ∞ log ÂàóË°®‰∏¶Áï´Âúñ\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_spearmanes.append(train_spearman)\n",
    "    val_spearmanes.append(val_spearman)\n",
    "    \n",
    "\n",
    "\n",
    "    # plot loss\n",
    "    plot_losses(train_losses, val_losses, ax=axes[0][0], title=\"MSE Loss\")\n",
    "    plot_losses(train_spearmanes, val_spearmanes, ax=axes[0][1], title=\"Spearman Loss\")\n",
    "\n",
    "    cell_names = [f\"C{i+1}\" for i in range(35)]\n",
    "    # plot per-cell stats\n",
    "    plot_per_cell_metrics(mse_per_cell, spearman_per_cell,cell_names,\n",
    "                        ax_mse=axes[1][0], ax_spearman=axes[1][1])\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    clear_output(wait=True)  # Ê∏ÖÈô§‰πãÂâçÁöÑËº∏Âá∫\n",
    "    display(fig)\n",
    "    plt.pause(0.1)  # Êö´ÂÅú‰ª•‰æøÊõ¥Êñ∞Áï´Èù¢\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"‚õî Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# ÈóúÈñâ CSV log Ê™îÊ°à\n",
    "log_file.close()\n",
    "\n",
    "# ÂÑ≤Â≠òÊúÄÁµÇÂúñÂΩ¢\n",
    "fig.savefig(loss_plot_path)\n",
    "plt.close(fig)\n",
    "print(f\"Ë®ìÁ∑¥ÁµêÊùüÔºåloss Êõ≤Á∑öÂúñÂ∑≤ÂÑ≤Â≠òËá≥ {loss_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_1688/3109279275.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionWithCoord_MTL(\n",
       "  (encoder): SimpleCNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (12): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU()\n",
       "      (15): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): MultiTaskDecoder(\n",
       "    (shared): Sequential(\n",
       "      (0): Linear(in_features=66, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (7): ReLU()\n",
       "    )\n",
       "    (heads): ModuleList(\n",
       "      (0-34): 35 x Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== ÈúÄË¶ÅÁöÑ Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "import os\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "best_model_path = os.path.join(save_folder, \"best_model.pt\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_1688/1705579048.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"./dataset/final_data/test_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (tile, coords)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import inspect\n",
    "from python_scripts.import_data import importDataset, preprocess_data\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Ê≠£Á¢∫ÊñπÂºè\n",
    "\n",
    "test_data = torch.load(\"./dataset/final_data/test_dataset.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in test_data['coords']:\n",
    "    if _meta is not None:\n",
    "        x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "test_dataset = {\n",
    "        'tile': torch.stack([torch.tensor(img).permute(2, 0, 1) for img in test_data['M_tiles']]),\n",
    "        #'subtiles': test_data['subtiles'],\n",
    "        #'neighbor_tiles': test_data['neighbor_tiles'],\n",
    "        'coords': normalized_coords,\n",
    "        'label': np.zeros((len(test_data['M_tiles']), 35))\n",
    "    }\n",
    "my_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "\n",
    "image_keys = ['tile']\n",
    "\n",
    "\n",
    "\n",
    "#processed_data = preprocess_data(test_dataset, image_keys, my_transform)\n",
    "\n",
    "test_dataset = importDataset(\n",
    "        data_dict=test_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking dataset sample: 1000\n",
      "üìè tile shape: torch.Size([4, 112, 112]) | dtype: torch.float32 | min: 0.000, max: 1.000, mean: 0.779, std: 0.203\n",
      "üìè coords shape: torch.Size([2]) | dtype: torch.float32 | min: -0.393, max: 0.416, mean: 0.012, std: 0.572\n",
      "--- coords head (Ââç 10 ÂÄãÂÖÉÁ¥†):\n",
      "tensor([-0.3929,  0.4160])\n",
      "üìè label shape: torch.Size([35]) | dtype: torch.float32 | min: 0.000, max: 0.000, mean: 0.000, std: 0.000\n",
      "--- label head (Ââç 10 ÂÄãÂÖÉÁ¥†):\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "‚úÖ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "test_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82993376, -0.35686785,  0.8224873 , ..., -0.41686007,\n",
       "         0.14185855, -0.09103155],\n",
       "       [ 0.14049199, -0.4052957 ,  0.12465326, ..., -0.28855395,\n",
       "        -0.24106702, -0.4477588 ],\n",
       "       [ 0.14049199, -0.4052957 ,  0.12465326, ..., -0.28855395,\n",
       "        -0.24106702, -0.4477588 ],\n",
       "       ...,\n",
       "       [ 0.14049199, -0.4052957 ,  0.12465326, ..., -0.28855395,\n",
       "        -0.24106702, -0.4477588 ],\n",
       "       [-0.0380049 , -0.59372497, -0.06408934, ..., -0.32383734,\n",
       "        -0.3725784 , -0.52910906],\n",
       "       [-0.38289717, -0.2602251 , -0.4121772 , ...,  0.10326526,\n",
       "         0.01796564, -0.05405343]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = predict(model, test_loader, device)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Ê∏¨Ë©¶ revert_log2_predictions\n",
      "[[ 1.01396717e+00 -8.07002121e-03  1.80511435e+00  1.13151159e-01\n",
      "   2.00707840e+00  7.59623606e-02  6.96093327e-02  7.63244836e-03\n",
      "  -4.43450703e-02  4.99186394e-04  1.08874982e-02 -2.43959104e-02\n",
      "   4.05517143e-03  2.29893147e-02  6.26831059e-02  8.12215671e-02\n",
      "   2.64344670e-01  2.31829931e-02  1.36808254e-02 -1.33865438e-03\n",
      "  -2.55863969e-02  1.04127442e-02  1.75559047e-03 -3.42817691e-02\n",
      "   8.95906936e-02  2.71020152e-03  7.19174871e-02  1.14335168e-02\n",
      "  -2.35451263e-02  5.75979741e-03  1.10113500e-01  8.37440865e-02\n",
      "   1.95191196e-02  1.52221790e-02 -7.13266282e-02]\n",
      " [ 3.28478151e+00  3.26228103e-01 -1.51059413e-01  2.10342036e-01\n",
      "  -3.70963169e-01  9.46231454e-02  1.08288662e-02  4.10010213e-03\n",
      "   3.99134882e-01  1.60557299e-02 -6.11376033e-03  1.01965609e-03\n",
      "   7.99558069e-03  1.75983651e-02  1.73605903e-01  1.38104129e+00\n",
      "   1.07006572e-02 -5.08118852e-02  1.18068944e-03  6.97391458e-02\n",
      "  -2.20179089e-03  2.30087185e-02 -5.02672957e-03  1.61468807e-01\n",
      "   4.75605425e-02  9.87347349e-03 -3.15146508e-02 -6.39880534e-03\n",
      "   7.97088743e-03  2.97967609e-03  1.01989491e-01  5.37938927e-02\n",
      "   5.04197225e-03 -2.38961325e-03  7.05009362e-02]]\n",
      "üîÅ Ê∏¨Ë©¶ revert_boxcox_predictions\n",
      "‚ö†Ô∏è C1: 3 NaNs filled with median\n",
      "‚ö†Ô∏è C3: 1 NaNs filled with median\n",
      "‚ö†Ô∏è C5: 1 NaNs filled with median\n",
      "‚ö†Ô∏è C15: 1 NaNs filled with median\n",
      "‚ö†Ô∏è C18: 1 NaNs filled with median\n",
      "‚ö†Ô∏è C21: 1 NaNs filled with median\n",
      "[[5.69941215e-01 1.44401062e-02 2.16613543e-01 6.92434560e-02\n",
      "  1.52222846e+00 7.35393786e-02 5.17280355e-02 7.45352587e-03\n",
      "  1.00000241e-06 5.03471033e-04 1.07043069e-02 1.00001083e-06\n",
      "  3.68039741e-03 1.77321683e-02 5.42891212e-02 9.05469999e-02\n",
      "  2.29843179e-01 2.03432510e-02 1.03645722e-02 5.64149836e-03\n",
      "  1.00000957e-06 7.89466761e-03 1.77324432e-02 1.00000313e-06\n",
      "  1.04105599e-01 2.45926188e-03 5.81362032e-02 1.06301747e-02\n",
      "  1.00000794e-06 5.75564067e-03 8.02693318e-02 6.09578730e-02\n",
      "  1.91569223e-02 1.16400356e-02 1.00000711e-06]\n",
      " [8.43054539e-02 5.75388366e-01 1.00000164e-06 1.33292774e-01\n",
      "  1.00000117e-06 9.89988965e-02 1.17600325e-02 3.70104409e-03\n",
      "  3.94138868e-01 1.56028871e-02 1.00000730e-06 2.48993407e-03\n",
      "  8.29558963e-03 1.37160833e-02 2.09324024e-01 9.28974183e-01\n",
      "  1.47278636e-02 1.00000466e-06 2.89474341e-03 5.27115733e-02\n",
      "  1.00000957e-06 1.92639808e-02 1.44715060e-02 9.72692180e-02\n",
      "  4.52473814e-02 9.35846123e-03 1.00000310e-06 1.00001765e-06\n",
      "  6.97255594e-03 2.91593392e-03 7.42650631e-02 4.12930919e-02\n",
      "  4.26394940e-03 1.00001483e-06 5.04165762e-02]]\n"
     ]
    }
   ],
   "source": [
    "from python_scripts.revert_utils import (\n",
    "    load_json_params,\n",
    "    revert_log2_predictions,\n",
    "    revert_boxcox_predictions\n",
    ")\n",
    "\n",
    "# ÈÇÑÂéü Box-Cox\n",
    "params = load_json_params(\"./dataset/boxcox_zscore_params.json\")\n",
    "restored = revert_boxcox_predictions(test_preds, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1864083 , 0.0703001 , 0.7612741 , ..., 0.00406682, 0.01734674,\n",
       "        0.04675364],\n",
       "       [0.54058456, 0.06550384, 0.3668126 , ..., 0.00607252, 0.00961936,\n",
       "        0.02623594],\n",
       "       [0.54058456, 0.06550384, 0.3668126 , ..., 0.00607252, 0.00961936,\n",
       "        0.02623594],\n",
       "       ...,\n",
       "       [0.54058456, 0.06550384, 0.3668126 , ..., 0.00607252, 0.00961936,\n",
       "        0.02623594],\n",
       "       [0.4070865 , 0.04704511, 0.27621973, ..., 0.00552046, 0.00697911,\n",
       "        0.0216136 ],\n",
       "       [0.18102789, 0.07993639, 0.12459803, ..., 0.01222217, 0.01484013,\n",
       "        0.04890382]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from python_scripts.revert_utils import (\n",
    "    load_json_params,\n",
    "    revert_log2_predictions,\n",
    ")\n",
    "params = load_json_params(json_path=\"./dataset/zscore_params.json\")\n",
    "\n",
    "# ÊàñÈÇÑÂéü log2\n",
    "params = load_json_params(\"./dataset/zscore_params.json\")\n",
    "restored = revert_log2_predictions(test_preds, params, add_constant=1)\n",
    "\n",
    "restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved submission.csv in output_folder/with_all_preprocess_log2/GU+CNN4+MTD4/submission.csv\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== ËÆÄÂèñ test spot index Áî®ÊñºÂ∞çÊáâ ID ====\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "submission_path = os.path.join(save_folder, \"submission.csv\")\n",
    "\n",
    "ensemble_df = pd.DataFrame(restored, columns=[f\"C{i+1}\" for i in range(restored.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(submission_path, index=False)\n",
    "print(f\"‚úÖ Saved submission.csv in {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
