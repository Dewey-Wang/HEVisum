{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.shortcut is not None:\n",
    "            identity = self.shortcut(x)\n",
    "        out += identity\n",
    "        return self.relu2(out)\n",
    "\n",
    "\n",
    "class DeepTileEncoder(nn.Module):\n",
    "    \"\"\"加深的 Tile 分支：全局信息，多尺度池化 + 三层 MLP\"\"\"\n",
    "    def __init__(self, out_dim, in_channels=3, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.MaxPool2d(2)  # 78→39\n",
    "        )\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResidualBlock(32, 64, negative_slope=negative_slope),\n",
    "            ResidualBlock(64, 64, negative_slope=negative_slope),\n",
    "            nn.MaxPool2d(2)  # 39→19\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128, negative_slope=negative_slope),\n",
    "            ResidualBlock(128, 128, negative_slope=negative_slope),\n",
    "            nn.MaxPool2d(2)  # 19→9\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResidualBlock(128, 256, negative_slope=negative_slope),\n",
    "            ResidualBlock(256, 256, negative_slope=negative_slope)\n",
    "        )  # 保持 9×9\n",
    "\n",
    "        # 多尺度池化\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # [B,256,1,1]\n",
    "        self.mid_pool    = nn.AdaptiveAvgPool2d((3, 3))  # [B,256,3,3]\n",
    "\n",
    "        total_dim = 256*1*1 + 256*3*3\n",
    "        # 三层 MLP：total_dim → 2*out_dim → out_dim → out_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(total_dim, out_dim*4),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(out_dim*4, out_dim*2),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(out_dim*2, out_dim),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        # x: [B,256,9,9]\n",
    "        g = self.global_pool(x).contiguous().reshape(x.size(0), -1)  # [B,256]\n",
    "        m = self.mid_pool(x).contiguous().reshape(x.size(0), -1)     # [B,256*3*3]\n",
    "        return self.fc(torch.cat([g, m], dim=1))\n",
    "\n",
    "\n",
    "class SubtileEncoder(nn.Module):\n",
    "    \"\"\"多尺度 Subtile 分支：局部信息 + 两层 MLP\"\"\"\n",
    "    def __init__(self, out_dim, in_channels=3, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.MaxPool2d(2)  # 26→13\n",
    "        )\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResidualBlock(32, 64, negative_slope=negative_slope),\n",
    "            ResidualBlock(64, 64, negative_slope=negative_slope),\n",
    "            nn.MaxPool2d(2)  # 13→6\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128, negative_slope=negative_slope),\n",
    "            ResidualBlock(128, 128, negative_slope=negative_slope)\n",
    "        )  # 保持 6×6\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.mid_pool    = nn.AdaptiveAvgPool2d((2,2))\n",
    "        total_dim = 128*1*1 + 128*2*2\n",
    "        # 两层 MLP：total_dim → out_dim*2 → out_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(total_dim, out_dim*2),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(out_dim*2, out_dim),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.contiguous().reshape(B*N, C, H, W)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        # g,m: [B*N, feat]\n",
    "        g = self.global_pool(x).contiguous().reshape(B, N, -1)\n",
    "        m = self.mid_pool(x).contiguous().reshape(B, N, -1)\n",
    "        # 合并 N 张 subtiles，再 FC\n",
    "        feat = torch.cat([g, m], dim=2).mean(dim=1)  # [B, total_dim]\n",
    "        return self.fc(feat)\n",
    "\n",
    "\n",
    "class VisionMLP_MultiTask(nn.Module):\n",
    "    \"\"\"整体多任务模型：融合全局 Tile 与局部 Subtile 信息\"\"\"\n",
    "    def __init__(self, tile_dim=64, subtile_dim=64, output_dim=35):\n",
    "        super().__init__()\n",
    "        self.encoder_tile    = DeepTileEncoder(tile_dim)\n",
    "        self.encoder_subtile = SubtileEncoder(subtile_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(tile_dim + subtile_dim, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, tile, subtiles):\n",
    "        # tile: [B,3,78,78]\n",
    "        f_tile = self.encoder_tile(tile)\n",
    "        # subtiles: [B,9,3,26,26]\n",
    "        f_sub  = self.encoder_subtile(subtiles)\n",
    "        x = torch.cat([f_tile, f_sub], dim=1)\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "# 用法示例\n",
    "model = VisionMLP_MultiTask(tile_dim=64, subtile_dim=64, output_dim=35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from python_scripts.import_model import load_model_classes\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "\n",
    "# ==============================================\n",
    "# 範例使用\n",
    "# ==============================================\n",
    "folder = \"./output_folder/CNN+MLP/\"  # 替換成實際的資料夾路徑，該路徑下應有 model.py\n",
    "try:\n",
    "    loaded_classes = load_model_classes(folder)\n",
    "    print(\"載入的 class 名稱:\", list(loaded_classes.keys()))\n",
    "except Exception as e:\n",
    "    print(\"載入模型 class 發生錯誤:\", e)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假設 loaded_classes 是你已經從 model.py 載入的 class 字典\n",
    "name_of_class = 'VisionMLPModelWithCoord'\n",
    "ModelClass = loaded_classes.get(name_of_class)\n",
    "if ModelClass is None:\n",
    "    raise ValueError(f\"找不到 {name_of_class} 這個 class\")\n",
    "# 這裡呼叫 ModelClass() 建立實例，注意不能直接用 name_of_class() (因為它是一個字串)\n",
    "model = ModelClass()  # 如果需要參數，請在此處傳入\n",
    "get_model_inputs(model)\n",
    "#print(\"建立的模型實例:\", model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same in multiple .pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/import_data.py:299: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded keys: dict_keys(['source_idx', 'tile', 'label', 'subtiles'])\n",
      "Samples: 88759\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import inspect\n",
    "from python_scripts.import_data import load_all_tile_data\n",
    "\n",
    "# 用法範例\n",
    "folder = \"dataset/spot-rank/version-1/only_tile_sub/train\"\n",
    "grouped_data = load_all_tile_data(\n",
    "        folder_path=folder,\n",
    "        model=model,\n",
    "        fraction=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # grouped_data 現在只會有 model.forward() 需要的 key，\n",
    "    # 像 ['tile','subtiles','neighbors','norm_coord','node_feat','adj_list','edge_feat','label','source_idx']\n",
    "print(\"Loaded keys:\", grouped_data.keys())\n",
    "print(\"Samples:\", len(next(iter(grouped_data.values()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (tile, subtiles)\n"
     ]
    }
   ],
   "source": [
    "from python_scripts.import_data import importDataset\n",
    "\n",
    "image_keys = [ 'tile', 'subtiles']\n",
    "\n",
    "train_dataset = importDataset(\n",
    "        data_dict=grouped_data,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking dataset sample: 13\n",
      "📏 tile shape: torch.Size([3, 78, 78]) | dtype: torch.float32 | min: 0.000, max: 0.993, mean: 0.260, std: 0.334\n",
      "📏 subtiles shape: torch.Size([9, 3, 26, 26]) | dtype: torch.float32 | min: 0.000, max: 0.993, mean: 0.260, std: 0.334\n",
      "📏 label shape: torch.Size([35]) | dtype: torch.float32 | min: 1.000, max: 35.000, mean: 18.000, std: 10.247\n",
      "--- label head (前 10 個元素):\n",
      "tensor([28., 30., 26., 20., 35., 13., 24.,  6., 15., 33.])\n",
      "✅ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "train_dataset.check_item(13, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 79883 samples\n",
      "✅ Val: 8876 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 設定比例\n",
    "train_ratio = 0.9\n",
    "val_ratio = 1 - train_ratio\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# 拆分 Dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "print(f\"✅ Train: {len(train_set)} samples\")\n",
    "print(f\"✅ Val: {len(val_set)} samples\")\n",
    "\n",
    "# 🔹 將其包成 DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save in one pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from python_scripts.import_data import importDataset, preprocess_data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 載入資料\n",
    "#tile = torch.load(\"./dataset/final_data/M_tiles.pt\")\n",
    "#subtiles = torch.load(\"./train_dataset_sep_v2/subtiles.pt\")\n",
    "#neighbor_tiles = torch.load(\"./train_dataset_sep_v2/neighbor_tiles.pt\")\n",
    "#label = torch.load(\"./dataset/final_data/gu_log2_labels.pt\")\n",
    "#meta = torch.load(\"./dataset/final_data/meta_info.pt\")\n",
    "\n",
    "\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in grouped_data['coords']:\n",
    "    if _meta is not None:\n",
    "         x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "train_dataset = {\n",
    "        'tile': tile,\n",
    "        #'subtiles': subtiles,\n",
    "        #'neighbor_tiles': neighbor_tiles,\n",
    "        'coords': normalized_coords,\n",
    "        'label': label\n",
    "    }\n",
    "\n",
    "my_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "\n",
    "image_keys = ['tile']\n",
    "\n",
    "#processed_data = preprocess_data(train_dataset, image_keys, my_transform)\n",
    "\n",
    "train_dataset = importDataset(\n",
    "        data_dict=train_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = importDataset(\n",
    "        data_dict=processed_data,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 設定比例\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# 拆分 Dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "print(f\"✅ Train: {len(train_set)} samples\")\n",
    "print(f\"✅ Val: {len(val_set)} samples\")\n",
    "\n",
    "# 🔹 將其包成 DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from python_scripts.operate_model import train_one_epoch, evaluate, predict, EarlyStopping, plot_losses, plot_per_cell_metrics\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------------------\n",
    "# 指定儲存資料夾\n",
    "# ---------------------------\n",
    "save_folder = \"/Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/CNN+Res+MLP_version-1/softrank/no_coords_neighbor/masked/\"  # 修改為你想要的資料夾名稱\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 364/2497 [01:12<05:55,  6.00it/s, avg=1.32e+3, loss=826]    "
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 🔧 設定裝置\n",
    "# ---------------------------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 初始化模型 & 優化器\n",
    "# ---------------------------\n",
    "model = model.to(device)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "#loss_fn = lambda pred, tgt: hybrid_weighted_spearman_loss(pred, tgt, alpha=0.5, beta=1.0)\n",
    "\n",
    "#loss_fn = nn.MSELoss()\n",
    "# loss_fn = spearman_loss\n",
    "#LOSS_TYPE = \"weighted_mse\"  # 或 \"mse\", \"mae\", \"spearman\"\n",
    "\n",
    "import torch\n",
    "\n",
    "HARD_IDX = [7, 12, 19, 31, 32]\n",
    "WEIGHT_VEC = torch.ones(35)\n",
    "WEIGHT_VEC[HARD_IDX] = 5.0\n",
    "WEIGHT_VEC = WEIGHT_VEC.to(device)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# ---------------------------\n",
    "# 儲存 log 的設定\n",
    "# ---------------------------\n",
    "log_path = os.path.join(save_folder, \"training_log.csv\")\n",
    "log_file = open(log_path, mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# ---------------------------\n",
    "# 用來畫圖的變數\n",
    "# ---------------------------\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_spearmanes = []\n",
    "val_spearmanes = []\n",
    "\n",
    "# ---------------------------\n",
    "# 指定最佳模型儲存路徑\n",
    "# ---------------------------\n",
    "best_model_path = os.path.join(save_folder, \"best_model.pt\")\n",
    "loss_plot_path = os.path.join(save_folder, \"loss_curve.png\")\n",
    "\n",
    "# ---------------------------\n",
    "# 開始訓練\n",
    "# ---------------------------\n",
    "num_epochs = 150\n",
    "best_val_loss = 0\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "display(fig)  # 初始顯示圖形\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_spearman = train_one_epoch(model, train_loader, optimizer, device,current_epoch= epoch,initial_alpha=0, final_alpha=0,target_epoch = 10 )\n",
    "    val_loss, val_spearman, mse_per_cell, spearman_per_cell = evaluate(model, val_loader, device, current_epoch= epoch,initial_alpha=0, final_alpha=0, target_epoch =10)\n",
    "    clear_output(wait=True)  # 清除之前的輸出\n",
    "    axes[0][0].clear()\n",
    "    axes[0][1].clear()\n",
    "    axes[1][0].clear()\n",
    "    axes[1][1].clear()\n",
    "    # 儲存最佳模型\n",
    "    if best_val_loss < val_spearman:\n",
    "        best_val_loss = val_spearman\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\"✅ Saved best model!\")\n",
    "        \n",
    "\n",
    "    # 調整學習率\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 寫入 CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch + 1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # 印出 Epoch 結果\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | lr: {lr:.2e}\")\n",
    "    print(f\"train spearman: {train_spearman:.4f} | Val spearman: {val_spearman:.4f} | ρ: {val_spearman:.4f}\")\n",
    "\n",
    "    # 更新 log 列表並畫圖\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_spearmanes.append(train_spearman)\n",
    "    val_spearmanes.append(val_spearman)\n",
    "    \n",
    "\n",
    "\n",
    "    # plot loss\n",
    "    plot_losses(train_losses, val_losses, ax=axes[0][0], title=\"MSE Loss\")\n",
    "    plot_losses(train_spearmanes, val_spearmanes, ax=axes[0][1], title=\"Spearman Loss\")\n",
    "\n",
    "    cell_names = [f\"C{i+1}\" for i in range(35)]\n",
    "    # plot per-cell stats\n",
    "    plot_per_cell_metrics(mse_per_cell, spearman_per_cell,cell_names,\n",
    "                        ax_mse=axes[1][0], ax_spearman=axes[1][1])\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    display(fig)\n",
    "    plt.pause(0.1)  # 暫停以便更新畫面\n",
    "    fig.savefig(loss_plot_path)\n",
    "    print(f\"曲線圖已儲存至 {loss_plot_path}\")\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"⛔ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# 關閉 CSV log 檔案\n",
    "log_file.close()\n",
    "\n",
    "# 儲存最終圖形\n",
    "plt.close(fig)\n",
    "print(f\"訓練結束，loss 曲線圖已儲存至 {loss_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cell C1 / 35 ===\n",
      "  tile        : 0.0012 ± 0.0000\n",
      "  subtiles    : 0.0056 ± 0.0001\n",
      "\n",
      "=== Cell C2 / 35 ===\n",
      "  tile        : 0.0011 ± 0.0000\n",
      "  subtiles    : 0.0033 ± 0.0000\n",
      "\n",
      "=== Cell C3 / 35 ===\n",
      "  tile        : 0.0016 ± 0.0000\n",
      "  subtiles    : 0.0056 ± 0.0001\n",
      "\n",
      "=== Cell C4 / 35 ===\n",
      "  tile        : 0.0021 ± 0.0000\n",
      "  subtiles    : 0.0058 ± 0.0001\n",
      "\n",
      "=== Cell C5 / 35 ===\n",
      "  tile        : 0.0041 ± 0.0001\n",
      "  subtiles    : 0.0114 ± 0.0001\n",
      "\n",
      "=== Cell C6 / 35 ===\n",
      "  tile        : 0.0055 ± 0.0001\n",
      "  subtiles    : 0.0115 ± 0.0004\n",
      "\n",
      "=== Cell C7 / 35 ===\n",
      "  tile        : 0.0013 ± 0.0000\n",
      "  subtiles    : 0.0025 ± 0.0001\n",
      "\n",
      "=== Cell C8 / 35 ===\n",
      "  tile        : 0.0014 ± 0.0000\n",
      "  subtiles    : 0.0043 ± 0.0000\n",
      "\n",
      "=== Cell C9 / 35 ===\n",
      "  tile        : 0.0064 ± 0.0000\n",
      "  subtiles    : 0.0124 ± 0.0006\n",
      "\n",
      "=== Cell C10 / 35 ===\n",
      "  tile        : 0.0038 ± 0.0000\n",
      "  subtiles    : 0.0102 ± 0.0003\n",
      "\n",
      "=== Cell C11 / 35 ===\n",
      "  tile        : 0.0042 ± 0.0001\n",
      "  subtiles    : 0.0051 ± 0.0001\n",
      "\n",
      "=== Cell C12 / 35 ===\n",
      "  tile        : 0.0059 ± 0.0000\n",
      "  subtiles    : 0.0055 ± 0.0001\n",
      "\n",
      "=== Cell C13 / 35 ===\n",
      "  tile        : 0.0015 ± 0.0000\n",
      "  subtiles    : 0.0048 ± 0.0001\n",
      "\n",
      "=== Cell C14 / 35 ===\n",
      "  tile        : 0.0020 ± 0.0000\n",
      "  subtiles    : 0.0043 ± 0.0001\n",
      "\n",
      "=== Cell C15 / 35 ===\n",
      "  tile        : 0.0068 ± 0.0001\n",
      "  subtiles    : 0.0112 ± 0.0003\n",
      "\n",
      "=== Cell C16 / 35 ===\n",
      "  tile        : 0.0060 ± 0.0000\n",
      "  subtiles    : 0.0094 ± 0.0002\n",
      "\n",
      "=== Cell C17 / 35 ===\n",
      "  tile        : 0.0025 ± 0.0000\n",
      "  subtiles    : 0.0076 ± 0.0001\n",
      "\n",
      "=== Cell C18 / 35 ===\n",
      "  tile        : 0.0014 ± 0.0000\n",
      "  subtiles    : 0.0068 ± 0.0001\n",
      "\n",
      "=== Cell C19 / 35 ===\n",
      "  tile        : 0.0011 ± 0.0000\n",
      "  subtiles    : 0.0090 ± 0.0002\n",
      "\n",
      "=== Cell C20 / 35 ===\n",
      "  tile        : 0.0032 ± 0.0000\n",
      "  subtiles    : 0.0074 ± 0.0001\n",
      "\n",
      "=== Cell C21 / 35 ===\n",
      "  tile        : 0.0012 ± 0.0000\n",
      "  subtiles    : 0.0046 ± 0.0001\n",
      "\n",
      "=== Cell C22 / 35 ===\n",
      "  tile        : 0.0025 ± 0.0000\n",
      "  subtiles    : 0.0093 ± 0.0001\n",
      "\n",
      "=== Cell C23 / 35 ===\n",
      "  tile        : 0.0007 ± 0.0000\n",
      "  subtiles    : 0.0039 ± 0.0000\n",
      "\n",
      "=== Cell C24 / 35 ===\n",
      "  tile        : 0.0025 ± 0.0000\n",
      "  subtiles    : 0.0054 ± 0.0001\n",
      "\n",
      "=== Cell C25 / 35 ===\n",
      "  tile        : 0.0050 ± 0.0001\n",
      "  subtiles    : 0.0057 ± 0.0001\n",
      "\n",
      "=== Cell C26 / 35 ===\n",
      "  tile        : 0.0025 ± 0.0000\n",
      "  subtiles    : 0.0036 ± 0.0002\n",
      "\n",
      "=== Cell C27 / 35 ===\n",
      "  tile        : 0.0077 ± 0.0000\n",
      "  subtiles    : 0.0119 ± 0.0004\n",
      "\n",
      "=== Cell C28 / 35 ===\n",
      "  tile        : 0.0024 ± 0.0000\n",
      "  subtiles    : 0.0035 ± 0.0001\n",
      "\n",
      "=== Cell C29 / 35 ===\n",
      "  tile        : 0.0026 ± 0.0000\n",
      "  subtiles    : 0.0102 ± 0.0001\n",
      "\n",
      "=== Cell C30 / 35 ===\n",
      "  tile        : 0.0060 ± 0.0001\n",
      "  subtiles    : 0.0050 ± 0.0001\n",
      "\n",
      "=== Cell C31 / 35 ===\n",
      "  tile        : 0.0009 ± 0.0000\n",
      "  subtiles    : 0.0043 ± 0.0000\n",
      "\n",
      "=== Cell C32 / 35 ===\n",
      "  tile        : 0.0018 ± 0.0000\n",
      "  subtiles    : 0.0042 ± 0.0001\n",
      "\n",
      "=== Cell C33 / 35 ===\n",
      "  tile        : 0.0010 ± 0.0000\n",
      "  subtiles    : 0.0078 ± 0.0002\n",
      "\n",
      "=== Cell C34 / 35 ===\n",
      "  tile        : 0.0019 ± 0.0000\n",
      "  subtiles    : 0.0045 ± 0.0001\n",
      "\n",
      "=== Cell C35 / 35 ===\n",
      "  tile        : 0.0023 ± 0.0000\n",
      "  subtiles    : 0.0039 ± 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tile_mean</th>\n",
       "      <th>tile_std</th>\n",
       "      <th>subtiles_mean</th>\n",
       "      <th>subtiles_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.004125</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.007657</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.012378</td>\n",
       "      <td>0.000583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tile_mean   tile_std  subtiles_mean  subtiles_std\n",
       "count  35.000000  35.000000      35.000000     35.000000\n",
       "mean    0.002968   0.000034       0.006608      0.000137\n",
       "std     0.001983   0.000022       0.002901      0.000123\n",
       "min     0.000693   0.000008       0.002473      0.000024\n",
       "25%     0.001406   0.000021       0.004307      0.000072\n",
       "50%     0.002420   0.000028       0.005564      0.000087\n",
       "75%     0.004125   0.000041       0.009136      0.000138\n",
       "max     0.007657   0.000095       0.012378      0.000583"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from captum.attr import IntegratedGradients\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "\n",
    "def automate_ig_all_targets(model,\n",
    "                            batch,\n",
    "                            n_cells: int = 35,\n",
    "                            n_steps: int = 50,\n",
    "                            n_runs: int = 20,\n",
    "                            baseline_stdevs: Dict[str, float] = None):\n",
    "    \"\"\"\n",
    "    对每个输出 t∈[0,n_cells):\n",
    "      - 在 n_runs 个随机 baseline 上跑 IG\n",
    "      - 对图像输入噪声范围 [0,1]*stdev_img；对坐标噪声范围 [-1,1]*stdev_coord\n",
    "      - 计算 mean|IG| & std|IG|，并打印\n",
    "    最终返回一个 DataFrame，行是 cell，列是每个输入的 mean±std。\n",
    "    \"\"\"\n",
    "    cpu = torch.device(\"cpu\")\n",
    "    model_cpu = model.to(cpu).float().eval()\n",
    "    sig = get_model_inputs(model_cpu, print_sig=False)\n",
    "    param_names = [p for p in sig.parameters if p != \"self\"]\n",
    "    # 只保留 tensor\n",
    "    tensor_names = [n for n in param_names if isinstance(batch[n], torch.Tensor)]\n",
    "\n",
    "    # 默认各自 stdev\n",
    "    baseline_stdevs = baseline_stdevs or {}\n",
    "    # e.g. {\"tile\": 0.1, \"subtiles\": 0.1, \"neighbors\": 0.1, \"norm_coord\": 0.2}\n",
    "\n",
    "    # 预先把 inputs 拷贝到 CPU, 取第 1 样本\n",
    "    inputs_cpu = []\n",
    "    for n in tensor_names:\n",
    "        x = batch[n].detach().cpu().float()[0:1].requires_grad_(True)\n",
    "        inputs_cpu.append(x)\n",
    "\n",
    "    ig = IntegratedGradients(model_cpu)\n",
    "    results = {t: {n: [] for n in tensor_names} for t in range(n_cells)}\n",
    "\n",
    "    for t in range(n_cells):\n",
    "        print(f\"\\n=== Cell C{t+1} / {n_cells} ===\")\n",
    "        for run in range(n_runs):\n",
    "            baselines = []\n",
    "            for name, inp in zip(tensor_names, inputs_cpu):\n",
    "                stdev = baseline_stdevs.get(name, 0.1)\n",
    "                if name == \"norm_coord\":\n",
    "                    # 坐标范围 [-1,1]\n",
    "                    noise = (torch.rand_like(inp) * 2 - 1) * stdev\n",
    "                else:\n",
    "                    # 图像范围 [0,1]\n",
    "                    noise = torch.rand_like(inp) * stdev\n",
    "                baselines.append(noise)\n",
    "            attributions = ig.attribute(\n",
    "                inputs=tuple(inputs_cpu),\n",
    "                baselines=tuple(baselines),\n",
    "                target=t,\n",
    "                n_steps=n_steps,\n",
    "                internal_batch_size=1\n",
    "            )\n",
    "            for name, attr in zip(tensor_names, attributions):\n",
    "                results[t][name].append(attr.abs().mean().item())\n",
    "\n",
    "        # 打印这一 cell 的 mean±std\n",
    "        for name in tensor_names:\n",
    "            arr = np.array(results[t][name], dtype=np.float32)\n",
    "            print(f\"  {name:12s}: {arr.mean():.4f} ± {arr.std():.4f}\")\n",
    "\n",
    "    # 汇总到 DataFrame\n",
    "    rows = []\n",
    "    for t in range(n_cells):\n",
    "        row = {\"cell\": f\"C{t+1}\"}\n",
    "        for name in tensor_names:\n",
    "            arr = np.array(results[t][name], dtype=np.float32)\n",
    "            row[f\"{name}_mean\"] = arr.mean()\n",
    "            row[f\"{name}_std\"]  = arr.std()\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows).set_index(\"cell\")\n",
    "    return df\n",
    "baseline_stdevs = {\n",
    "    \"tile\":        0.15,\n",
    "    \"subtiles\":    0.15,\n",
    "    \"neighbors\":   0.22,\n",
    "    \"norm_coord\":  0.37,\n",
    "}\n",
    "batch = next(iter(val_loader))\n",
    "\n",
    "df_scores = automate_ig_all_targets(\n",
    "    model, batch,\n",
    "    n_cells=35, n_steps=50, n_runs=5,\n",
    "    baseline_stdevs=baseline_stdevs)\n",
    "display(df_scores.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n",
      "✅ Loading model from /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/CNN+Res+MLP_version-1/softrank/no_coords_neighbor/best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_43026/4199911870.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionMLP_MultiTask(\n",
       "  (encoder_tile): DeepTileEncoder(\n",
       "    (layer0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (mid_pool): AdaptiveAvgPool2d(output_size=(3, 3))\n",
       "    (fc): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): Linear(in_features=2560, out_features=256, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Dropout(p=0.1, inplace=False)\n",
       "      (5): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (6): LeakyReLU(negative_slope=0.01)\n",
       "      (7): Dropout(p=0.1, inplace=False)\n",
       "      (8): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (9): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (encoder_subtile): SubtileEncoder(\n",
       "    (layer0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (mid_pool): AdaptiveAvgPool2d(output_size=(2, 2))\n",
       "    (fc): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): Linear(in_features=640, out_features=128, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Dropout(p=0.1, inplace=False)\n",
       "      (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (6): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=35, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 需要的 Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "import os\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "best_model_path = os.path.join(save_folder, \"best_model.pt\")\n",
    "model = model.to(device)\n",
    "print(f\"✅ Loading model from {best_model_path}\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_43026/2614805245.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  raw = torch.load(pt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 从 'tile' 推断样本数量: 2088\n",
      "Model forward signature: (tile, subtiles)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "\n",
    "def load_node_feature_data(pt_path: str, model, num_cells: int = 35) -> dict:\n",
    "    \"\"\"\n",
    "    根据 model.forward 的参数自动加载 .pt 里对应的字段，\n",
    "    并且如果没有 label，就自动创建一个全 0 的 label 张量，\n",
    "    其尺寸为 (样本数, num_cells)，样本数从第一个有 __len__ 的输入推断。\n",
    "\n",
    "    参数：\n",
    "      pt_path:     str，.pt 文件路径\n",
    "      model:       已实例化的 PyTorch 模型\n",
    "      num_cells:   int，label 的列数（默认 35）\n",
    "\n",
    "    返回：\n",
    "      dict: key 对应模型 forward 中的参数名（不含 self），\n",
    "            value 是对应的 Tensor/ndarray，\n",
    "            并额外保证有 'label' 字段。\n",
    "    \"\"\"\n",
    "    # 1) 载入原始数据\n",
    "    raw = torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "    # 2) 取模型 forward 入参签名（不含 self）\n",
    "    sig = inspect.signature(model.forward)\n",
    "    param_names = [p for p in sig.parameters if p != \"self\"]\n",
    "\n",
    "    out = {}\n",
    "    for name in param_names:\n",
    "        # a) 直接同名\n",
    "        if name in raw:\n",
    "            out[name] = raw[name]\n",
    "            continue\n",
    "        # b) 复数形式\n",
    "        if name + \"s\" in raw:\n",
    "            out[name] = raw[name + \"s\"]\n",
    "            continue\n",
    "        # c) 模糊匹配（下划线、复数或前后缀）\n",
    "        cands = [k for k in raw if name in k or k in name]\n",
    "        if len(cands) == 1:\n",
    "            out[name] = raw[cands[0]]\n",
    "            continue\n",
    "        raise KeyError(f\"无法找到 '{name}' 在 pt 文件中的对应字段，raw keys: {list(raw.keys())}\")\n",
    "\n",
    "    # 3) 用第一个支持 len() 的输入推断样本数\n",
    "    dataset_size = None\n",
    "    for v in out.keys():\n",
    "        if hasattr(out[v], \"__len__\"):\n",
    "            dataset_size = len(out[v])\n",
    "            print(f\"⚠️ 从 '{v}' 推断样本数量: {dataset_size}\")\n",
    "            break\n",
    "    if dataset_size is None:\n",
    "        raise RuntimeError(\"无法从任何输入中推断样本数量，请检查 pt 文件内容。\")\n",
    "\n",
    "    # 4) 自动补 label\n",
    "\n",
    "    out[\"label\"] = torch.zeros((dataset_size, num_cells), dtype=torch.float32)\n",
    "    return out\n",
    "\n",
    "\n",
    "image_keys = [ 'tile', 'subtiles']\n",
    "\n",
    "\n",
    "# 用法示例\n",
    "from python_scripts.import_data import importDataset\n",
    "# 假设你的 model 已经定义好并实例化为 `model`\n",
    "test_dataset = load_node_feature_data(\"dataset/spot-rank/version-1/node_features_test/test_dataset.pt\", model)\n",
    "test_dataset = importDataset(\n",
    "        data_dict=test_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_77211/2443631239.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(pt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各欄位長度檢查：\n",
      "  tile           : type=list      , length=2088\n",
      "  subtiles       : type=list      , length=2088\n",
      "  neighbors      : type=list      , length=2088\n",
      "  label          : type=ndarray   , length=2088\n",
      "  norm_coord     : type=list      , length=2088\n",
      "  node_feat      : type=list      , length=2088\n",
      "  adj_list       : type=list      , length=2088\n",
      "  edge_feat      : type=list      , length=2088\n",
      "Model forward signature: (tile, subtiles, neighbors, norm_coord)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import inspect\n",
    "from python_scripts.import_data import importDataset, preprocess_data\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# 正確方式\n",
    "import torch\n",
    "\n",
    "def load_node_feature_data(pt_path):\n",
    "    \"\"\"\n",
    "    Load a single .pt file containing node-feature data saved as a dict.\n",
    "\n",
    "    Parameters:\n",
    "        pt_path (str): Path to the .pt file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys:\n",
    "            - 'tiles'\n",
    "            - 'subtiles'\n",
    "            - 'neighbor_tiles'\n",
    "            - 'labels'\n",
    "            - 'coords'\n",
    "            - 'normal_coords'\n",
    "            - 'node_feats'\n",
    "            - 'adj_lists'\n",
    "            - 'edge_feats'\n",
    "    \"\"\"\n",
    "    # Load the saved checkpoint into CPU memory\n",
    "    data = torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "    # 依照你當初儲存時的 key 取值\n",
    "    return {\n",
    "        'tile':           data.get('tiles'),\n",
    "        'subtiles':        data.get('subtiles'),\n",
    "        'neighbors':  data.get('neighbor_tiles'),\n",
    "        'label':          np.zeros((len(data.get('tiles')), 35)),\n",
    "        'norm_coord':   data.get('normal_coords'),\n",
    "        'node_feat':      data.get('node_feat'),\n",
    "        'adj_list':       data.get('adj_list'),\n",
    "        'edge_feat':      data.get('edge_feat'),\n",
    "    }\n",
    "\n",
    "# 範例用法\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"./dataset/spot-rank/version-1/node_features_test/test_dataset.pt\"\n",
    "    test_dataset = load_node_feature_data(path)\n",
    "    print(\"各欄位長度檢查：\")\n",
    "    for k, v in test_dataset.items():\n",
    "        L = len(v) if hasattr(v, \"__len__\") else \"N/A\"\n",
    "        print(f\"  {k:15s}: type={type(v).__name__:10s}, length={L}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_keys = ['tile','subtiles', 'neighbors']\n",
    "\n",
    "\n",
    "\n",
    "#processed_data = preprocess_data(test_dataset, image_keys, my_transform)\n",
    "\n",
    "test_dataset = importDataset(\n",
    "        data_dict=test_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking dataset sample: 1000\n",
      "📏 tile shape: torch.Size([3, 78, 78]) | dtype: torch.float32 | min: 0.000, max: 1.000, mean: 0.636, std: 0.251\n",
      "📏 subtiles shape: torch.Size([9, 3, 26, 26]) | dtype: torch.float32 | min: 0.000, max: 1.000, mean: 0.636, std: 0.251\n",
      "📏 label shape: torch.Size([35]) | dtype: torch.float32 | min: 0.000, max: 0.000, mean: 0.000, std: 0.000\n",
      "--- label head (前 10 個元素):\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "✅ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23.246147, 34.771843, 26.373205, ..., 21.444412, 16.545738,\n",
       "        25.812304],\n",
       "       [19.496866, 30.091673, 22.121897, ..., 17.232475, 11.77669 ,\n",
       "        20.429758],\n",
       "       [18.353577, 31.180635, 21.12514 , ..., 18.722095, 12.266075,\n",
       "        21.144463],\n",
       "       ...,\n",
       "       [19.296957, 32.581196, 21.971876, ..., 19.422009, 13.699101,\n",
       "        21.966719],\n",
       "       [27.498642, 31.75253 , 26.321768, ..., 22.10948 , 20.20831 ,\n",
       "        18.199139],\n",
       "       [21.484125, 27.5657  , 23.111822, ..., 16.676172, 11.198922,\n",
       "        20.000557]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = predict(model, test_loader, device)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved submission.csv in /Users/deweywang/Desktop/GitHub/HEVisum/output_folder/rank-spot/CNN+Res+MLP_version-1/softrank/no_coords_neighbor/submission.csv\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== 讀取 test spot index 用於對應 ID ====\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "submission_path = os.path.join(save_folder, \"submission.csv\")\n",
    "\n",
    "ensemble_df = pd.DataFrame(test_preds, columns=[f\"C{i+1}\" for i in range(test_preds.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(submission_path, index=False)\n",
    "print(f\"✅ Saved submission.csv in {submission_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_scripts.revert_utils import (\n",
    "    load_json_params,\n",
    "    revert_log2_predictions,\n",
    ")\n",
    "\n",
    "#params = load_json_params(json_path=\"./dataset/spots-data/version-1/zscore_params.json\")\n",
    "#params = load_json_params('data preprocessing/spot data cleaning/zscore_params.json')\n",
    "\n",
    "# 或還原 log2\n",
    "params = load_json_params(\"dataset/spots-data/version-5/zscore_params.json\")\n",
    "restored = revert_log2_predictions(test_preds, params, add_constant=1)\n",
    "\n",
    "restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== 讀取 test spot index 用於對應 ID ====\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "submission_path = os.path.join(save_folder, \"submission.csv\")\n",
    "\n",
    "ensemble_df = pd.DataFrame(restored, columns=[f\"C{i+1}\" for i in range(restored.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(submission_path, index=False)\n",
    "print(f\"✅ Saved submission.csv in {submission_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
