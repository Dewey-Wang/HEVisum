{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ËºâÂÖ•ÁöÑ class ÂêçÁ®±: ['CNNEncoder', 'MLPDecoder', 'VisionMLPModelWithCoord']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from python_scripts.import_model import load_model_classes\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "\n",
    "# ==============================================\n",
    "# ÁØÑ‰æã‰ΩøÁî®\n",
    "# ==============================================\n",
    "folder = \"./output_folder/CNN+MLP/\"  # ÊõøÊèõÊàêÂØ¶ÈöõÁöÑË≥áÊñôÂ§æË∑ØÂæëÔºåË©≤Ë∑ØÂæë‰∏ãÊáâÊúâ model.py\n",
    "try:\n",
    "    loaded_classes = load_model_classes(folder)\n",
    "    print(\"ËºâÂÖ•ÁöÑ class ÂêçÁ®±:\", list(loaded_classes.keys()))\n",
    "except Exception as e:\n",
    "    print(\"ËºâÂÖ•Ê®°Âûã class ÁôºÁîüÈåØË™§:\", e)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Signature (center_tile, subtiles, neighbor_tiles, coords)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÂÅáË®≠ loaded_classes ÊòØ‰Ω†Â∑≤Á∂ìÂæû model.py ËºâÂÖ•ÁöÑ class Â≠óÂÖ∏\n",
    "name_of_class = 'VisionMLPModelWithCoord'\n",
    "ModelClass = loaded_classes.get(name_of_class)\n",
    "if ModelClass is None:\n",
    "    raise ValueError(f\"Êâæ‰∏çÂà∞ {name_of_class} ÈÄôÂÄã class\")\n",
    "# ÈÄôË£°ÂëºÂè´ ModelClass() Âª∫Á´ãÂØ¶‰æãÔºåÊ≥®ÊÑè‰∏çËÉΩÁõ¥Êé•Áî® name_of_class() (Âõ†ÁÇ∫ÂÆÉÊòØ‰∏ÄÂÄãÂ≠ó‰∏≤)\n",
    "model = ModelClass()  # Â¶ÇÊûúÈúÄË¶ÅÂèÉÊï∏ÔºåË´ãÂú®Ê≠§ËôïÂÇ≥ÂÖ•\n",
    "get_model_inputs(model)\n",
    "#print(\"Âª∫Á´ãÁöÑÊ®°ÂûãÂØ¶‰æã:\", model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_95032/1475077652.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  center_tile = torch.load(\"./train_dataset_sep_v2/tiles.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_95032/1475077652.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  subtiles = torch.load(\"./train_dataset_sep_v2/subtiles.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_95032/1475077652.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  neighbor_tiles = torch.load(\"./train_dataset_sep_v2/neighbor_tiles.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_95032/1475077652.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  label = torch.load(\"./train_dataset_sep_v2/labels.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_95032/1475077652.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta = torch.load(\"./train_dataset_sep_v2/meta_info.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method importDataset.check_item of <import_data.importDataset object at 0x103af4eb0>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from python_scripts.import_data import importDataset, preprocess_data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# ËºâÂÖ•Ë≥áÊñô\n",
    "center_tile = torch.load(\"./train_dataset_sep_v2/tiles.pt\")\n",
    "subtiles = torch.load(\"./train_dataset_sep_v2/subtiles.pt\")\n",
    "neighbor_tiles = torch.load(\"./train_dataset_sep_v2/neighbor_tiles.pt\")\n",
    "label = torch.load(\"./train_dataset_sep_v2/labels.pt\")\n",
    "meta = torch.load(\"./train_dataset_sep_v2/meta_info.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in meta:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "train_dataset = {\n",
    "        'center_tile': center_tile,\n",
    "        'subtiles': subtiles,\n",
    "        'neighbor_tiles': neighbor_tiles,\n",
    "        'coords': normalized_coords,\n",
    "        'label': label\n",
    "    }\n",
    "\n",
    "my_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "\n",
    "image_keys = ['center_tile', 'subtiles', 'neighbor_tiles']\n",
    "\n",
    "processed_data = preprocess_data(train_dataset, image_keys, my_transform)\n",
    "\n",
    "train_dataset = importDataset(\n",
    "        data_dict=processed_data,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = importDataset(\n",
    "        data_dict=processed_data,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking dataset sample: 1000\n",
      "üìè center_tile shape: torch.Size([3, 78, 78]) | dtype: torch.float32 | min: -0.208, max: 1.000, mean: 0.488, std: 0.232\n",
      "üìè subtiles shape: torch.Size([9, 3, 26, 26]) | dtype: torch.float32 | min: -0.208, max: 1.000, mean: 0.488, std: 0.232\n",
      "üìè neighbor_tiles shape: torch.Size([8, 3, 78, 78]) | dtype: torch.float32 | min: -0.631, max: 1.000, mean: 0.497, std: 0.246\n",
      "üìè coords shape: torch.Size([2]) | dtype: torch.float32 | min: -1.243, max: 1.148, mean: -0.047, std: 1.691\n",
      "--- coords head (Ââç 10 ÂÄãÂÖÉÁ¥†):\n",
      "tensor([ 1.1484, -1.2428])\n",
      "üìè label shape: torch.Size([35]) | dtype: torch.float32 | min: -0.678, max: 3.737, mean: -0.151, std: 0.945\n",
      "--- label head (Ââç 10 ÂÄãÂÖÉÁ¥†):\n",
      "tensor([-0.5575, -0.4912, -0.5658, -0.4039,  0.8988, -0.3776, -0.4374, -0.4394,\n",
      "        -0.4366, -0.3719])\n",
      "‚úÖ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "train_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train: 6679 samples\n",
      "‚úÖ Val: 1670 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Ë®≠ÂÆöÊØî‰æã\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# ÊãÜÂàÜ Dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_set)} samples\")\n",
    "print(f\"‚úÖ Val: {len(val_set)} samples\")\n",
    "\n",
    "# üîπ Â∞áÂÖ∂ÂåÖÊàê DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from python_scripts.operate_model import get_model_inputs, train_one_epoch, evaluate, predict, EarlyStopping, plot_losses\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------------------\n",
    "# ÊåáÂÆöÂÑ≤Â≠òË≥áÊñôÂ§æ\n",
    "# ---------------------------\n",
    "save_folder = \"output_folder/try\"  # ‰øÆÊîπÁÇ∫‰Ω†ÊÉ≥Ë¶ÅÁöÑË≥áÊñôÂ§æÂêçÁ®±\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m display(fig)  \u001b[38;5;66;03m# ÂàùÂßãÈ°ØÁ§∫ÂúñÂΩ¢\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 49\u001b[0m     train_loss, train_spearman \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     val_loss, val_spearman \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, loss_fn, device)\n\u001b[1;32m     52\u001b[0m     axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/Desktop/GitHub/HEVisum/operate_model.py:54\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, loss_fn, device, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m all_preds, all_targets \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     52\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     55\u001b[0m     inputs, label \u001b[38;5;241m=\u001b[39m make_input_to_device(model, batch, device)\n\u001b[1;32m     56\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[63], line 91\u001b[0m, in \u001b[0;36mimportDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_keys:\n\u001b[0;32m---> 91\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m         value \u001b[38;5;241m=\u001b[39m convert_item(value, is_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[63], line 26\u001b[0m, in \u001b[0;36mconvert_item\u001b[0;34m(item, is_image)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     converted_list \u001b[38;5;241m=\u001b[39m [convert_item(elem, is_image\u001b[38;5;241m=\u001b[39mis_image) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m item]\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mËΩâÊèõÂàóË°®‰∏≠ÁöÑÂÖÉÁ¥†Â§±ÊïóÔºåÂàóË°®ÂÖßÂÆπ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# üîß Ë®≠ÂÆöË£ùÁΩÆ\n",
    "# ---------------------------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "# ---------------------------\n",
    "# ÂàùÂßãÂåñÊ®°Âûã & ÂÑ™ÂåñÂô®\n",
    "# ---------------------------\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "# loss_fn = spearman_loss\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# ---------------------------\n",
    "# ÂÑ≤Â≠ò log ÁöÑË®≠ÂÆö\n",
    "# ---------------------------\n",
    "log_path = os.path.join(save_folder, \"training_log.csv\")\n",
    "log_file = open(log_path, mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# ---------------------------\n",
    "# Áî®‰æÜÁï´ÂúñÁöÑËÆäÊï∏\n",
    "# ---------------------------\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_spearmanes = []\n",
    "val_spearmanes = []\n",
    "\n",
    "# ---------------------------\n",
    "# ÊåáÂÆöÊúÄ‰Ω≥Ê®°ÂûãÂÑ≤Â≠òË∑ØÂæë\n",
    "# ---------------------------\n",
    "best_model_path = os.path.join(save_folder, \"best_model.pt\")\n",
    "loss_plot_path = os.path.join(save_folder, \"loss_curve.png\")\n",
    "\n",
    "# ---------------------------\n",
    "# ÈñãÂßãË®ìÁ∑¥\n",
    "# ---------------------------\n",
    "num_epochs = 500\n",
    "best_val_loss = float('inf')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "display(fig)  # ÂàùÂßãÈ°ØÁ§∫ÂúñÂΩ¢\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_spearman = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_spearman = evaluate(model, val_loader, loss_fn, device)\n",
    "    \n",
    "    axes[0].clear()\n",
    "    axes[1].clear()\n",
    "\n",
    "    # ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°Âûã\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\"‚úÖ Saved best model!\")\n",
    "\n",
    "    # Ë™øÊï¥Â≠∏ÁøíÁéá\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # ÂØ´ÂÖ• CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch + 1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # Âç∞Âá∫ Epoch ÁµêÊûú\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | œÅ: {val_spearman:.4f} | lr: {lr:.2e}\")\n",
    "\n",
    "    # Êõ¥Êñ∞ log ÂàóË°®‰∏¶Áï´Âúñ\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_spearmanes.append(train_spearman)\n",
    "    val_spearmanes.append(val_spearman)\n",
    "\n",
    "    # Êõ¥Êñ∞ Loss Âúñ\n",
    "    plot_losses(train_losses, val_losses, ax=axes[0], title=\"MSE Loss\")\n",
    "    # Êõ¥Êñ∞ Spearman Âúñ\n",
    "    plot_losses(train_spearmanes, val_spearmanes, ax=axes[1], title=\"Spearman Loss\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    clear_output(wait=True)  # Ê∏ÖÈô§‰πãÂâçÁöÑËº∏Âá∫\n",
    "    display(fig)\n",
    "    plt.pause(0.1)  # Êö´ÂÅú‰ª•‰æøÊõ¥Êñ∞Áï´Èù¢\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"‚õî Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# ÈóúÈñâ CSV log Ê™îÊ°à\n",
    "log_file.close()\n",
    "\n",
    "# ÂÑ≤Â≠òÊúÄÁµÇÂúñÂΩ¢\n",
    "fig.savefig(loss_plot_path)\n",
    "plt.close(fig)\n",
    "print(f\"Ë®ìÁ∑¥ÁµêÊùüÔºåloss Êõ≤Á∑öÂúñÂ∑≤ÂÑ≤Â≠òËá≥ {loss_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ÈúÄË¶ÅÁöÑ Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_87169/3739314886.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"./test_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward signature: (center_tile, subtiles, neighbor_tiles, coords)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import inspect\n",
    "from python_scripts.import_data import importDataset, preprocess_data\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Ê≠£Á¢∫ÊñπÂºè\n",
    "\n",
    "test_data = torch.load(\"./test_dataset.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in test_data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "test_dataset = {\n",
    "        'center_tile': test_data['tiles'],\n",
    "        'subtiles': test_data['subtiles'],\n",
    "        'neighbor_tiles': test_data['neighbor_tiles'],\n",
    "        'coords': normalized_coords,\n",
    "        'label': np.zeros((len(test_data['tiles']), 35))\n",
    "    }\n",
    "\n",
    "\n",
    "processed_data = preprocess_data(test_dataset, image_keys, my_transform)\n",
    "\n",
    "test_dataset = importDataset(\n",
    "        data_dict=test_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset.check_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = predict(model, test_loader, device)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from revert_predict_data import revert_prediction_array\n",
    "\n",
    "test_predsÔºøre = revert_prediction_array(test_preds)\n",
    "test_predsÔºøre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== ËÆÄÂèñ test spot index Áî®ÊñºÂ∞çÊáâ ID ====\n",
    "with h5py.File(\"./elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "submission_path = os.path.join(save_folder, \"submission.csv\")\n",
    "\n",
    "ensemble_df = pd.DataFrame(test_predsÔºøre, columns=[f\"C{i+1}\" for i in range(test_predsÔºøre.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(submission_path, index=False)\n",
    "print(f\"‚úÖ Saved submission.csv in {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
