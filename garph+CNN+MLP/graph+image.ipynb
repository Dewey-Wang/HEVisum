{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6db04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b70981c-db23-4ea7-bd9c-dca6279d7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from hevisum_dataset import importDataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fb8f4b-1df0-484b-8931-3805eb21c5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_2755/235426519.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_data = torch.load(\"../train_dataset.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_2755/235426519.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(\"../train_graph_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from hevisum_dataset import importDataset\n",
    "\n",
    "# 載入資料\n",
    "img_data = torch.load(\"../train_dataset.pt\")\n",
    "graph_data = torch.load(\"../train_graph_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ef8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正確方式\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in img_data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "# ====== Step 2: 建立 Dataset ======\n",
    "train_dataset = importDataset(\n",
    "    center_tile=img_data['tiles'],\n",
    "    subtiles=img_data['subtiles'],\n",
    "    neighbor_tiles=img_data['neighbor_tiles'],\n",
    "    label=img_data['labels'],\n",
    "    meta=normalized_coords,\n",
    "    # 👇 加上 graph 資料\n",
    "    node_feat=graph_data['node_feats'],\n",
    "    adj_lists=graph_data['adj_lists'],\n",
    "    edge_feats=graph_data['edge_feats']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47c70e7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgraph_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madj_lists\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "graph_data['adj_lists'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e8a237d-3fff-47d5-831c-0b915cbecb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking dataset sample: 0\n",
      "📏 Center tile shape: torch.Size([3, 78, 78]) | dtype: torch.float32 | min: -0.192, max: 0.953\n",
      "📏 Subtiles shape: torch.Size([9, 3, 26, 26]) | dtype: torch.float32\n",
      "📏 Neighbor tiles shape: torch.Size([8, 3, 78, 78]) | dtype: torch.float32\n",
      "🧬 Label shape: torch.Size([35]) | dtype: torch.float32\n",
      "📌 Coordinates (meta): x = 1.3731348514556885, y = 0.7239122986793518\n",
      "📊 Node feature shape: torch.Size([14]) | dtype: torch.float32\n",
      "🔗 Edge feature shape: torch.Size([7, 5]) | dtype: torch.float32\n",
      "🔗 Sample edge_feat[0]: tensor([26.0000, 26.0000,  0.0000,  0.0000,  0.0385])\n",
      "📎 Adjacency list shape: torch.Size([7, 2]) | dtype: torch.float32\n",
      "📎 Sample: tensor([[6.2800e+02, 3.8462e-02],\n",
      "        [2.3700e+02, 3.8462e-02],\n",
      "        [1.3880e+03, 3.7851e-02]])\n",
      "✅ All checks passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_2755/3104603402.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float32) if self.node_feats is not None else None,\n"
     ]
    }
   ],
   "source": [
    "def check_dataset_item(dataset, idx=0):\n",
    "    item = dataset[idx]\n",
    "\n",
    "    print(\"🔍 Checking dataset sample:\", idx)\n",
    "\n",
    "    # --- Center Tile ---\n",
    "    tile = item['center_tile']\n",
    "    print(f\"📏 Center tile shape: {tile.shape} | dtype: {tile.dtype} | min: {tile.min():.3f}, max: {tile.max():.3f}\")\n",
    "    assert tile.ndim == 3 and tile.shape[0] == 3, \"❌ Center tile shape 不正確，應為 (3, H, W)\"\n",
    "\n",
    "    # --- Subtiles ---\n",
    "    subtiles = item['subtiles']\n",
    "    print(f\"📏 Subtiles shape: {subtiles.shape} | dtype: {subtiles.dtype}\")\n",
    "    assert subtiles.shape[0] == 9 and subtiles.shape[1] == 3, \"❌ Subtile shape 不正確，應為 (9, 3, h, w)\"\n",
    "\n",
    "    # --- Neighbors ---\n",
    "    neighbors = item['neighbor_tiles']\n",
    "    print(f\"📏 Neighbor tiles shape: {neighbors.shape} | dtype: {neighbors.dtype}\")\n",
    "    assert neighbors.shape[0] == 8 and neighbors.shape[1] == 3, \"❌ Neighbor tile shape 不正確，應為 (8, 3, H, W)\"\n",
    "\n",
    "    # --- Label ---\n",
    "    label = item['label']\n",
    "    print(f\"🧬 Label shape: {label.shape} | dtype: {label.dtype}\")\n",
    "    assert label.shape[0] == 35 and label.dtype == torch.float32, \"❌ Label 應為 float32 且長度為 35\"\n",
    "\n",
    "    # --- Coordinates (meta) ---\n",
    "    coordinates = item['meta']\n",
    "    print(f\"📌 Coordinates (meta): x = {coordinates[0]}, y = {coordinates[1]}\")\n",
    "    assert isinstance(coordinates, torch.Tensor) and coordinates.shape == (2,), \"❌ meta 應為 shape (2,) 的 tensor\"\n",
    "\n",
    "    # --- Node Features ---\n",
    "    if 'node_feat' in item and item['node_feat'] is not None:\n",
    "        node_feat = item['node_feat']\n",
    "        print(f\"📊 Node feature shape: {node_feat.shape} | dtype: {node_feat.dtype}\")\n",
    "        assert node_feat.ndim == 1, \"❌ Node feature 應為 1D tensor\"\n",
    "    else:\n",
    "        print(\"⚠️ Node features 未提供\")\n",
    "\n",
    "    # --- Edge Features ---\n",
    "    if 'edge_feat' in item and item['edge_feat'] is not None:\n",
    "        edge_feat = item['edge_feat']\n",
    "        print(f\"🔗 Edge feature shape: {edge_feat.shape} | dtype: {edge_feat.dtype}\")\n",
    "        print(f\"🔗 Sample edge_feat[0]: {edge_feat[0]}\")\n",
    "        assert edge_feat.ndim == 2 and edge_feat.shape[1] == 5, \"❌ Edge feature 應為 (k, 5)\"\n",
    "    else:\n",
    "        print(\"⚠️ Edge features 未提供\")\n",
    "\n",
    "    # --- Adjacency List ---\n",
    "    if 'adj_list' in item and item['adj_list'] is not None:\n",
    "        adj_list = item['adj_list']\n",
    "        print(f\"📎 Adjacency list shape: {adj_list.shape} | dtype: {adj_list.dtype}\")\n",
    "        print(f\"📎 Sample: {adj_list[:3]}\")\n",
    "        assert adj_list.ndim == 2 and adj_list.shape[1] == 2, \"❌ Adjacency list 應為 (k, 2)\"\n",
    "    else:\n",
    "        print(\"⚠️ Adjacency list 未提供\")\n",
    "\n",
    "    print(\"✅ All checks passed!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "check_dataset_item(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46d21906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 6679 samples\n",
      "✅ Val: 1670 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 設定比例\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# 拆分 Dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "print(f\"✅ Train: {len(train_set)} samples\")\n",
    "print(f\"✅ Val: {len(val_set)} samples\")\n",
    "\n",
    "# 🔹 將其包成 DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccdc3b",
   "metadata": {},
   "source": [
    "Poteintial issues:\n",
    "# 1. my val_set tiles image may be included in the sub_tiles of train_set\n",
    "\n",
    "Note: Since neighbor tiles are reused across samples, some mild information overlap may exist between train and val sets. However, final test set is completely held out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45327c2e",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac1df12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv  # 確保你有導入這個模組\n",
    "\n",
    "# GAT Encoder：將 node_feats, adj_lists, edge_feats → 局部 GAT 表徵\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_dim=14, out_dim=128, heads=4, dropout_rate=0.2):\n",
    "        super(GATModel, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.heads = heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # MLP 層：將節點特徵進行初步變換\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_dim)  # 輸入 GAT 層所需的維度\n",
    "        )\n",
    "        \n",
    "        # 初始化多個注意力頭的權重\n",
    "        self.W = nn.Parameter(torch.zeros(heads, out_dim, out_dim))  # (heads, out_dim, out_dim)\n",
    "        self.a1 = nn.Parameter(torch.zeros(heads, out_dim, 1))  # Attention weight 1\n",
    "        self.a2 = nn.Parameter(torch.zeros(heads, out_dim, 1))  # Attention weight 2\n",
    "\n",
    "    def forward(self, x, adj_lists, edge_feats):\n",
    "        \"\"\"\n",
    "        x: node_feats, (N, in_dim) → 節點特徵\n",
    "        adj_lists: (B, E, 2) → 每個圖的鄰接列表 [B, E, 2]\n",
    "        edge_feats: (B, E, edge_feat_dim) → 邊特徵\n",
    "        \"\"\"\n",
    "        # Step 1: 使用 MLP 進行節點特徵處理\n",
    "        x = self.mlp(x)  # 經過 MLP 之後 (N, out_dim)\n",
    "\n",
    "        # Step 2: 計算圖注意力，這裡我們使用 `gat_attention_layer` 來處理每個圖\n",
    "        multi_head_output = self.gat_attention_layer(x, adj_lists, edge_feats)\n",
    "        return multi_head_output  # (N, out_dim)\n",
    "\n",
    "    def gat_attention_layer(self, node_features, adj_lists, edge_feats):\n",
    "        \"\"\"\n",
    "        自定義圖注意力機制，根據邊特徵調整注意力分數\n",
    "        \"\"\"\n",
    "        num_nodes = node_features.size(0)\n",
    "        \n",
    "        # Step 3: 使用多頭注意力進行計算\n",
    "        multi_head_output = torch.zeros((num_nodes, self.heads * self.out_dim)).to(node_features.device)\n",
    "        \n",
    "        for head in range(self.heads):\n",
    "            attention_scores_head = torch.zeros((num_nodes, num_nodes)).to(node_features.device)\n",
    "\n",
    "            # Step 4: 計算每個節點對鄰居的注意力，使用自定義計算的權重\n",
    "            for i in range(num_nodes):\n",
    "                for j in adj_lists[i].long():  # .long() 確保 j 是 int64 類型\n",
    "                    attention_scores_head[i, j] = self.compute_attention_score(node_features[i], node_features[j], edge_feats[i], head)\n",
    "\n",
    "            # 使用 softmax 計算注意力權重\n",
    "            attention_scores_head = F.softmax(attention_scores_head, dim=1)\n",
    "\n",
    "            # 根據注意力權重更新節點特徵\n",
    "            weighted_sum_head = torch.matmul(attention_scores_head, node_features)\n",
    "            multi_head_output[:, head*self.out_dim:(head+1)*self.out_dim] = weighted_sum_head\n",
    "\n",
    "        return multi_head_output\n",
    "\n",
    "    def compute_attention_score(self, node_i_features, node_j_features, edge_feats, head):\n",
    "        \"\"\"\n",
    "        計算節點間的注意力分數\n",
    "        \"\"\"\n",
    "        # 計算節點特徵間的相似度，這裡使用 torch.matmul 來處理 2D 張量\n",
    "        distance_score = torch.matmul(node_i_features, node_j_features.T)  # 使用 .T 轉置，使之成為點積\n",
    "\n",
    "        # 取得邊的特徵（邊的距離或權重）\n",
    "        edge_weight = edge_feats[0][0]  # 假設邊權重來自邊特徵\n",
    "        \n",
    "        # 計算注意力分數，這裡將邊特徵影響納入考慮\n",
    "        attention_score = distance_score * edge_weight\n",
    "        \n",
    "        # 使用 LeakyReLU 激活函數來處理非線性部分（這部分在原模型中有）\n",
    "        attention_score = F.leaky_relu(attention_score, negative_slope=0.01)\n",
    "        \n",
    "        return attention_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(64, out_dim)\n",
    "\n",
    "    def forward(self, x):  # x: (B, 3, H, W)\n",
    "        x = self.cnn(x)     # → (B, 64, 1, 1)\n",
    "        x = self.flatten(x) # → (B, 64)\n",
    "        x = self.linear(x)  # → (B, out_dim)\n",
    "        return x\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)  # 👉 Linear activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "class VisionGraphFusionModel(nn.Module):\n",
    "    def __init__(self, cnn_out_dim=64, gat_out_dim=128, output_dim=35):\n",
    "        super().__init__()\n",
    "        self.encoder_spot = CNNEncoder(cnn_out_dim)\n",
    "        self.encoder_subtiles = CNNEncoder(cnn_out_dim)\n",
    "        self.encoder_neighbors = CNNEncoder(cnn_out_dim)\n",
    "\n",
    "        # 局部 GAT Encoder\n",
    "        self.gat_encoder = GATEncoder(in_dim=14, out_dim=gat_out_dim)\n",
    "\n",
    "        # concat: 3×cnn + gat\n",
    "        self.decoder = MLPDecoder(input_dim=cnn_out_dim * 3 + gat_out_dim, output_dim=output_dim)\n",
    "\n",
    "    def forward(self, center_tile, subtiles, neighbor_tiles, node_feats, adj_lists, edge_feats):\n",
    "        \"\"\"\n",
    "        node_feats:     (N, 14)         → 所有 node 的 features\n",
    "        adj_lists:      (E, 2)          → 局部鄰接列表，形狀 [邊數, 2]\n",
    "        edge_feats:     (E, edge_feat_dim) → 邊的特徵\n",
    "        \"\"\"\n",
    "        B = center_tile.size(0)\n",
    "\n",
    "        # Image encoders\n",
    "        f_center = self.encoder_spot(center_tile)  # (B, D)\n",
    "\n",
    "        B, N, C, h, w = subtiles.shape\n",
    "        subtiles = subtiles.view(B * N, C, h, w)\n",
    "        f_sub = self.encoder_subtiles(subtiles).view(B, N, -1).mean(dim=1)  # (B, D)\n",
    "\n",
    "        B, N, C, H, W = neighbor_tiles.shape\n",
    "        neighbor_tiles = neighbor_tiles.view(B * N, C, H, W)\n",
    "        f_neigh = self.encoder_neighbors(neighbor_tiles).view(B, N, -1).mean(dim=1)  # (B, D)\n",
    "\n",
    "        # Graph encoder (局部圖處理)\n",
    "        f_graph = self.gat_encoder(node_feats, adj_lists, edge_feats)  # (N, 128)\n",
    "\n",
    "        # Fuse everything\n",
    "        x = torch.cat([f_center, f_sub, f_neigh, f_graph], dim=1)  # (B, 3D + GAT)\n",
    "        return self.decoder(x)  # (B, 35)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9d3692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 🧠 訓練一個 epoch\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in pbar:\n",
    "        center = batch['center_tile'].to(device)\n",
    "        subtiles = batch['subtiles'].to(device)\n",
    "        neighbors = batch['neighbor_tiles'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "\n",
    "        # Fetch graph data\n",
    "        node_feats = batch['node_feat'].to(device)  # (N, 14)\n",
    "        adj_lists = batch['adj_list'].to(device)  # (E, 2), 局部鄰接列表\n",
    "        edge_feats = batch['edge_feat'].to(device)  # (E, edge_feat_dim), 邊特徵\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(center, subtiles, neighbors, node_feats, adj_lists, edge_feats)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * center.size(0)\n",
    "        avg_loss = total_loss / ((pbar.n + 1) * dataloader.batch_size)\n",
    "        pbar.set_postfix(loss=loss.item(), avg=avg_loss)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# 📏 驗證模型\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            center = batch['center_tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "            neighbors = batch['neighbor_tiles'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Fetch graph data\n",
    "            node_feats = batch['node_feat'].to(device)  # (N, 14)\n",
    "            adj_lists = batch['adj_list'].to(device)  # (E, 2), 局部鄰接列表\n",
    "            edge_feats = batch['edge_feat'].to(device)  # (E, edge_feat_dim), 邊特徵\n",
    "\n",
    "            out = model(center, subtiles, neighbors, node_feats, adj_lists, edge_feats)\n",
    "            loss = loss_fn(out, label)\n",
    "\n",
    "            total_loss += loss.item() * center.size(0)\n",
    "            preds.append(out.cpu())\n",
    "            targets.append(label.cpu())\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    targets = torch.cat(targets).numpy()\n",
    "\n",
    "    # ✅ Spearman correlation for each gene\n",
    "    scores = [spearmanr(preds[:, i], targets[:, i])[0] for i in range(preds.shape[1])]\n",
    "    spearman_avg = np.nanmean(scores)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), spearman_avg\n",
    "\n",
    "# 🔮 預測\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_meta = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            center = batch['center_tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "            neighbors = batch['neighbor_tiles'].to(device)\n",
    "            # Fetch graph data\n",
    "            node_feats = batch['node_feat'].to(device)  # (N, 14)\n",
    "            adj_lists = batch['adj_list'].to(device)  # (E, 2), 局部鄰接列表\n",
    "            edge_feats = batch['edge_feat'].to(device)  # (E, edge_feat_dim), 邊特徵\n",
    "\n",
    "            out = model(center, subtiles, neighbors, node_feats, adj_lists, edge_feats)\n",
    "            all_preds.append(out.cpu())\n",
    "            all_meta.extend(batch['meta'])\n",
    "\n",
    "    return torch.cat(all_preds).numpy(), all_meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b268",
   "metadata": {},
   "source": [
    "# callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6e2ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None or val_loss < self.best_score:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(val_losses, label=\"Val Loss\", marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.show()\n",
    "# 收集資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/209 [00:00<?, ?it/s]/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_2755/3104603402.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float32) if self.node_feats is not None else None,\n",
      "Training:   6%|▌         | 12/209 [00:04<00:50,  3.90it/s, avg=nan, loss=nan]       "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 🔧 設定裝置\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# 🔧 初始化模型 & 優化器\n",
    "model = VisionGraphFusionModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# 🔧 儲存 log 的設定\n",
    "log_file = open(\"training_log.csv\", mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# 🔧 用來畫圖\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "# 🔁 開始訓練\n",
    "num_epochs = 150\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_spearman = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "    # ✅ 儲存最好的模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"✅ Saved best model!\")\n",
    "\n",
    "    # ✅ 調整學習率\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # ✅ 寫入 CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch+1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # ✅ 印 epoch 結果\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | ρ: {val_spearman:.4f} | lr: {lr:.2e}\")\n",
    "\n",
    "    # ✅ 更新 loss list 並畫圖\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    plot_losses(train_losses, val_losses)\n",
    "\n",
    "    # ✅ Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"⛔ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# ✅ 關閉 log 檔案\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f55274",
   "metadata": {},
   "source": [
    "＃# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7220265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/3135847424.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionMLPModelWithCoord(\n",
       "  (encoder_spot): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_subtiles): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_neighbors): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): MLPDecoder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=194, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=35, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 需要的 Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "\n",
    "# ===== 載入訓練好的模型權重 =====\n",
    "from hevisum_model import HEVisumModel\n",
    "from hevisum_dataset import importDataset\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "model = VisionMLPModelWithCoord().to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77d22647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/1343659088.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"test_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "# 正確方式\n",
    "test_data = torch.load(\"test_dataset.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "# ====== Step 2: 建立 Dataset ======\n",
    "test_dataset = importDataset(\n",
    "    center_tile=test_data['tiles'],\n",
    "    subtiles=test_data['subtiles'],\n",
    "    neighbor_tiles=test_data['neighbor_tiles'],\n",
    "    label=np.zeros((len(test_data['tiles']), 35)),  # dummy label\n",
    "    meta=normalized_coords\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea4d8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "test_preds, test_meta = predict(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ddbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.25503415, 0.13337179, 0.1524582 , ..., 0.04139816, 0.01808124,\n",
       "        0.03039141],\n",
       "       ...,\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.23385419, 0.13244098, 0.13941698, ..., 0.04153137, 0.01910294,\n",
       "        0.03005139]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f26de3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== 讀取 test spot index 用於對應 ID ====\n",
    "with h5py.File(\"./elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "\n",
    "ensemble_df = pd.DataFrame(test_preds, columns=[f\"C{i+1}\" for i in range(test_preds.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ Saved submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
