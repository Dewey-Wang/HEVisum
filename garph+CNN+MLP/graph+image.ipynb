{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6db04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b70981c-db23-4ea7-bd9c-dca6279d7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from hevisum_dataset import importDataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fb8f4b-1df0-484b-8931-3805eb21c5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_2755/235426519.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_data = torch.load(\"../train_dataset.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_2755/235426519.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(\"../train_graph_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from hevisum_dataset import importDataset\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™\n",
    "img_data = torch.load(\"../train_dataset.pt\")\n",
    "graph_data = torch.load(\"../train_graph_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ef8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­£ç¢ºæ–¹å¼\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in img_data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "# ====== Step 2: å»ºç«‹ Dataset ======\n",
    "train_dataset = importDataset(\n",
    "    center_tile=img_data['tiles'],\n",
    "    subtiles=img_data['subtiles'],\n",
    "    neighbor_tiles=img_data['neighbor_tiles'],\n",
    "    label=img_data['labels'],\n",
    "    meta=normalized_coords,\n",
    "    # ğŸ‘‡ åŠ ä¸Š graph è³‡æ–™\n",
    "    node_feat=graph_data['node_feats'],\n",
    "    adj_lists=graph_data['adj_lists'],\n",
    "    edge_feats=graph_data['edge_feats']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47c70e7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgraph_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madj_lists\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "graph_data['adj_lists'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e8a237d-3fff-47d5-831c-0b915cbecb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking dataset sample: 0\n",
      "ğŸ“ Center tile shape: torch.Size([3, 78, 78]) | dtype: torch.float32 | min: -0.192, max: 0.953\n",
      "ğŸ“ Subtiles shape: torch.Size([9, 3, 26, 26]) | dtype: torch.float32\n",
      "ğŸ“ Neighbor tiles shape: torch.Size([8, 3, 78, 78]) | dtype: torch.float32\n",
      "ğŸ§¬ Label shape: torch.Size([35]) | dtype: torch.float32\n",
      "ğŸ“Œ Coordinates (meta): x = 1.3731348514556885, y = 0.7239122986793518\n",
      "ğŸ“Š Node feature shape: torch.Size([14]) | dtype: torch.float32\n",
      "ğŸ”— Edge feature shape: torch.Size([7, 5]) | dtype: torch.float32\n",
      "ğŸ”— Sample edge_feat[0]: tensor([26.0000, 26.0000,  0.0000,  0.0000,  0.0385])\n",
      "ğŸ“ Adjacency list shape: torch.Size([7, 2]) | dtype: torch.float32\n",
      "ğŸ“ Sample: tensor([[6.2800e+02, 3.8462e-02],\n",
      "        [2.3700e+02, 3.8462e-02],\n",
      "        [1.3880e+03, 3.7851e-02]])\n",
      "âœ… All checks passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_2755/3104603402.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float32) if self.node_feats is not None else None,\n"
     ]
    }
   ],
   "source": [
    "def check_dataset_item(dataset, idx=0):\n",
    "    item = dataset[idx]\n",
    "\n",
    "    print(\"ğŸ” Checking dataset sample:\", idx)\n",
    "\n",
    "    # --- Center Tile ---\n",
    "    tile = item['center_tile']\n",
    "    print(f\"ğŸ“ Center tile shape: {tile.shape} | dtype: {tile.dtype} | min: {tile.min():.3f}, max: {tile.max():.3f}\")\n",
    "    assert tile.ndim == 3 and tile.shape[0] == 3, \"âŒ Center tile shape ä¸æ­£ç¢ºï¼Œæ‡‰ç‚º (3, H, W)\"\n",
    "\n",
    "    # --- Subtiles ---\n",
    "    subtiles = item['subtiles']\n",
    "    print(f\"ğŸ“ Subtiles shape: {subtiles.shape} | dtype: {subtiles.dtype}\")\n",
    "    assert subtiles.shape[0] == 9 and subtiles.shape[1] == 3, \"âŒ Subtile shape ä¸æ­£ç¢ºï¼Œæ‡‰ç‚º (9, 3, h, w)\"\n",
    "\n",
    "    # --- Neighbors ---\n",
    "    neighbors = item['neighbor_tiles']\n",
    "    print(f\"ğŸ“ Neighbor tiles shape: {neighbors.shape} | dtype: {neighbors.dtype}\")\n",
    "    assert neighbors.shape[0] == 8 and neighbors.shape[1] == 3, \"âŒ Neighbor tile shape ä¸æ­£ç¢ºï¼Œæ‡‰ç‚º (8, 3, H, W)\"\n",
    "\n",
    "    # --- Label ---\n",
    "    label = item['label']\n",
    "    print(f\"ğŸ§¬ Label shape: {label.shape} | dtype: {label.dtype}\")\n",
    "    assert label.shape[0] == 35 and label.dtype == torch.float32, \"âŒ Label æ‡‰ç‚º float32 ä¸”é•·åº¦ç‚º 35\"\n",
    "\n",
    "    # --- Coordinates (meta) ---\n",
    "    coordinates = item['meta']\n",
    "    print(f\"ğŸ“Œ Coordinates (meta): x = {coordinates[0]}, y = {coordinates[1]}\")\n",
    "    assert isinstance(coordinates, torch.Tensor) and coordinates.shape == (2,), \"âŒ meta æ‡‰ç‚º shape (2,) çš„ tensor\"\n",
    "\n",
    "    # --- Node Features ---\n",
    "    if 'node_feat' in item and item['node_feat'] is not None:\n",
    "        node_feat = item['node_feat']\n",
    "        print(f\"ğŸ“Š Node feature shape: {node_feat.shape} | dtype: {node_feat.dtype}\")\n",
    "        assert node_feat.ndim == 1, \"âŒ Node feature æ‡‰ç‚º 1D tensor\"\n",
    "    else:\n",
    "        print(\"âš ï¸ Node features æœªæä¾›\")\n",
    "\n",
    "    # --- Edge Features ---\n",
    "    if 'edge_feat' in item and item['edge_feat'] is not None:\n",
    "        edge_feat = item['edge_feat']\n",
    "        print(f\"ğŸ”— Edge feature shape: {edge_feat.shape} | dtype: {edge_feat.dtype}\")\n",
    "        print(f\"ğŸ”— Sample edge_feat[0]: {edge_feat[0]}\")\n",
    "        assert edge_feat.ndim == 2 and edge_feat.shape[1] == 5, \"âŒ Edge feature æ‡‰ç‚º (k, 5)\"\n",
    "    else:\n",
    "        print(\"âš ï¸ Edge features æœªæä¾›\")\n",
    "\n",
    "    # --- Adjacency List ---\n",
    "    if 'adj_list' in item and item['adj_list'] is not None:\n",
    "        adj_list = item['adj_list']\n",
    "        print(f\"ğŸ“ Adjacency list shape: {adj_list.shape} | dtype: {adj_list.dtype}\")\n",
    "        print(f\"ğŸ“ Sample: {adj_list[:3]}\")\n",
    "        assert adj_list.ndim == 2 and adj_list.shape[1] == 2, \"âŒ Adjacency list æ‡‰ç‚º (k, 2)\"\n",
    "    else:\n",
    "        print(\"âš ï¸ Adjacency list æœªæä¾›\")\n",
    "\n",
    "    print(\"âœ… All checks passed!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "check_dataset_item(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46d21906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train: 6679 samples\n",
      "âœ… Val: 1670 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# è¨­å®šæ¯”ä¾‹\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# æ‹†åˆ† Dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "print(f\"âœ… Train: {len(train_set)} samples\")\n",
    "print(f\"âœ… Val: {len(val_set)} samples\")\n",
    "\n",
    "# ğŸ”¹ å°‡å…¶åŒ…æˆ DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccdc3b",
   "metadata": {},
   "source": [
    "Poteintial issues:\n",
    "# 1. my val_set tiles image may be included in the sub_tiles of train_set\n",
    "\n",
    "Note: Since neighbor tiles are reused across samples, some mild information overlap may exist between train and val sets. However, final test set is completely held out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45327c2e",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac1df12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv  # ç¢ºä¿ä½ æœ‰å°å…¥é€™å€‹æ¨¡çµ„\n",
    "\n",
    "# GAT Encoderï¼šå°‡ node_feats, adj_lists, edge_feats â†’ å±€éƒ¨ GAT è¡¨å¾µ\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_dim=14, out_dim=128, heads=4, dropout_rate=0.2):\n",
    "        super(GATModel, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.heads = heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # MLP å±¤ï¼šå°‡ç¯€é»ç‰¹å¾µé€²è¡Œåˆæ­¥è®Šæ›\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_dim)  # è¼¸å…¥ GAT å±¤æ‰€éœ€çš„ç¶­åº¦\n",
    "        )\n",
    "        \n",
    "        # åˆå§‹åŒ–å¤šå€‹æ³¨æ„åŠ›é ­çš„æ¬Šé‡\n",
    "        self.W = nn.Parameter(torch.zeros(heads, out_dim, out_dim))  # (heads, out_dim, out_dim)\n",
    "        self.a1 = nn.Parameter(torch.zeros(heads, out_dim, 1))  # Attention weight 1\n",
    "        self.a2 = nn.Parameter(torch.zeros(heads, out_dim, 1))  # Attention weight 2\n",
    "\n",
    "    def forward(self, x, adj_lists, edge_feats):\n",
    "        \"\"\"\n",
    "        x: node_feats, (N, in_dim) â†’ ç¯€é»ç‰¹å¾µ\n",
    "        adj_lists: (B, E, 2) â†’ æ¯å€‹åœ–çš„é„°æ¥åˆ—è¡¨ [B, E, 2]\n",
    "        edge_feats: (B, E, edge_feat_dim) â†’ é‚Šç‰¹å¾µ\n",
    "        \"\"\"\n",
    "        # Step 1: ä½¿ç”¨ MLP é€²è¡Œç¯€é»ç‰¹å¾µè™•ç†\n",
    "        x = self.mlp(x)  # ç¶“é MLP ä¹‹å¾Œ (N, out_dim)\n",
    "\n",
    "        # Step 2: è¨ˆç®—åœ–æ³¨æ„åŠ›ï¼Œé€™è£¡æˆ‘å€‘ä½¿ç”¨ `gat_attention_layer` ä¾†è™•ç†æ¯å€‹åœ–\n",
    "        multi_head_output = self.gat_attention_layer(x, adj_lists, edge_feats)\n",
    "        return multi_head_output  # (N, out_dim)\n",
    "\n",
    "    def gat_attention_layer(self, node_features, adj_lists, edge_feats):\n",
    "        \"\"\"\n",
    "        è‡ªå®šç¾©åœ–æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œæ ¹æ“šé‚Šç‰¹å¾µèª¿æ•´æ³¨æ„åŠ›åˆ†æ•¸\n",
    "        \"\"\"\n",
    "        num_nodes = node_features.size(0)\n",
    "        \n",
    "        # Step 3: ä½¿ç”¨å¤šé ­æ³¨æ„åŠ›é€²è¡Œè¨ˆç®—\n",
    "        multi_head_output = torch.zeros((num_nodes, self.heads * self.out_dim)).to(node_features.device)\n",
    "        \n",
    "        for head in range(self.heads):\n",
    "            attention_scores_head = torch.zeros((num_nodes, num_nodes)).to(node_features.device)\n",
    "\n",
    "            # Step 4: è¨ˆç®—æ¯å€‹ç¯€é»å°é„°å±…çš„æ³¨æ„åŠ›ï¼Œä½¿ç”¨è‡ªå®šç¾©è¨ˆç®—çš„æ¬Šé‡\n",
    "            for i in range(num_nodes):\n",
    "                for j in adj_lists[i].long():  # .long() ç¢ºä¿ j æ˜¯ int64 é¡å‹\n",
    "                    attention_scores_head[i, j] = self.compute_attention_score(node_features[i], node_features[j], edge_feats[i], head)\n",
    "\n",
    "            # ä½¿ç”¨ softmax è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡\n",
    "            attention_scores_head = F.softmax(attention_scores_head, dim=1)\n",
    "\n",
    "            # æ ¹æ“šæ³¨æ„åŠ›æ¬Šé‡æ›´æ–°ç¯€é»ç‰¹å¾µ\n",
    "            weighted_sum_head = torch.matmul(attention_scores_head, node_features)\n",
    "            multi_head_output[:, head*self.out_dim:(head+1)*self.out_dim] = weighted_sum_head\n",
    "\n",
    "        return multi_head_output\n",
    "\n",
    "    def compute_attention_score(self, node_i_features, node_j_features, edge_feats, head):\n",
    "        \"\"\"\n",
    "        è¨ˆç®—ç¯€é»é–“çš„æ³¨æ„åŠ›åˆ†æ•¸\n",
    "        \"\"\"\n",
    "        # è¨ˆç®—ç¯€é»ç‰¹å¾µé–“çš„ç›¸ä¼¼åº¦ï¼Œé€™è£¡ä½¿ç”¨ torch.matmul ä¾†è™•ç† 2D å¼µé‡\n",
    "        distance_score = torch.matmul(node_i_features, node_j_features.T)  # ä½¿ç”¨ .T è½‰ç½®ï¼Œä½¿ä¹‹æˆç‚ºé»ç©\n",
    "\n",
    "        # å–å¾—é‚Šçš„ç‰¹å¾µï¼ˆé‚Šçš„è·é›¢æˆ–æ¬Šé‡ï¼‰\n",
    "        edge_weight = edge_feats[0][0]  # å‡è¨­é‚Šæ¬Šé‡ä¾†è‡ªé‚Šç‰¹å¾µ\n",
    "        \n",
    "        # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸ï¼Œé€™è£¡å°‡é‚Šç‰¹å¾µå½±éŸ¿ç´å…¥è€ƒæ…®\n",
    "        attention_score = distance_score * edge_weight\n",
    "        \n",
    "        # ä½¿ç”¨ LeakyReLU æ¿€æ´»å‡½æ•¸ä¾†è™•ç†éç·šæ€§éƒ¨åˆ†ï¼ˆé€™éƒ¨åˆ†åœ¨åŸæ¨¡å‹ä¸­æœ‰ï¼‰\n",
    "        attention_score = F.leaky_relu(attention_score, negative_slope=0.01)\n",
    "        \n",
    "        return attention_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(64, out_dim)\n",
    "\n",
    "    def forward(self, x):  # x: (B, 3, H, W)\n",
    "        x = self.cnn(x)     # â†’ (B, 64, 1, 1)\n",
    "        x = self.flatten(x) # â†’ (B, 64)\n",
    "        x = self.linear(x)  # â†’ (B, out_dim)\n",
    "        return x\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)  # ğŸ‘‰ Linear activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "class VisionGraphFusionModel(nn.Module):\n",
    "    def __init__(self, cnn_out_dim=64, gat_out_dim=128, output_dim=35):\n",
    "        super().__init__()\n",
    "        self.encoder_spot = CNNEncoder(cnn_out_dim)\n",
    "        self.encoder_subtiles = CNNEncoder(cnn_out_dim)\n",
    "        self.encoder_neighbors = CNNEncoder(cnn_out_dim)\n",
    "\n",
    "        # å±€éƒ¨ GAT Encoder\n",
    "        self.gat_encoder = GATEncoder(in_dim=14, out_dim=gat_out_dim)\n",
    "\n",
    "        # concat: 3Ã—cnn + gat\n",
    "        self.decoder = MLPDecoder(input_dim=cnn_out_dim * 3 + gat_out_dim, output_dim=output_dim)\n",
    "\n",
    "    def forward(self, center_tile, subtiles, neighbor_tiles, node_feats, adj_lists, edge_feats):\n",
    "        \"\"\"\n",
    "        node_feats:     (N, 14)         â†’ æ‰€æœ‰ node çš„ features\n",
    "        adj_lists:      (E, 2)          â†’ å±€éƒ¨é„°æ¥åˆ—è¡¨ï¼Œå½¢ç‹€ [é‚Šæ•¸, 2]\n",
    "        edge_feats:     (E, edge_feat_dim) â†’ é‚Šçš„ç‰¹å¾µ\n",
    "        \"\"\"\n",
    "        B = center_tile.size(0)\n",
    "\n",
    "        # Image encoders\n",
    "        f_center = self.encoder_spot(center_tile)  # (B, D)\n",
    "\n",
    "        B, N, C, h, w = subtiles.shape\n",
    "        subtiles = subtiles.view(B * N, C, h, w)\n",
    "        f_sub = self.encoder_subtiles(subtiles).view(B, N, -1).mean(dim=1)  # (B, D)\n",
    "\n",
    "        B, N, C, H, W = neighbor_tiles.shape\n",
    "        neighbor_tiles = neighbor_tiles.view(B * N, C, H, W)\n",
    "        f_neigh = self.encoder_neighbors(neighbor_tiles).view(B, N, -1).mean(dim=1)  # (B, D)\n",
    "\n",
    "        # Graph encoder (å±€éƒ¨åœ–è™•ç†)\n",
    "        f_graph = self.gat_encoder(node_feats, adj_lists, edge_feats)  # (N, 128)\n",
    "\n",
    "        # Fuse everything\n",
    "        x = torch.cat([f_center, f_sub, f_neigh, f_graph], dim=1)  # (B, 3D + GAT)\n",
    "        return self.decoder(x)  # (B, 35)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9d3692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ğŸ§  è¨“ç·´ä¸€å€‹ epoch\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in pbar:\n",
    "        center = batch['center_tile'].to(device)\n",
    "        subtiles = batch['subtiles'].to(device)\n",
    "        neighbors = batch['neighbor_tiles'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "\n",
    "        # Fetch graph data\n",
    "        node_feats = batch['node_feat'].to(device)  # (N, 14)\n",
    "        adj_lists = batch['adj_list'].to(device)  # (E, 2), å±€éƒ¨é„°æ¥åˆ—è¡¨\n",
    "        edge_feats = batch['edge_feat'].to(device)  # (E, edge_feat_dim), é‚Šç‰¹å¾µ\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(center, subtiles, neighbors, node_feats, adj_lists, edge_feats)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * center.size(0)\n",
    "        avg_loss = total_loss / ((pbar.n + 1) * dataloader.batch_size)\n",
    "        pbar.set_postfix(loss=loss.item(), avg=avg_loss)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# ğŸ“ é©—è­‰æ¨¡å‹\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            center = batch['center_tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "            neighbors = batch['neighbor_tiles'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Fetch graph data\n",
    "            node_feats = batch['node_feat'].to(device)  # (N, 14)\n",
    "            adj_lists = batch['adj_list'].to(device)  # (E, 2), å±€éƒ¨é„°æ¥åˆ—è¡¨\n",
    "            edge_feats = batch['edge_feat'].to(device)  # (E, edge_feat_dim), é‚Šç‰¹å¾µ\n",
    "\n",
    "            out = model(center, subtiles, neighbors, node_feats, adj_lists, edge_feats)\n",
    "            loss = loss_fn(out, label)\n",
    "\n",
    "            total_loss += loss.item() * center.size(0)\n",
    "            preds.append(out.cpu())\n",
    "            targets.append(label.cpu())\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    targets = torch.cat(targets).numpy()\n",
    "\n",
    "    # âœ… Spearman correlation for each gene\n",
    "    scores = [spearmanr(preds[:, i], targets[:, i])[0] for i in range(preds.shape[1])]\n",
    "    spearman_avg = np.nanmean(scores)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), spearman_avg\n",
    "\n",
    "# ğŸ”® é æ¸¬\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_meta = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            center = batch['center_tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "            neighbors = batch['neighbor_tiles'].to(device)\n",
    "            # Fetch graph data\n",
    "            node_feats = batch['node_feat'].to(device)  # (N, 14)\n",
    "            adj_lists = batch['adj_list'].to(device)  # (E, 2), å±€éƒ¨é„°æ¥åˆ—è¡¨\n",
    "            edge_feats = batch['edge_feat'].to(device)  # (E, edge_feat_dim), é‚Šç‰¹å¾µ\n",
    "\n",
    "            out = model(center, subtiles, neighbors, node_feats, adj_lists, edge_feats)\n",
    "            all_preds.append(out.cpu())\n",
    "            all_meta.extend(batch['meta'])\n",
    "\n",
    "    return torch.cat(all_preds).numpy(), all_meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b268",
   "metadata": {},
   "source": [
    "# callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6e2ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None or val_loss < self.best_score:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(val_losses, label=\"Val Loss\", marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.show()\n",
    "# æ”¶é›†è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/209 [00:00<?, ?it/s]/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_2755/3104603402.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float32) if self.node_feats is not None else None,\n",
      "Training:   6%|â–Œ         | 12/209 [00:04<00:50,  3.90it/s, avg=nan, loss=nan]       "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ğŸ”§ è¨­å®šè£ç½®\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "# ğŸ”§ åˆå§‹åŒ–æ¨¡å‹ & å„ªåŒ–å™¨\n",
    "model = VisionGraphFusionModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# ğŸ”§ å„²å­˜ log çš„è¨­å®š\n",
    "log_file = open(\"training_log.csv\", mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# ğŸ”§ ç”¨ä¾†ç•«åœ–\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "# ğŸ” é–‹å§‹è¨“ç·´\n",
    "num_epochs = 150\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_spearman = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "    # âœ… å„²å­˜æœ€å¥½çš„æ¨¡å‹\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"âœ… Saved best model!\")\n",
    "\n",
    "    # âœ… èª¿æ•´å­¸ç¿’ç‡\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # âœ… å¯«å…¥ CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch+1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # âœ… å° epoch çµæœ\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | Ï: {val_spearman:.4f} | lr: {lr:.2e}\")\n",
    "\n",
    "    # âœ… æ›´æ–° loss list ä¸¦ç•«åœ–\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    plot_losses(train_losses, val_losses)\n",
    "\n",
    "    # âœ… Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"â›” Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# âœ… é—œé–‰ log æª”æ¡ˆ\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f55274",
   "metadata": {},
   "source": [
    "ï¼ƒ# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7220265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/3135847424.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionMLPModelWithCoord(\n",
       "  (encoder_spot): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_subtiles): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_neighbors): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): MLPDecoder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=194, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=35, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== éœ€è¦çš„ Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "\n",
    "# ===== è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹æ¬Šé‡ =====\n",
    "from hevisum_model import HEVisumModel\n",
    "from hevisum_dataset import importDataset\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "model = VisionMLPModelWithCoord().to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77d22647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/1343659088.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"test_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "# æ­£ç¢ºæ–¹å¼\n",
    "test_data = torch.load(\"test_dataset.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "# ====== Step 2: å»ºç«‹ Dataset ======\n",
    "test_dataset = importDataset(\n",
    "    center_tile=test_data['tiles'],\n",
    "    subtiles=test_data['subtiles'],\n",
    "    neighbor_tiles=test_data['neighbor_tiles'],\n",
    "    label=np.zeros((len(test_data['tiles']), 35)),  # dummy label\n",
    "    meta=normalized_coords\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea4d8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "test_preds, test_meta = predict(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ddbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.25503415, 0.13337179, 0.1524582 , ..., 0.04139816, 0.01808124,\n",
       "        0.03039141],\n",
       "       ...,\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.23385419, 0.13244098, 0.13941698, ..., 0.04153137, 0.01910294,\n",
       "        0.03005139]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f26de3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== è®€å– test spot index ç”¨æ–¼å°æ‡‰ ID ====\n",
    "with h5py.File(\"./elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "\n",
    "ensemble_df = pd.DataFrame(test_preds, columns=[f\"C{i+1}\" for i in range(test_preds.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"âœ… Saved submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
