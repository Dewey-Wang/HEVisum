{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6db04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b70981c-db23-4ea7-bd9c-dca6279d7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/spatialhackathon/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from hevisum_dataset import importDataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fb8f4b-1df0-484b-8931-3805eb21c5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_2755/235426519.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_data = torch.load(\"../train_dataset.pt\")\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_2755/235426519.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(\"../train_graph_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from hevisum_dataset import importDataset\n",
    "\n",
    "# 載入資料\n",
    "img_data = torch.load(\"../train_dataset.pt\")\n",
    "graph_data = torch.load(\"../train_graph_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ef8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正確方式\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in img_data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "# ====== Step 2: 建立 Dataset ======\n",
    "train_dataset = importDataset(\n",
    "    center_tile=img_data['tiles'],\n",
    "    subtiles=img_data['subtiles'],\n",
    "    neighbor_tiles=img_data['neighbor_tiles'],\n",
    "    label=img_data['labels'],\n",
    "    meta=normalized_coords,\n",
    "    # 👇 加上 graph 資料\n",
    "    node_feat=graph_data['node_feats'],\n",
    "    adj_lists=graph_data['adj_lists'],\n",
    "    edge_feats=graph_data['edge_feats']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47c70e7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgraph_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madj_lists\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "graph_data['adj_lists'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e8a237d-3fff-47d5-831c-0b915cbecb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking dataset sample: 0\n",
      "📏 Center tile shape: torch.Size([3, 78, 78]) | dtype: torch.float32 | min: -0.192, max: 0.953\n",
      "📏 Subtiles shape: torch.Size([9, 3, 26, 26]) | dtype: torch.float32\n",
      "📏 Neighbor tiles shape: torch.Size([8, 3, 78, 78]) | dtype: torch.float32\n",
      "🧬 Label shape: torch.Size([35]) | dtype: torch.float32\n",
      "📌 Coordinates (meta): x = 1.3731348514556885, y = 0.7239122986793518\n",
      "📊 Node feature shape: torch.Size([14]) | dtype: torch.float32\n",
      "🔗 Edge feature shape: torch.Size([7, 5]) | dtype: torch.float32\n",
      "🔗 Sample edge_feat[0]: tensor([26.0000, 26.0000,  0.0000,  0.0000,  0.0385])\n",
      "📎 Adjacency list shape: torch.Size([7, 2]) | dtype: torch.float32\n",
      "📎 Sample: tensor([[6.2800e+02, 3.8462e-02],\n",
      "        [2.3700e+02, 3.8462e-02],\n",
      "        [1.3880e+03, 3.7851e-02]])\n",
      "✅ All checks passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_2755/3104603402.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'node_feat': torch.tensor(self.node_feats[idx], dtype=torch.float32) if self.node_feats is not None else None,\n"
     ]
    }
   ],
   "source": [
    "def check_dataset_item(dataset, idx=0):\n",
    "    item = dataset[idx]\n",
    "\n",
    "    print(\"🔍 Checking dataset sample:\", idx)\n",
    "\n",
    "    # --- Center Tile ---\n",
    "    tile = item['center_tile']\n",
    "    print(f\"📏 Center tile shape: {tile.shape} | dtype: {tile.dtype} | min: {tile.min():.3f}, max: {tile.max():.3f}\")\n",
    "    assert tile.ndim == 3 and tile.shape[0] == 3, \"❌ Center tile shape 不正確，應為 (3, H, W)\"\n",
    "\n",
    "    # --- Subtiles ---\n",
    "    subtiles = item['subtiles']\n",
    "    print(f\"📏 Subtiles shape: {subtiles.shape} | dtype: {subtiles.dtype}\")\n",
    "    assert subtiles.shape[0] == 9 and subtiles.shape[1] == 3, \"❌ Subtile shape 不正確，應為 (9, 3, h, w)\"\n",
    "\n",
    "    # --- Neighbors ---\n",
    "    neighbors = item['neighbor_tiles']\n",
    "    print(f\"📏 Neighbor tiles shape: {neighbors.shape} | dtype: {neighbors.dtype}\")\n",
    "    assert neighbors.shape[0] == 8 and neighbors.shape[1] == 3, \"❌ Neighbor tile shape 不正確，應為 (8, 3, H, W)\"\n",
    "\n",
    "    # --- Label ---\n",
    "    label = item['label']\n",
    "    print(f\"🧬 Label shape: {label.shape} | dtype: {label.dtype}\")\n",
    "    assert label.shape[0] == 35 and label.dtype == torch.float32, \"❌ Label 應為 float32 且長度為 35\"\n",
    "\n",
    "    # --- Coordinates (meta) ---\n",
    "    coordinates = item['meta']\n",
    "    print(f\"📌 Coordinates (meta): x = {coordinates[0]}, y = {coordinates[1]}\")\n",
    "    assert isinstance(coordinates, torch.Tensor) and coordinates.shape == (2,), \"❌ meta 應為 shape (2,) 的 tensor\"\n",
    "\n",
    "    # --- Node Features ---\n",
    "    if 'node_feat' in item and item['node_feat'] is not None:\n",
    "        node_feat = item['node_feat']\n",
    "        print(f\"📊 Node feature shape: {node_feat.shape} | dtype: {node_feat.dtype}\")\n",
    "        assert node_feat.ndim == 1, \"❌ Node feature 應為 1D tensor\"\n",
    "    else:\n",
    "        print(\"⚠️ Node features 未提供\")\n",
    "\n",
    "    # --- Edge Features ---\n",
    "    if 'edge_feat' in item and item['edge_feat'] is not None:\n",
    "        edge_feat = item['edge_feat']\n",
    "        print(f\"🔗 Edge feature shape: {edge_feat.shape} | dtype: {edge_feat.dtype}\")\n",
    "        print(f\"🔗 Sample edge_feat[0]: {edge_feat[0]}\")\n",
    "        assert edge_feat.ndim == 2 and edge_feat.shape[1] == 5, \"❌ Edge feature 應為 (k, 5)\"\n",
    "    else:\n",
    "        print(\"⚠️ Edge features 未提供\")\n",
    "\n",
    "    # --- Adjacency List ---\n",
    "    if 'adj_list' in item and item['adj_list'] is not None:\n",
    "        adj_list = item['adj_list']\n",
    "        print(f\"📎 Adjacency list shape: {adj_list.shape} | dtype: {adj_list.dtype}\")\n",
    "        print(f\"📎 Sample: {adj_list[:3]}\")\n",
    "        assert adj_list.ndim == 2 and adj_list.shape[1] == 2, \"❌ Adjacency list 應為 (k, 2)\"\n",
    "    else:\n",
    "        print(\"⚠️ Adjacency list 未提供\")\n",
    "\n",
    "    print(\"✅ All checks passed!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "check_dataset_item(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46d21906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 6679 samples\n",
      "✅ Val: 1670 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 設定比例\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(train_ratio * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "# 拆分 Dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "print(f\"✅ Train: {len(train_set)} samples\")\n",
    "print(f\"✅ Val: {len(val_set)} samples\")\n",
    "\n",
    "# 🔹 將其包成 DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccdc3b",
   "metadata": {},
   "source": [
    "Poteintial issues:\n",
    "# 1. my val_set tiles image may be included in the sub_tiles of train_set\n",
    "\n",
    "Note: Since neighbor tiles are reused across samples, some mild information overlap may exist between train and val sets. However, final test set is completely held out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45327c2e",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac1df12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv  # 確保你有導入這個模組\n",
    "\n",
    "# GAT Encoder：將 node_feats, adj_lists, edge_feats → 局部 GAT 表徵\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_dim=14, out_dim=128, heads=4, dropout_rate=0.2):\n",
    "        super(GATModel, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.heads = heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # MLP 層：將節點特徵進行初步變換\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_dim)  # 輸入 GAT 層所需的維度\n",
    "        )\n",
    "        \n",
    "        # 初始化多個注意力頭的權重\n",
    "        self.W = nn.Parameter(torch.zeros(heads, out_dim, out_dim))  # (heads, out_dim, out_dim)\n",
    "        self.a1 = nn.Parameter(torch.zeros(heads, out_dim, 1))  # Attention weight 1\n",
    "        self.a2 = nn.Parameter(torch.zeros(heads, out_dim, 1))  # Attention weight 2\n",
    "\n",
    "    def forward(self, x, adj_lists, edge_feats):\n",
    "        \"\"\"\n",
    "        x: node_feats, (N, in_dim) → 節點特徵\n",
    "        adj_lists: (B, E, 2) → 每個圖的鄰接列表 [B, E, 2]\n",
    "        edge_feats: (B, E, edge_feat_dim) → 邊特徵\n",
    "        \"\"\"\n",
    "        # Step 1: 使用 MLP 進行節點特徵處理\n",
    "        x = self.mlp(x)  # 經過 MLP 之後 (N, out_dim)\n",
    "\n",
    "        # Step 2: 計算圖注意力，這裡我們使用 `gat_attention_layer` 來處理每個圖\n",
    "        multi_head_output = self.gat_attention_layer(x, adj_lists, edge_feats)\n",
    "        return multi_head_output  # (N, out_dim)\n",
    "\n",
    "    def gat_attention_layer(self, node_features, adj_lists, edge_feats):\n",
    "        \"\"\"\n",
    "        自定義圖注意力機制，根據邊特徵調整注意力分數\n",
    "        \"\"\"\n",
    "        num_nodes = node_features.size(0)\n",
    "        \n",
    "        # Step 3: 使用多頭注意力進行計算\n",
    "        multi_head_output = torch.zeros((num_nodes, self.heads * self.out_dim)).to(node_features.device)\n",
    "        \n",
    "        for head in range(self.heads):\n",
    "            attention_scores_head = torch.zeros((num_nodes, num_nodes)).to(node_features.device)\n",
    "\n",
    "            # Step 4: 計算每個節點對鄰居的注意力，使用自定義計算的權重\n",
    "            for i in range(num_nodes):\n",
    "                for j in adj_lists[i].long():  # .long() 確保 j 是 int64 類型\n",
    "                    attention_scores_head[i, j] = self.compute_attention_score(node_features[i], node_features[j], edge_feats[i], head)\n",
    "\n",
    "            # 使用 softmax 計算注意力權重\n",
    "            attention_scores_head = F.softmax(attention_scores_head, dim=1)\n",
    "\n",
    "            # 根據注意力權重更新節點特徵\n",
    "            weighted_sum_head = torch.matmul(attention_scores_head, node_features)\n",
    "            multi_head_output[:, head*self.out_dim:(head+1)*self.out_dim] = weighted_sum_head\n",
    "\n",
    "        return multi_head_output\n",
    "\n",
    "    def compute_attention_score(self, node_i_features, node_j_features, edge_feats, head):\n",
    "        \"\"\"\n",
    "        計算節點間的注意力分數\n",
    "        \"\"\"\n",
    "        # 計算節點特徵間的相似度，這裡使用 torch.matmul 來處理 2D 張量\n",
    "        distance_score = torch.matmul(node_i_features, node_j_features.T)  # 使用 .T 轉置，使之成為點積\n",
    "\n",
    "        # 取得邊的特徵（邊的距離或權重）\n",
    "        edge_weight = edge_feats[0][0]  # 假設邊權重來自邊特徵\n",
    "        \n",
    "        # 計算注意力分數，這裡將邊特徵影響納入考慮\n",
    "        attention_score = distance_score * edge_weight\n",
    "        \n",
    "        # 使用 LeakyReLU 激活函數來處理非線性部分（這部分在原模型中有）\n",
    "        attention_score = F.leaky_relu(attention_score, negative_slope=0.01)\n",
    "        \n",
    "        return attention_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(64, out_dim)\n",
    "\n",
    "    def forward(self, x):  # x: (B, 3, H, W)\n",
    "        x = self.cnn(x)     # → (B, 64, 1, 1)\n",
    "        x = self.flatten(x) # → (B, 64)\n",
    "        x = self.linear(x)  # → (B, out_dim)\n",
    "        return x\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)  # 👉 Linear activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "class VisionGraphFusionModel(nn.Module):\n",
    "    def __init__(self, cnn_out_dim=64, gat_out_dim=128, output_dim=35):\n",
    "        super().__init__()\n",
    "        self.encoder_spot = CNNEncoder(cnn_out_dim)\n",
    "        self.encoder_subtiles = CNNEncoder(cnn_out_dim)\n",
    "        self.encoder_neighbors = CNNEncoder(cnn_out_dim)\n",
    "\n",
    "        # 局部 GAT Encoder\n",
    "        self.gat_encoder = GATEncoder(in_dim=14, out_dim=gat_out_dim)\n",
    "\n",
    "        # concat: 3×cnn + gat\n",
    "        self.decoder = MLPDecoder(input_dim=cnn_out_dim * 3 + gat_out_dim, output_dim=output_dim)\n",
    "\n",
    "    def forward(self, center_tile, subtiles, neighbor_tiles, node_feats, adj_lists, edge_feats):\n",
    "        \"\"\"\n",
    "        node_feats:     (N, 14)         → 所有 node 的 features\n",
    "        adj_lists:      (E, 2)          → 局部鄰接列表，形狀 [邊數, 2]\n",
    "        edge_feats:     (E, edge_feat_dim) → 邊的特徵\n",
    "        \"\"\"\n",
    "        B = center_tile.size(0)\n",
    "\n",
    "        # Image encoders\n",
    "        f_center = self.encoder_spot(center_tile)  # (B, D)\n",
    "\n",
    "        B, N, C, h, w = subtiles.shape\n",
    "        subtiles = subtiles.view(B * N, C, h, w)\n",
    "        f_sub = self.encoder_subtiles(subtiles).view(B, N, -1).mean(dim=1)  # (B, D)\n",
    "\n",
    "        B, N, C, H, W = neighbor_tiles.shape\n",
    "        neighbor_tiles = neighbor_tiles.view(B * N, C, H, W)\n",
    "        f_neigh = self.encoder_neighbors(neighbor_tiles).view(B, N, -1).mean(dim=1)  # (B, D)\n",
    "\n",
    "        # Graph encoder (局部圖處理)\n",
    "        f_graph = self.gat_encoder(node_feats, adj_lists, edge_feats)  # (N, 128)\n",
    "\n",
    "        # Fuse everything\n",
    "        x = torch.cat([f_center, f_sub, f_neigh, f_graph], dim=1)  # (B, 3D + GAT)\n",
    "        return self.decoder(x)  # (B, 35)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 🧠 訓練一個 epoch\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in pbar:\n",
    "        center = batch['center_tile'].to(device)\n",
    "        subtiles = batch['subtiles'].to(device)\n",
    "        neighbors = batch['neighbor_tiles'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "\n",
    "        # Fetch graph data\n",
    "        node_feats = batch['node_feat'].to(device)  # (N, 14)\n",
    "        adj_lists = batch['adj_list'].to(device)  # (E, 2), 局部鄰接列表\n",
    "        edge_feats = batch['edge_feat'].to(device)  # (E, edge_feat_dim), 邊特徵\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(center, subtiles, neighbors, node_feats, adj_lists, edge_feats)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * center.size(0)\n",
    "        avg_loss = total_loss / ((pbar.n + 1) * dataloader.batch_size)\n",
    "        pbar.set_postfix(loss=loss.item(), avg=avg_loss)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# 📏 驗證模型\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            center = batch['center_tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "            neighbors = batch['neighbor_tiles'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Fetch graph data\n",
    "            node_feats = batch['node_feat'].to(device)  # (N, 14)\n",
    "            adj_lists = batch['adj_list'].to(device)  # (E, 2), 局部鄰接列表\n",
    "            edge_feats = batch['edge_feat'].to(device)  # (E, edge_feat_dim), 邊特徵\n",
    "\n",
    "            out = model(center, subtiles, neighbors, node_feats, adj_lists, edge_feats)\n",
    "            loss = loss_fn(out, label)\n",
    "\n",
    "            total_loss += loss.item() * center.size(0)\n",
    "            preds.append(out.cpu())\n",
    "            targets.append(label.cpu())\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    targets = torch.cat(targets).numpy()\n",
    "\n",
    "    # ✅ Spearman correlation for each gene\n",
    "    scores = [spearmanr(preds[:, i], targets[:, i])[0] for i in range(preds.shape[1])]\n",
    "    spearman_avg = np.nanmean(scores)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), spearman_avg\n",
    "\n",
    "# 🔮 預測\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_meta = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            center = batch['center_tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "            neighbors = batch['neighbor_tiles'].to(device)\n",
    "            # Fetch graph data\n",
    "            node_feats = batch['node_feat'].to(device)  # (N, 14)\n",
    "            adj_lists = batch['adj_list'].to(device)  # (E, 2), 局部鄰接列表\n",
    "            edge_feats = batch['edge_feat'].to(device)  # (E, edge_feat_dim), 邊特徵\n",
    "\n",
    "            out = model(center, subtiles, neighbors, node_feats, adj_lists, edge_feats)\n",
    "            all_preds.append(out.cpu())\n",
    "            all_meta.extend(batch['meta'])\n",
    "\n",
    "    return torch.cat(all_preds).numpy(), all_meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b268",
   "metadata": {},
   "source": [
    "# callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6e2ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None or val_loss < self.best_score:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(val_losses, label=\"Val Loss\", marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.show()\n",
    "# 收集資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c9b4556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 34\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     val_loss, val_spearman \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, loss_fn, device)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# ✅ 儲存最好的模型\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[63], line 24\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m edge_feats \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_feat\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (E, edge_feat_dim), 邊特徵\u001b[39;00m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out, label)\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[69], line 176\u001b[0m, in \u001b[0;36mVisionGraphFusionModel.forward\u001b[0;34m(self, center_tile, subtiles, neighbor_tiles, node_feats, adj_lists, edge_feats)\u001b[0m\n\u001b[1;32m    173\u001b[0m f_neigh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_neighbors(neighbor_tiles)\u001b[38;5;241m.\u001b[39mview(B, N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, D)\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Graph encoder (局部圖處理)\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m f_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (N, 128)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Fuse everything\u001b[39;00m\n\u001b[1;32m    179\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([f_center, f_sub, f_neigh, f_graph], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, 3D + GAT)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[52], line 40\u001b[0m, in \u001b[0;36mGATEncoder.forward\u001b[0;34m(self, x, adj_lists, edge_feats)\u001b[0m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x)  \u001b[38;5;66;03m# 經過 MLP 之後 (N, out_dim)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Step 2: 計算圖注意力，這裡我們使用 `gat_attention_layer` 來處理每個圖\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m multi_head_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat_attention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m multi_head_output\n",
      "Cell \u001b[0;32mIn[52], line 56\u001b[0m, in \u001b[0;36mGATEncoder.gat_attention_layer\u001b[0;34m(self, node_features, adj_lists, edge_feats)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_nodes):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# 確保 j 是整數型索引\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m adj_lists[i]\u001b[38;5;241m.\u001b[39mlong():  \u001b[38;5;66;03m# .long() 確保 j 是 int64 類型\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m         attention_scores[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_attention_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 使用 softmax 計算注意力權重\u001b[39;00m\n\u001b[1;32m     59\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 70\u001b[0m, in \u001b[0;36mGATEncoder.compute_attention_score\u001b[0;34m(self, node_i_features, node_j_features, edge_feats)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m計算節點間的注意力分數\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# 計算節點特徵間的相似度，這裡使用 torch.matmul 來處理 2D 張量\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m distance_score \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_i_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_j_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 使用 .T 轉置，使之成為點積\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# 取得邊的特徵（邊的距離或權重）\u001b[39;00m\n\u001b[1;32m     73\u001b[0m edge_weight \u001b[38;5;241m=\u001b[39m edge_feats[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# 假設邊權重來自邊特徵\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 🔧 設定裝置\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# 🔧 初始化模型 & 優化器\n",
    "model = VisionGraphFusionModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# 🔧 儲存 log 的設定\n",
    "log_file = open(\"training_log.csv\", mode=\"w\", newline=\"\")\n",
    "csv_writer = csv.writer(log_file)\n",
    "csv_writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Spearman\", \"Learning Rate\"])\n",
    "\n",
    "# 🔧 用來畫圖\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "# 🔁 開始訓練\n",
    "num_epochs = 150\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_spearman = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "    # ✅ 儲存最好的模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"✅ Saved best model!\")\n",
    "\n",
    "    # ✅ 調整學習率\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # ✅ 寫入 CSV log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    csv_writer.writerow([epoch+1, train_loss, val_loss, val_spearman, lr])\n",
    "\n",
    "    # ✅ 印 epoch 結果\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | ρ: {val_spearman:.4f} | lr: {lr:.2e}\")\n",
    "\n",
    "    # ✅ 更新 loss list 並畫圖\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    plot_losses(train_losses, val_losses)\n",
    "\n",
    "    # ✅ Early stopping\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"⛔ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# ✅ 關閉 log 檔案\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f55274",
   "metadata": {},
   "source": [
    "＃# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7220265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/3135847424.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionMLPModelWithCoord(\n",
       "  (encoder_spot): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_subtiles): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder_neighbors): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): MLPDecoder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=194, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=35, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 需要的 Libraries =====\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "\n",
    "# ===== 載入訓練好的模型權重 =====\n",
    "from hevisum_model import HEVisumModel\n",
    "from hevisum_dataset import importDataset\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "model = VisionMLPModelWithCoord().to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77d22647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_54542/1343659088.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"test_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "# 正確方式\n",
    "test_data = torch.load(\"test_dataset.pt\")\n",
    "\n",
    "raw_coords = []\n",
    "for _meta in data['meta_info']:\n",
    "    if _meta is not None:\n",
    "        _, x, y = _meta\n",
    "    else:\n",
    "        x, y = 0, 0\n",
    "    raw_coords.append([x, y])\n",
    "\n",
    "raw_coords = np.array(raw_coords)\n",
    "\n",
    "\n",
    "coord_scaler = StandardScaler()\n",
    "normalized_coords = coord_scaler.fit_transform(raw_coords)\n",
    "\n",
    "# ====== Step 2: 建立 Dataset ======\n",
    "test_dataset = importDataset(\n",
    "    center_tile=test_data['tiles'],\n",
    "    subtiles=test_data['subtiles'],\n",
    "    neighbor_tiles=test_data['neighbor_tiles'],\n",
    "    label=np.zeros((len(test_data['tiles']), 35)),  # dummy label\n",
    "    meta=normalized_coords\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea4d8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([32, 2])\n",
      "coords.shape = torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "test_preds, test_meta = predict(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ddbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.25503415, 0.13337179, 0.1524582 , ..., 0.04139816, 0.01808124,\n",
       "        0.03039141],\n",
       "       ...,\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.29158217, 0.12338457, 0.17179191, ..., 0.03849616, 0.01655579,\n",
       "        0.02754541],\n",
       "       [0.23385419, 0.13244098, 0.13941698, ..., 0.04153137, 0.01910294,\n",
       "        0.03005139]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f26de3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# ==== 讀取 test spot index 用於對應 ID ====\n",
    "with h5py.File(\"./elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    test_spot_table = pd.DataFrame(np.array(test_spots['S_7']))  # Example: S_7\n",
    "\n",
    "\n",
    "ensemble_df = pd.DataFrame(test_preds, columns=[f\"C{i+1}\" for i in range(test_preds.shape[1])])\n",
    "ensemble_df.insert(0, 'ID', test_spot_table.index)\n",
    "ensemble_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ Saved submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
