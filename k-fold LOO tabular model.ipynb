{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable / total params = 6,679,843 / 6,679,843\n",
      "Trainable / total params = 6,679,843 / 6,679,843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionMLP_MultiTask(\n",
       "  (encoder_tile): DeepTileEncoder(\n",
       "    (layer0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (mid_pool): AdaptiveAvgPool2d(output_size=(3, 3))\n",
       "    (fc): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): Linear(in_features=2560, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Dropout(p=0.1, inplace=False)\n",
       "      (5): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (6): LeakyReLU(negative_slope=0.01)\n",
       "      (7): Dropout(p=0.1, inplace=False)\n",
       "      (8): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (9): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (encoder_subtile): SubtileEncoder(\n",
       "    (layer0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (mid_pool): AdaptiveAvgPool2d(output_size=(2, 2))\n",
       "    (large_pool): AdaptiveAvgPool2d(output_size=(3, 3))\n",
       "    (fc): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): Linear(in_features=1792, out_features=256, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Dropout(p=0.1, inplace=False)\n",
       "      (5): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (6): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (encoder_center): CenterSubtileEncoder(\n",
       "    (layer0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (mid_pool): AdaptiveAvgPool2d(output_size=(2, 2))\n",
       "    (large_pool): AdaptiveAvgPool2d(output_size=(3, 3))\n",
       "    (fc): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): Linear(in_features=1792, out_features=256, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Dropout(p=0.1, inplace=False)\n",
       "      (5): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (6): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Dropout(p=0.1, inplace=False)\n",
       "    (9): Linear(in_features=64, out_features=35, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(out_channels)\n",
    "        self.act1  = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        self.act2 = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.act1(self.bn1(self.conv1(x)))\n",
    "        out = self.act2(self.bn2(self.conv2(out)))\n",
    "        if self.shortcut is not None:\n",
    "            identity = self.shortcut(x)\n",
    "        return out + identity\n",
    "\n",
    "\n",
    "\n",
    "class DeepTileEncoder(nn.Module):\n",
    "    \"\"\"加深的 Tile 分支：全局信息，多尺度池化 + 三层 MLP\"\"\"\n",
    "    def __init__(self, out_dim, in_channels=3, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "            nn.MaxPool2d(2)  # 78→39\n",
    "        )\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResidualBlock(32, 64),\n",
    "            ResidualBlock(64, 64),\n",
    "            nn.MaxPool2d(2)  # 39→19\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128),\n",
    "            ResidualBlock(128, 128),\n",
    "            nn.MaxPool2d(2)  # 19→9\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResidualBlock(128, 256),\n",
    "            ResidualBlock(256, 256)\n",
    "        )  # 保持 9×9\n",
    "\n",
    "        # 多尺度池化\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # [B,256,1,1]\n",
    "        self.mid_pool    = nn.AdaptiveAvgPool2d((3, 3))  # [B,256,3,3]\n",
    "\n",
    "        total_dim = 256*1*1 + 256*3*3\n",
    "        # 三层 MLP：total_dim → 2*out_dim → out_dim → out_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(total_dim, out_dim*4),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(out_dim*4, out_dim*2),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(out_dim*2, out_dim),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        # x: [B,256,9,9]\n",
    "        g = self.global_pool(x).contiguous().reshape(x.size(0), -1)  # [B,256]\n",
    "        m = self.mid_pool(x).contiguous().reshape(x.size(0), -1)     # [B,256*3*3]\n",
    "\n",
    "        return self.fc(torch.cat([g, m], dim=1))\n",
    "\n",
    "\n",
    "class SubtileEncoder(nn.Module):\n",
    "    \"\"\"多尺度 Subtile 分支：局部信息 + 两层 MLP\"\"\"\n",
    "    def __init__(self, out_dim, in_channels=3, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "            nn.MaxPool2d(2)  # 26→13\n",
    "        )\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResidualBlock(32, 64),\n",
    "            ResidualBlock(64, 64),\n",
    "            nn.MaxPool2d(2)  # 13→6\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128),\n",
    "            ResidualBlock(128, 128)\n",
    "        )  # 保持 6×6\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.mid_pool    = nn.AdaptiveAvgPool2d((2,2))\n",
    "        self.large_pool    = nn.AdaptiveAvgPool2d((3,3))\n",
    "\n",
    "        total_dim = 128*1*1 + 128*2*2 + 128*3*3\n",
    "        # 两层 MLP：total_dim → out_dim*2 → out_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(total_dim, out_dim*2),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(out_dim*2, out_dim),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.contiguous().reshape(B*N, C, H, W)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        # g,m: [B*N, feat]\n",
    "        g = self.global_pool(x).contiguous().reshape(B, N, -1)\n",
    "        m = self.mid_pool(x).contiguous().reshape(B, N, -1)\n",
    "        l = self.large_pool(x).contiguous().reshape(B, N, -1)\n",
    "\n",
    "        # 合并 N 张 subtiles，再 FC\n",
    "        feat = torch.cat([g, m, l], dim=2).mean(dim=1).contiguous()  # [B, total_dim]\n",
    "        return self.fc(feat)\n",
    "class CenterSubtileEncoder(nn.Module):\n",
    "    \"\"\"專門處理中心 subtile 的 Encoder\"\"\"\n",
    "    def __init__(self, out_dim, in_channels=3, negative_slope= 0.01):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "            nn.MaxPool2d(2)  # 26→13\n",
    "        )\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResidualBlock(32, 64),\n",
    "            ResidualBlock(64, 64),\n",
    "            nn.MaxPool2d(2)  # 13→6\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128),\n",
    "            ResidualBlock(128, 128)\n",
    "        )  # 6×6\n",
    "\n",
    "        # 多尺度池化\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.mid_pool    = nn.AdaptiveAvgPool2d((2,2))\n",
    "        self.large_pool    = nn.AdaptiveAvgPool2d((3,3))\n",
    "\n",
    "        total_dim = 128*1*1 + 128*2*2 + 128*3*3\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(total_dim, out_dim*2),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(out_dim*2, out_dim),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        g = self.global_pool(x).contiguous().reshape(x.size(0), -1)\n",
    "        m = self.mid_pool(x).contiguous().reshape(x.size(0), -1)\n",
    "        l = self.large_pool(x).contiguous().reshape(x.size(0), -1)\n",
    "\n",
    "        return self.fc(torch.cat([g, m, l], dim=1)).contiguous()\n",
    "\n",
    "\n",
    "\n",
    "class VisionMLP_MultiTask(nn.Module):\n",
    "    \"\"\"整體多任務模型：融合 tile + subtile + center，使用動態權重融合\"\"\"\n",
    "    def __init__(self, tile_dim=128, subtile_dim=64, output_dim=35, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.encoder_tile    = DeepTileEncoder(tile_dim)\n",
    "        self.encoder_subtile = SubtileEncoder(subtile_dim)\n",
    "        self.encoder_center  = CenterSubtileEncoder(subtile_dim)\n",
    "\n",
    "        # 輸出 decoder：輸入為 tile_dim (因為融合後只剩一個 vector)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(tile_dim + subtile_dim + subtile_dim , 256),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, tile, subtiles):\n",
    "        tile = tile.contiguous()\n",
    "        subtiles = subtiles.contiguous()\n",
    "        center = subtiles[:, 4]\n",
    "\n",
    "        f_tile = self.encoder_tile(tile)         # [B, tile_dim]\n",
    "        f_sub  = self.encoder_subtile(subtiles)  # [B, subtile_dim]\n",
    "        f_center = self.encoder_center(center)   # [B, subtile_dim]\n",
    "\n",
    "        # 拼接三個分支做 gating\n",
    "        features_cat = torch.cat([f_tile, f_sub, f_center], dim=1)  # [B, tile+sub+center]\n",
    "        return self.decoder(features_cat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 用法示例\n",
    "model = VisionMLP_MultiTask(tile_dim=128, subtile_dim=128, output_dim=35)\n",
    "\n",
    "\n",
    "# —— 5) 确保只有 decoder 可训练 ——  \n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable / total params = {trainable:,} / {total:,}\")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable / total params = {trainable:,} / {total:,}\")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same in multiple .pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/import_data.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(fpath, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded keys: dict_keys(['tile', 'source_idx', 'label', 'position', 'slide_idx', 'subtiles'])\n",
      "Samples: 8348\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import inspect\n",
    "from python_scripts.import_data import load_all_tile_data\n",
    "\n",
    "# 用法範例\n",
    "#folder = \"dataset/spot-rank/version-3/only_tile_sub/original_train\"\n",
    "folder = \"dataset/spot-rank/filtered_directly_rank/masked/realign/Macenko_masked/filtered/train_data/\"\n",
    "\n",
    "grouped_data = load_all_tile_data( \n",
    "        folder_path=folder,\n",
    "        model=model,\n",
    "        fraction=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # grouped_data 現在只會有 model.forward() 需要的 key，\n",
    "    # 像 ['tile','subtiles','neighbors','norm_coord','node_feat','adj_list','edge_feat','label','source_idx']\n",
    "print(\"Loaded keys:\", grouped_data.keys())\n",
    "print(\"Samples:\", len(next(iter(grouped_data.values()))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking dataset sample: 0\n",
      "📏 tile shape: torch.Size([3, 78, 78]) | dtype: torch.float32 | min: 0.157, max: 1.000, mean: 0.680, std: 0.142\n",
      "📏 subtiles shape: torch.Size([9, 3, 26, 26]) | dtype: torch.float32 | min: 0.157, max: 1.000, mean: 0.680, std: 0.142\n",
      "📏 label shape: torch.Size([35]) | dtype: torch.float32 | min: 1.000, max: 35.000, mean: 18.000, std: 10.247\n",
      "--- label head (前 5 個元素):\n",
      "tensor([12., 24., 18.,  6., 30.])\n",
      "📏 source_idx shape: torch.Size([]) | dtype: torch.int64 | min: 0.000, max: 0.000, mean: 0.000, std: nan\n",
      "--- source_idx 資料為純量: tensor(0)\n",
      "📏 position shape: torch.Size([2]) | dtype: torch.float32 | min: 0.171, max: 0.632, mean: 0.401, std: 0.326\n",
      "--- position head (前 5 個元素):\n",
      "tensor([0.6318, 0.1707])\n",
      "✅ All checks passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_3581/3062057575.py:79: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1744320376245/work/aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  std = tensor_float.std().item()\n"
     ]
    }
   ],
   "source": [
    "from python_scripts.import_data import convert_item, get_model_inputs\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import inspect\n",
    "import numpy as np\n",
    "\n",
    "class importDataset(Dataset):\n",
    "    def __init__(self, data_dict, model, image_keys=None, transform=None, print_sig=False):\n",
    "        self.data = data_dict\n",
    "        self.image_keys = set(image_keys) if image_keys is not None else set()\n",
    "        self.transform = transform if transform is not None else lambda x: x\n",
    "        self.forward_keys = list(get_model_inputs(model, print_sig=print_sig).parameters.keys())\n",
    "\n",
    "        expected_length = None\n",
    "        for key, value in self.data.items():\n",
    "            if expected_length is None:\n",
    "                expected_length = len(value)\n",
    "            if len(value) != expected_length:\n",
    "                raise ValueError(f\"資料欄位 '{key}' 的長度 ({len(value)}) 與預期 ({expected_length}) 不一致。\")\n",
    "\n",
    "        for key in self.forward_keys:\n",
    "            if key not in self.data:\n",
    "                raise ValueError(f\"data_dict 缺少模型 forward 所需欄位: '{key}'。目前可用的欄位: {list(self.data.keys())}\")\n",
    "        if \"label\" not in self.data:\n",
    "            raise ValueError(f\"data_dict 必須包含 'label' 欄位。可用的欄位: {list(self.data.keys())}\")\n",
    "        if \"source_idx\" not in self.data:\n",
    "            raise ValueError(\"data_dict 必須包含 'source_idx' 欄位，用於 trace 原始順序對應。\")\n",
    "        if \"position\" not in self.data:\n",
    "            raise ValueError(\"data_dict 必須包含 'position' 欄位，用於 trace 原始順序對應。\")\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.data.values())))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for key in self.forward_keys:\n",
    "            value = self.data[key][idx]\n",
    "            value = self.transform(value)\n",
    "            value = convert_item(value, is_image=(key in self.image_keys))\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                value = value.float()\n",
    "            sample[key] = value\n",
    "\n",
    "        label = self.transform(self.data[\"label\"][idx])\n",
    "        label = convert_item(label, is_image=False)\n",
    "        if isinstance(label, torch.Tensor):\n",
    "            label = label.float()\n",
    "        sample[\"label\"] = label\n",
    "\n",
    "        # 加入 source_idx\n",
    "        source_idx = self.data[\"source_idx\"][idx]\n",
    "        sample[\"source_idx\"] = torch.tensor(source_idx, dtype=torch.long)\n",
    "        # 加入 position （假设 data_dict 中 'position' 是 (x, y) 或 [x, y]）\n",
    "        pos = self.data[\"position\"][idx]\n",
    "        sample[\"position\"] = torch.tensor(pos, dtype=torch.float)\n",
    "        return sample\n",
    "    def check_item(self, idx=0, num_lines=5):\n",
    "        expected_keys = self.forward_keys + ['label', 'source_idx', 'position']\n",
    "        sample = self[idx]\n",
    "        print(f\"🔍 Checking dataset sample: {idx}\")\n",
    "        for key in expected_keys:\n",
    "            if key not in sample:\n",
    "                print(f\"❌ 資料中缺少 key: {key}\")\n",
    "                continue\n",
    "            tensor = sample[key]\n",
    "            if isinstance(tensor, torch.Tensor):\n",
    "                try:\n",
    "                    shape = tensor.shape\n",
    "                except Exception:\n",
    "                    shape = \"N/A\"\n",
    "                dtype = tensor.dtype if hasattr(tensor, \"dtype\") else \"N/A\"\n",
    "                output_str = f\"📏 {key} shape: {shape} | dtype: {dtype}\"\n",
    "                if tensor.numel() > 0:\n",
    "                    try:\n",
    "                        tensor_float = tensor.float()\n",
    "                        mn = tensor_float.min().item()\n",
    "                        mx = tensor_float.max().item()\n",
    "                        mean = tensor_float.mean().item()\n",
    "                        std = tensor_float.std().item()\n",
    "                        output_str += f\" | min: {mn:.3f}, max: {mx:.3f}, mean: {mean:.3f}, std: {std:.3f}\"\n",
    "                    except Exception:\n",
    "                        output_str += \" | 無法計算統計數據\"\n",
    "                print(output_str)\n",
    "                if key not in self.image_keys:\n",
    "                    if tensor.ndim == 0:\n",
    "                        print(f\"--- {key} 資料為純量:\", tensor)\n",
    "                    elif tensor.ndim == 1:\n",
    "                        print(f\"--- {key} head (前 {num_lines} 個元素):\")\n",
    "                        print(tensor[:num_lines])\n",
    "                    else:\n",
    "                        print(f\"--- {key} head (前 {num_lines} 列):\")\n",
    "                        print(tensor[:num_lines])\n",
    "            else:\n",
    "                # 如果 position 存的是 list/tuple/etc，也会走这里\n",
    "                print(f\"📏 {key} (非 tensor 資料):\", tensor)\n",
    "        print(\"✅ All checks passed!\")\n",
    "\n",
    "\n",
    "full_dataset = importDataset(grouped_data, model,\n",
    "                             image_keys=['tile','subtiles'],\n",
    "                             transform=lambda x: x)\n",
    "\n",
    "full_dataset.check_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from python_scripts.image_features import  *\n",
    "from python_scripts.prediction_features import  *\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# === Main Function with Names ===\n",
    "def generate_meta_features(dataset, model_for_recon, device, ae_type, oof_preds = None, latents = None):\n",
    "    \"\"\"\n",
    "    Generate meta-features and corresponding names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : np.ndarray, shape (n_samples, n_features)\n",
    "    names    : list of str, length n_features\n",
    "    \"\"\"\n",
    "\n",
    "    # loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # 1) 收集所有 (feats, names) 到同一个 outputs 列表\n",
    "    outputs = []\n",
    "\n",
    "    # # # AE reconstruction loss\n",
    "    # feats, names = compute_ae_reconstruction_loss(model_for_recon, loader, device, ae_type)\n",
    "    # feats = feats[:, None]\n",
    "    # outputs.append((feats, names))\n",
    "\n",
    "    # # # # AE embeddings\n",
    "    # # feats, names = compute_ae_embeddings(loader, model_for_recon, device)\n",
    "    # # outputs.append((feats, names))\n",
    "    # if latents is not None:\n",
    "    #     # 原始 35 维 preds\n",
    "    #     n_classes = latents.shape[1]\n",
    "    #     raw_names = [f\"trained-latents_{i}\" for i in range(n_classes)]\n",
    "    #     outputs.append((latents, raw_names))\n",
    "    # # # # Latent stats\n",
    "    # # # latent_feats = outputs[1][0]\n",
    "    # # # feats, names = compute_latent_stats(latent_feats)\n",
    "    # # # outputs.append((feats, names))\n",
    "\n",
    "    # # # # RGB stats\n",
    "\n",
    "    # feats, names = compute_center_subtile_rgb_stats(dataset)\n",
    "    # outputs.append((feats, names))\n",
    "    # feats, names = compute_subtiles_except_center_rgb_stats(dataset)\n",
    "    # outputs.append((feats, names))\n",
    "    # feats, names = compute_tile_rgb_stats(dataset)\n",
    "    # outputs.append((feats, names))\n",
    "    # feats, names = compute_subtile_contrast_stats(dataset)\n",
    "    # outputs.append((feats, names))\n",
    "\n",
    "    # # # # Texture & pattern features\n",
    "    # feats, names = compute_wavelet_stats(dataset)\n",
    "    # outputs.append((feats, names))\n",
    "    # feats, names = compute_sobel_stats(dataset)\n",
    "    # outputs.append((feats, names))\n",
    "\n",
    "    # # Color & distribution features\n",
    "    # feats, names = compute_hsv_stats(dataset)\n",
    "    # outputs.append((feats, names))\n",
    "    # # feats, names = compute_color_moments(dataset)\n",
    "    # # outputs.append((feats, names))\n",
    "\n",
    "    # # H&E stain features\n",
    "    # feats, names = compute_he_stats(dataset)\n",
    "    # outputs.append((feats, names))\n",
    "\n",
    "    # # # # Sliding-window std stats\n",
    "    # # # feats, names = compute_sliding_window_stats(dataset)\n",
    "    # # # outputs.append((feats, names))\n",
    "\n",
    "    # # 9) OOF-based features (only if provided)\n",
    "    if oof_preds is not None:\n",
    "        # 原始 35 维 preds\n",
    "        n_classes = oof_preds.shape[1]\n",
    "        raw_names = [f\"oof_pred_{i}\" for i in range(n_classes)]\n",
    "        outputs.append((oof_preds, raw_names))\n",
    "\n",
    "        # # 相邻差异\n",
    "        feats, names = compute_adjacent_diffs(oof_preds, stride=1)\n",
    "        outputs.append((feats, names))\n",
    "\n",
    "        # # top-2..top-6 统计\n",
    "        feats, names = compute_lastn_stats_multi(oof_preds, max_n=35)\n",
    "        outputs.append((feats, names))\n",
    "        # # feats, names = compute_topn_stats_multi(oof_preds, max_n=6)\n",
    "        # # outputs.append((feats, names))\n",
    "        # # 更多可选——只需取消注释即可\n",
    "        # feats, names = compute_adj_diff_histogram(oof_preds)\n",
    "        # outputs.append((feats, names))\n",
    "        # feats, names = compute_multi_stride_diffs(oof_preds)\n",
    "        # outputs.append((feats, names))\n",
    "        feats, names = compute_median_mad(oof_preds)\n",
    "        outputs.append((feats, names))\n",
    "        feats, names = compute_skew_kurt(oof_preds)\n",
    "        outputs.append((feats, names))\n",
    "        # # feats, names = compute_percentile_iqr(oof_preds)\n",
    "        # # outputs.append((feats, names))\n",
    "        # # feats, names = compute_renyi_entropy(oof_preds, alpha=2)\n",
    "        # # outputs.append((feats, names))\n",
    "        # # feats, names = compute_js_uniform(oof_preds)\n",
    "        # # outputs.append((feats, names))\n",
    "        # # feats, names = compute_mass_topk(oof_preds, k=5)\n",
    "        # # outputs.append((feats, names))\n",
    "        # # feats, names = compute_cdf_slope(oof_preds)\n",
    "        # # outputs.append((feats, names))\n",
    "        # feats, names = compute_pca_components(oof_preds, n_components=10)\n",
    "        # outputs.append((feats, names))\n",
    "        # # feats, names = compute_log_stats(oof_preds)\n",
    "        # # outputs.append((feats, names))\n",
    "        # # feats, names = compute_peak_stats(oof_preds)\n",
    "        # # outputs.append((feats, names))\n",
    "        # # feats, names = compute_segment_stats(oof_preds)\n",
    "        # # outputs.append((feats, names))\n",
    "        # feats, names = compute_ar_coeffs(oof_preds)\n",
    "        # outputs.append((feats, names))\n",
    "        feats, names = compute_autocorr_features(oof_preds)\n",
    "        outputs.append((feats, names))\n",
    "        feats, names = compute_second_order_diffs(oof_preds)\n",
    "        outputs.append((feats, names))\n",
    "        feats, names = compute_third_order_diffs(oof_preds)\n",
    "        outputs.append((feats, names))\n",
    "        # feats, names = compute_log_adjacent_diffs(oof_preds)\n",
    "        # outputs.append((feats, names))\n",
    "        # feats, names = compute_relative_diffs(oof_preds)\n",
    "        # outputs.append((feats, names))\n",
    "        # feats, names = compute_diff_ratio_of_diffs(oof_preds)\n",
    "        # outputs.append((feats, names))\n",
    "\n",
    "\n",
    "    # 2) unzip 成 feat_list 和 name_seq\n",
    "    feat_list, name_seq = zip(*outputs)\n",
    "\n",
    "    # 3) 逐块校验 feats 列数与 names 长度\n",
    "    for feats, names_block in zip(feat_list, name_seq):\n",
    "        ncols = feats.shape[1] if feats.ndim == 2 else 1\n",
    "        if ncols != len(names_block):\n",
    "            raise ValueError(\n",
    "                f\"Mismatch: got {ncols} columns but {len(names_block)} names \"\n",
    "                f\"in block '{names_block[0].split('_')[0]}'\"\n",
    "            )\n",
    "        print(\n",
    "            f\"{names_block[0].split('_')[0]:12s} -> cols: {ncols:4d}, names: {len(names_block):4d} OK\"\n",
    "        )\n",
    "\n",
    "    # 4) 扁平化 names 并拼接 features\n",
    "    name_list = [nm for block in name_seq for nm in block]\n",
    "    features = np.concatenate(feat_list, axis=1)\n",
    "    print(f\"✅ Generated meta-features with shape: {features.shape}\")\n",
    "\n",
    "    return features, name_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def diagnose_meta_nonfinite(meta: np.ndarray, names: list[str]):\n",
    "    \"\"\"\n",
    "    按名字前缀分组，统计每组：\n",
    "      - 原始特征数（列数）\n",
    "      - 总值数（列数 × 行数）\n",
    "      - NaN 值数量\n",
    "      - ±Inf 值数量\n",
    "      - 非数值（non-finite）总数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    meta  : np.ndarray, shape (n_samples, n_features)\n",
    "    names : list of str, length n_features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats : dict[prefix, dict]  \n",
    "        每个 prefix 对应一个字典，\n",
    "        包含 'n_feats','total_vals','n_nan','n_inf','n_nonfinite'。\n",
    "    \"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    # 按前缀分组\n",
    "    for idx, nm in enumerate(names):\n",
    "        prefix = nm.split('_', 1)[0]\n",
    "        groups[prefix].append(idx)\n",
    "\n",
    "    stats = {}\n",
    "    for prefix, idxs in groups.items():\n",
    "        sub = meta[:, idxs]  # shape (n_samples, n_group_feats)\n",
    "        n_feats = sub.shape[1]\n",
    "        total_vals = sub.size\n",
    "        n_nan = np.isnan(sub).sum()\n",
    "        n_inf = np.isinf(sub).sum()\n",
    "        n_nonfinite = (~np.isfinite(sub)).sum()\n",
    "\n",
    "        stats[prefix] = {\n",
    "            'n_feats':        n_feats,\n",
    "            'total_vals':     total_vals,\n",
    "            'n_nan':          int(n_nan),\n",
    "            'n_inf':          int(n_inf),\n",
    "            'n_nonfinite':    int(n_nonfinite),\n",
    "        }\n",
    "        print(\n",
    "            f\"Group '{prefix}': \"\n",
    "            f\"features={n_feats}, \"\n",
    "            f\"values={total_vals}, \"\n",
    "            f\"non-finite={n_nonfinite} \"\n",
    "            f\"(nan={n_nan}, inf={n_inf})\"\n",
    "        )\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 'ae-recon-loss': original 1 dims, PCA-> 1\n",
      "Group 'ae': original 384 dims, PCA-> 76\n",
      "Group 'subtile4': original 12 dims, PCA-> 2\n",
      "Group 'exsubtiles': original 12 dims, PCA-> 2\n",
      "Group 'tile': original 12 dims, PCA-> 2\n",
      "Group 'contrast': original 3 dims, PCA-> 1\n",
      "Group 'wavelet-tile': original 4 dims, PCA-> 1\n",
      "Group 'wavelet': original 240 dims, PCA-> 48\n",
      "Group 'wavelet-subtile': original 36 dims, PCA-> 7\n",
      "Group 'sobel-tile': original 4 dims, PCA-> 1\n",
      "Group 'sobel-subtile': original 36 dims, PCA-> 7\n",
      "Group 'hsv-tile': original 12 dims, PCA-> 2\n",
      "Group 'hsv-subtile': original 108 dims, PCA-> 21\n",
      "Group 'color-tile': original 12 dims, PCA-> 2\n",
      "Group 'color-subtile': original 108 dims, PCA-> 21\n",
      "Group 'he-tile': original 8 dims, PCA-> 1\n",
      "Group 'he-subtile': original 72 dims, PCA-> 14\n",
      "Group 'oof': original 35 dims, PCA-> 7\n",
      "Group 'adj': original 34 dims, PCA-> 6\n",
      "Group 'adj-his': original 10 dims, PCA-> 2\n",
      "Group 'diff': original 595 dims, PCA-> 119\n",
      "Group 'value': original 783 dims, PCA-> 156\n",
      "Group 'mad': original 1 dims, PCA-> 1\n",
      "Group 'pca': original 10 dims, PCA-> 2\n",
      "Group 'arcoef': original 3 dims, PCA-> 1\n",
      "Group 'autocorr': original 5 dims, PCA-> 1\n",
      "Group 'diff2': original 33 dims, PCA-> 6\n",
      "Group 'diff3': original 32 dims, PCA-> 6\n",
      "Group 'logdiff': original 34 dims, PCA-> 6\n",
      "Group 'reldiff': original 34 dims, PCA-> 6\n",
      "Group 'dratio': original 33 dims, PCA-> 6\n",
      "总降维后维度：534\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def groupwise_reduce(\n",
    "    features: np.ndarray,\n",
    "    names: list[str],   \n",
    "    pca_ratio: float = 0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    对每个类别组做 PCA 降维：\n",
    "      - 对组内所有特征直接做 PCA，保留 pca_ratio 比例的主成分（至少1维）\n",
    "      - 若 pca_ratio * m >= m，则保留全部原始特征\n",
    "    返回拼接后的特征矩阵和对应名称\n",
    "    \"\"\"\n",
    "    # 1) 按名字前缀分组\n",
    "    groups = defaultdict(list)\n",
    "    for idx, nm in enumerate(names):\n",
    "        prefix = nm.split('_', 1)[0]\n",
    "        groups[prefix].append(idx)\n",
    "\n",
    "    all_feats = []\n",
    "    all_names = []\n",
    "\n",
    "    # 2) 对每个组直接做 PCA\n",
    "    for prefix, idxs in groups.items():\n",
    "        Xg = features[:, idxs]\n",
    "        # 清洗非有限值\n",
    "        Xg = np.nan_to_num(Xg, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        m = Xg.shape[1]\n",
    "        if m == 0:\n",
    "            continue\n",
    "\n",
    "        # 保留主成分数 k\n",
    "        k = max(1, int(np.floor(m * pca_ratio)))\n",
    "\n",
    "        if k >= m:\n",
    "            # 保留原始特征\n",
    "            Xg_p = Xg\n",
    "            names_p = [names[i] for i in idxs]\n",
    "        else:\n",
    "            # PCA 降维到 k\n",
    "            pca = PCA(n_components=k, random_state=0)\n",
    "            Xg_p = pca.fit_transform(Xg)\n",
    "            names_p = [f\"{prefix}_pca{i}\" for i in range(k)]\n",
    "\n",
    "        print(f\"Group '{prefix}': original {m} dims, PCA-> {Xg_p.shape[1]}\")\n",
    "        all_feats.append(Xg_p)\n",
    "        all_names += names_p\n",
    "\n",
    "    # 3) 拼接结果\n",
    "    if not all_feats:\n",
    "        raise RuntimeError(\"所有组降维失败，请检查输入。\")\n",
    "    reduced_feats = np.hstack(all_feats)\n",
    "    print(f\"总降维后维度：{reduced_feats.shape[1]}\")\n",
    "    return reduced_feats, all_names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "meta, name = groupwise_reduce(\n",
    "        features=meta_intial,\n",
    "        names=name_intial,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting fold 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_3581/2556379003.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:310: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "Computing AE recon loss: 100%|██████████| 35/35 [00:02<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae-recon-loss -> cols:    1, names:    1 OK\n",
      "ae           -> cols:  384, names:  384 OK\n",
      "subtile4     -> cols:   12, names:   12 OK\n",
      "exsubtiles   -> cols:   12, names:   12 OK\n",
      "tile         -> cols:   12, names:   12 OK\n",
      "contrast     -> cols:    3, names:    3 OK\n",
      "wavelet-tile -> cols:  280, names:  280 OK\n",
      "oof          -> cols:   35, names:   35 OK\n",
      "mad          -> cols:    1, names:    1 OK\n",
      "✅ Generated meta-features with shape: (2197, 740)\n",
      "Group 'ae-recon-loss': features=1, values=2197, non-finite=0 (nan=0, inf=0)\n",
      "Group 'ae': features=384, values=843648, non-finite=0 (nan=0, inf=0)\n",
      "Group 'subtile4': features=12, values=26364, non-finite=0 (nan=0, inf=0)\n",
      "Group 'exsubtiles': features=12, values=26364, non-finite=0 (nan=0, inf=0)\n",
      "Group 'tile': features=12, values=26364, non-finite=0 (nan=0, inf=0)\n",
      "Group 'contrast': features=3, values=6591, non-finite=0 (nan=0, inf=0)\n",
      "Group 'wavelet-tile': features=4, values=8788, non-finite=0 (nan=0, inf=0)\n",
      "Group 'wavelet': features=240, values=527280, non-finite=0 (nan=0, inf=0)\n",
      "Group 'wavelet-subtile': features=36, values=79092, non-finite=0 (nan=0, inf=0)\n",
      "Group 'oof': features=35, values=76895, non-finite=0 (nan=0, inf=0)\n",
      "Group 'mad': features=1, values=2197, non-finite=0 (nan=0, inf=0)\n",
      "[fold 0] training target 1 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 6.4736\n",
      "[200]\tvalid_0's rmse: 6.25645\n",
      "[300]\tvalid_0's rmse: 6.20793\n",
      "[400]\tvalid_0's rmse: 6.18453\n",
      "[500]\tvalid_0's rmse: 6.18401\n",
      "[600]\tvalid_0's rmse: 6.18296\n",
      "[700]\tvalid_0's rmse: 6.18382\n",
      "Early stopping, best iteration is:\n",
      "[575]\tvalid_0's rmse: 6.18063\n",
      "[fold 0] training target 2 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 2.91184\n",
      "[200]\tvalid_0's rmse: 2.40775\n",
      "[300]\tvalid_0's rmse: 2.29765\n",
      "[400]\tvalid_0's rmse: 2.26951\n",
      "[500]\tvalid_0's rmse: 2.26323\n",
      "[600]\tvalid_0's rmse: 2.26157\n",
      "[700]\tvalid_0's rmse: 2.26021\n",
      "[800]\tvalid_0's rmse: 2.25999\n",
      "[900]\tvalid_0's rmse: 2.25912\n",
      "[1000]\tvalid_0's rmse: 2.25796\n",
      "[1100]\tvalid_0's rmse: 2.25884\n",
      "[1200]\tvalid_0's rmse: 2.25902\n",
      "Early stopping, best iteration is:\n",
      "[1003]\tvalid_0's rmse: 2.25789\n",
      "[fold 0] training target 3 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 4.59983\n",
      "[200]\tvalid_0's rmse: 4.41081\n",
      "[300]\tvalid_0's rmse: 4.36725\n",
      "[400]\tvalid_0's rmse: 4.3615\n",
      "[500]\tvalid_0's rmse: 4.35981\n",
      "[600]\tvalid_0's rmse: 4.35907\n",
      "[700]\tvalid_0's rmse: 4.35697\n",
      "[800]\tvalid_0's rmse: 4.35613\n",
      "[900]\tvalid_0's rmse: 4.35808\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's rmse: 4.35534\n",
      "[fold 0] training target 4 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 6.89677\n",
      "[200]\tvalid_0's rmse: 6.83021\n",
      "[300]\tvalid_0's rmse: 6.81388\n",
      "[400]\tvalid_0's rmse: 6.80754\n",
      "[500]\tvalid_0's rmse: 6.81243\n",
      "[600]\tvalid_0's rmse: 6.81858\n",
      "Early stopping, best iteration is:\n",
      "[411]\tvalid_0's rmse: 6.8048\n",
      "[fold 0] training target 5 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 6.78647\n",
      "[200]\tvalid_0's rmse: 5.61799\n",
      "[300]\tvalid_0's rmse: 5.37731\n",
      "[400]\tvalid_0's rmse: 5.31466\n",
      "[500]\tvalid_0's rmse: 5.29551\n",
      "[600]\tvalid_0's rmse: 5.28805\n",
      "[700]\tvalid_0's rmse: 5.28633\n",
      "[800]\tvalid_0's rmse: 5.28116\n",
      "[900]\tvalid_0's rmse: 5.28074\n",
      "[1000]\tvalid_0's rmse: 5.28015\n",
      "Early stopping, best iteration is:\n",
      "[838]\tvalid_0's rmse: 5.2778\n",
      "[fold 0] training target 6 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.49461\n",
      "[200]\tvalid_0's rmse: 6.60013\n",
      "[300]\tvalid_0's rmse: 6.33968\n",
      "[400]\tvalid_0's rmse: 6.2551\n",
      "[500]\tvalid_0's rmse: 6.22707\n",
      "[600]\tvalid_0's rmse: 6.21246\n",
      "[700]\tvalid_0's rmse: 6.20449\n",
      "[800]\tvalid_0's rmse: 6.20246\n",
      "[900]\tvalid_0's rmse: 6.20374\n",
      "[1000]\tvalid_0's rmse: 6.2026\n",
      "[1100]\tvalid_0's rmse: 6.20405\n",
      "Early stopping, best iteration is:\n",
      "[991]\tvalid_0's rmse: 6.20196\n",
      "[fold 0] training target 7 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 4.4015\n",
      "[200]\tvalid_0's rmse: 4.06187\n",
      "[300]\tvalid_0's rmse: 4.01906\n",
      "[400]\tvalid_0's rmse: 4.01504\n",
      "[500]\tvalid_0's rmse: 4.01747\n",
      "Early stopping, best iteration is:\n",
      "[380]\tvalid_0's rmse: 4.01298\n",
      "[fold 0] training target 8 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 5.38683\n",
      "[200]\tvalid_0's rmse: 5.38883\n",
      "[300]\tvalid_0's rmse: 5.40409\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's rmse: 5.38303\n",
      "[fold 0] training target 9 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.14523\n",
      "[200]\tvalid_0's rmse: 6.48218\n",
      "[300]\tvalid_0's rmse: 6.33184\n",
      "[400]\tvalid_0's rmse: 6.28579\n",
      "[500]\tvalid_0's rmse: 6.26549\n",
      "[600]\tvalid_0's rmse: 6.25396\n",
      "[700]\tvalid_0's rmse: 6.25192\n",
      "[800]\tvalid_0's rmse: 6.25051\n",
      "[900]\tvalid_0's rmse: 6.24834\n",
      "[1000]\tvalid_0's rmse: 6.24769\n",
      "[1100]\tvalid_0's rmse: 6.2444\n",
      "[1200]\tvalid_0's rmse: 6.24476\n",
      "[1300]\tvalid_0's rmse: 6.24568\n",
      "[1400]\tvalid_0's rmse: 6.24456\n",
      "Early stopping, best iteration is:\n",
      "[1234]\tvalid_0's rmse: 6.2438\n",
      "[fold 0] training target 10 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 8.83237\n",
      "[200]\tvalid_0's rmse: 8.23889\n",
      "[300]\tvalid_0's rmse: 8.06068\n",
      "[400]\tvalid_0's rmse: 7.99901\n",
      "[500]\tvalid_0's rmse: 7.9679\n",
      "[600]\tvalid_0's rmse: 7.95447\n",
      "[700]\tvalid_0's rmse: 7.96057\n",
      "Early stopping, best iteration is:\n",
      "[597]\tvalid_0's rmse: 7.95351\n",
      "[fold 0] training target 11 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 8.50684\n",
      "[200]\tvalid_0's rmse: 8.19453\n",
      "[300]\tvalid_0's rmse: 8.1009\n",
      "[400]\tvalid_0's rmse: 8.06818\n",
      "[500]\tvalid_0's rmse: 8.05204\n",
      "[600]\tvalid_0's rmse: 8.05609\n",
      "[700]\tvalid_0's rmse: 8.05598\n",
      "Early stopping, best iteration is:\n",
      "[502]\tvalid_0's rmse: 8.05113\n",
      "[fold 0] training target 12 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 4.75498\n",
      "[200]\tvalid_0's rmse: 4.37706\n",
      "[300]\tvalid_0's rmse: 4.27482\n",
      "[400]\tvalid_0's rmse: 4.24081\n",
      "[500]\tvalid_0's rmse: 4.2264\n",
      "[600]\tvalid_0's rmse: 4.22626\n",
      "[700]\tvalid_0's rmse: 4.22578\n",
      "Early stopping, best iteration is:\n",
      "[581]\tvalid_0's rmse: 4.22423\n",
      "[fold 0] training target 13 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 5.95287\n",
      "[200]\tvalid_0's rmse: 5.93471\n",
      "[300]\tvalid_0's rmse: 5.94731\n",
      "[400]\tvalid_0's rmse: 5.94845\n",
      "Early stopping, best iteration is:\n",
      "[203]\tvalid_0's rmse: 5.93074\n",
      "[fold 0] training target 14 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 5.75118\n",
      "[200]\tvalid_0's rmse: 5.74793\n",
      "[300]\tvalid_0's rmse: 5.7615\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid_0's rmse: 5.73401\n",
      "[fold 0] training target 15 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 5.35797\n",
      "[200]\tvalid_0's rmse: 5.12561\n",
      "[300]\tvalid_0's rmse: 5.06011\n",
      "[400]\tvalid_0's rmse: 5.02929\n",
      "[500]\tvalid_0's rmse: 5.00371\n",
      "[600]\tvalid_0's rmse: 4.99811\n",
      "[700]\tvalid_0's rmse: 4.99458\n",
      "[800]\tvalid_0's rmse: 4.98762\n",
      "[900]\tvalid_0's rmse: 4.98667\n",
      "[1000]\tvalid_0's rmse: 4.98581\n",
      "[1100]\tvalid_0's rmse: 4.98318\n",
      "[1200]\tvalid_0's rmse: 4.98199\n",
      "[1300]\tvalid_0's rmse: 4.98257\n",
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's rmse: 4.98161\n",
      "[fold 0] training target 16 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 2.52792\n",
      "[200]\tvalid_0's rmse: 2.45168\n",
      "[300]\tvalid_0's rmse: 2.41873\n",
      "[400]\tvalid_0's rmse: 2.40534\n",
      "[500]\tvalid_0's rmse: 2.39342\n",
      "[600]\tvalid_0's rmse: 2.38132\n",
      "[700]\tvalid_0's rmse: 2.37582\n",
      "[800]\tvalid_0's rmse: 2.37476\n",
      "[900]\tvalid_0's rmse: 2.37257\n",
      "[1000]\tvalid_0's rmse: 2.36883\n",
      "[1100]\tvalid_0's rmse: 2.36819\n",
      "[1200]\tvalid_0's rmse: 2.36771\n",
      "[1300]\tvalid_0's rmse: 2.36815\n",
      "[1400]\tvalid_0's rmse: 2.36809\n",
      "Early stopping, best iteration is:\n",
      "[1230]\tvalid_0's rmse: 2.36666\n",
      "[fold 0] training target 17 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.51472\n",
      "[200]\tvalid_0's rmse: 6.84857\n",
      "[300]\tvalid_0's rmse: 6.71349\n",
      "[400]\tvalid_0's rmse: 6.67037\n",
      "[500]\tvalid_0's rmse: 6.65615\n",
      "[600]\tvalid_0's rmse: 6.65345\n",
      "[700]\tvalid_0's rmse: 6.64797\n",
      "[800]\tvalid_0's rmse: 6.64747\n",
      "[900]\tvalid_0's rmse: 6.64153\n",
      "[1000]\tvalid_0's rmse: 6.63774\n",
      "[1100]\tvalid_0's rmse: 6.63692\n",
      "[1200]\tvalid_0's rmse: 6.63529\n",
      "[1300]\tvalid_0's rmse: 6.63417\n",
      "[1400]\tvalid_0's rmse: 6.63368\n",
      "[1500]\tvalid_0's rmse: 6.63288\n",
      "[1600]\tvalid_0's rmse: 6.63248\n",
      "[1700]\tvalid_0's rmse: 6.6311\n",
      "[1800]\tvalid_0's rmse: 6.63017\n",
      "[1900]\tvalid_0's rmse: 6.62958\n",
      "[2000]\tvalid_0's rmse: 6.62939\n",
      "[2100]\tvalid_0's rmse: 6.62873\n",
      "[2200]\tvalid_0's rmse: 6.62844\n",
      "[2300]\tvalid_0's rmse: 6.62789\n",
      "[2400]\tvalid_0's rmse: 6.62742\n",
      "[2500]\tvalid_0's rmse: 6.62698\n",
      "[2600]\tvalid_0's rmse: 6.62667\n",
      "[2700]\tvalid_0's rmse: 6.62633\n",
      "[2800]\tvalid_0's rmse: 6.62632\n",
      "[2900]\tvalid_0's rmse: 6.62612\n",
      "[3000]\tvalid_0's rmse: 6.62609\n",
      "[3100]\tvalid_0's rmse: 6.62611\n",
      "Early stopping, best iteration is:\n",
      "[2986]\tvalid_0's rmse: 6.62599\n",
      "[fold 0] training target 18 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 3.82115\n",
      "[200]\tvalid_0's rmse: 3.79702\n",
      "[300]\tvalid_0's rmse: 3.7966\n",
      "[400]\tvalid_0's rmse: 3.79533\n",
      "[500]\tvalid_0's rmse: 3.79816\n",
      "Early stopping, best iteration is:\n",
      "[377]\tvalid_0's rmse: 3.79255\n",
      "[fold 0] training target 19 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.94531\n",
      "[200]\tvalid_0's rmse: 7.6549\n",
      "[300]\tvalid_0's rmse: 7.59659\n",
      "[400]\tvalid_0's rmse: 7.58404\n",
      "[500]\tvalid_0's rmse: 7.57959\n",
      "[600]\tvalid_0's rmse: 7.57902\n",
      "[700]\tvalid_0's rmse: 7.57977\n",
      "Early stopping, best iteration is:\n",
      "[591]\tvalid_0's rmse: 7.57821\n",
      "[fold 0] training target 20 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 5.37562\n",
      "[200]\tvalid_0's rmse: 5.01843\n",
      "[300]\tvalid_0's rmse: 4.93682\n",
      "[400]\tvalid_0's rmse: 4.9168\n",
      "[500]\tvalid_0's rmse: 4.90993\n",
      "[600]\tvalid_0's rmse: 4.90421\n",
      "[700]\tvalid_0's rmse: 4.90121\n",
      "[800]\tvalid_0's rmse: 4.90025\n",
      "[900]\tvalid_0's rmse: 4.90257\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's rmse: 4.89877\n",
      "[fold 0] training target 21 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.18912\n",
      "[200]\tvalid_0's rmse: 6.64397\n",
      "[300]\tvalid_0's rmse: 6.52794\n",
      "[400]\tvalid_0's rmse: 6.50115\n",
      "[500]\tvalid_0's rmse: 6.50481\n",
      "Early stopping, best iteration is:\n",
      "[390]\tvalid_0's rmse: 6.49737\n",
      "[fold 0] training target 22 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.96452\n",
      "[200]\tvalid_0's rmse: 7.48544\n",
      "[300]\tvalid_0's rmse: 7.36761\n",
      "[400]\tvalid_0's rmse: 7.35108\n",
      "[500]\tvalid_0's rmse: 7.33924\n",
      "[600]\tvalid_0's rmse: 7.34188\n",
      "[700]\tvalid_0's rmse: 7.34342\n",
      "Early stopping, best iteration is:\n",
      "[533]\tvalid_0's rmse: 7.33268\n",
      "[fold 0] training target 23 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 3.45868\n",
      "[200]\tvalid_0's rmse: 3.09553\n",
      "[300]\tvalid_0's rmse: 3.04571\n",
      "[400]\tvalid_0's rmse: 3.04036\n",
      "[500]\tvalid_0's rmse: 3.0405\n",
      "Early stopping, best iteration is:\n",
      "[380]\tvalid_0's rmse: 3.03919\n",
      "[fold 0] training target 24 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 6.73412\n",
      "[200]\tvalid_0's rmse: 6.60743\n",
      "[300]\tvalid_0's rmse: 6.56721\n",
      "[400]\tvalid_0's rmse: 6.5639\n",
      "[500]\tvalid_0's rmse: 6.55506\n",
      "[600]\tvalid_0's rmse: 6.55391\n",
      "[700]\tvalid_0's rmse: 6.54776\n",
      "[800]\tvalid_0's rmse: 6.54397\n",
      "[900]\tvalid_0's rmse: 6.54705\n",
      "Early stopping, best iteration is:\n",
      "[793]\tvalid_0's rmse: 6.54343\n",
      "[fold 0] training target 25 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.69794\n",
      "[200]\tvalid_0's rmse: 7.62567\n",
      "[300]\tvalid_0's rmse: 7.62649\n",
      "[400]\tvalid_0's rmse: 7.62428\n",
      "Early stopping, best iteration is:\n",
      "[217]\tvalid_0's rmse: 7.61639\n",
      "[fold 0] training target 26 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.07128\n",
      "[200]\tvalid_0's rmse: 6.96144\n",
      "[300]\tvalid_0's rmse: 6.95264\n",
      "[400]\tvalid_0's rmse: 6.9489\n",
      "Early stopping, best iteration is:\n",
      "[264]\tvalid_0's rmse: 6.94284\n",
      "[fold 0] training target 27 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.56929\n",
      "[200]\tvalid_0's rmse: 6.87766\n",
      "[300]\tvalid_0's rmse: 6.68616\n",
      "[400]\tvalid_0's rmse: 6.61933\n",
      "[500]\tvalid_0's rmse: 6.60458\n",
      "[600]\tvalid_0's rmse: 6.58419\n",
      "[700]\tvalid_0's rmse: 6.57301\n",
      "[800]\tvalid_0's rmse: 6.56875\n",
      "[900]\tvalid_0's rmse: 6.5708\n",
      "[1000]\tvalid_0's rmse: 6.56724\n",
      "[1100]\tvalid_0's rmse: 6.56629\n",
      "[1200]\tvalid_0's rmse: 6.56457\n",
      "[1300]\tvalid_0's rmse: 6.56381\n",
      "[1400]\tvalid_0's rmse: 6.56346\n",
      "Early stopping, best iteration is:\n",
      "[1244]\tvalid_0's rmse: 6.56277\n",
      "[fold 0] training target 28 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 6.39288\n",
      "[200]\tvalid_0's rmse: 6.02623\n",
      "[300]\tvalid_0's rmse: 5.9809\n",
      "[400]\tvalid_0's rmse: 5.9762\n",
      "[500]\tvalid_0's rmse: 5.98159\n",
      "Early stopping, best iteration is:\n",
      "[355]\tvalid_0's rmse: 5.97081\n",
      "[fold 0] training target 29 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.19677\n",
      "[200]\tvalid_0's rmse: 6.9668\n",
      "[300]\tvalid_0's rmse: 6.9012\n",
      "[400]\tvalid_0's rmse: 6.87431\n",
      "[500]\tvalid_0's rmse: 6.86847\n",
      "[600]\tvalid_0's rmse: 6.86248\n",
      "[700]\tvalid_0's rmse: 6.86296\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's rmse: 6.85991\n",
      "[fold 0] training target 30 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 4.92629\n",
      "[200]\tvalid_0's rmse: 4.87546\n",
      "[300]\tvalid_0's rmse: 4.85179\n",
      "[400]\tvalid_0's rmse: 4.84195\n",
      "[500]\tvalid_0's rmse: 4.83977\n",
      "[600]\tvalid_0's rmse: 4.8378\n",
      "Early stopping, best iteration is:\n",
      "[442]\tvalid_0's rmse: 4.83433\n",
      "[fold 0] training target 31 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 3.77155\n",
      "[200]\tvalid_0's rmse: 3.24512\n",
      "[300]\tvalid_0's rmse: 3.17213\n",
      "[400]\tvalid_0's rmse: 3.16963\n",
      "[500]\tvalid_0's rmse: 3.16886\n",
      "[600]\tvalid_0's rmse: 3.16558\n",
      "[700]\tvalid_0's rmse: 3.1637\n",
      "[800]\tvalid_0's rmse: 3.16368\n",
      "[900]\tvalid_0's rmse: 3.1637\n",
      "[1000]\tvalid_0's rmse: 3.16391\n",
      "Early stopping, best iteration is:\n",
      "[823]\tvalid_0's rmse: 3.16311\n",
      "[fold 0] training target 32 on meta features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 2.78094\n",
      "[200]\tvalid_0's rmse: 2.68952\n",
      "[300]\tvalid_0's rmse: 2.68142\n",
      "[400]\tvalid_0's rmse: 2.68159\n",
      "[500]\tvalid_0's rmse: 2.68727\n",
      "Early stopping, best iteration is:\n",
      "[367]\tvalid_0's rmse: 2.68003\n",
      "[fold 0] training target 33 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 7.71484\n",
      "[200]\tvalid_0's rmse: 7.70002\n",
      "[300]\tvalid_0's rmse: 7.7031\n",
      "[400]\tvalid_0's rmse: 7.7131\n",
      "Early stopping, best iteration is:\n",
      "[226]\tvalid_0's rmse: 7.69337\n",
      "[fold 0] training target 34 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 5.67512\n",
      "[200]\tvalid_0's rmse: 5.62672\n",
      "[300]\tvalid_0's rmse: 5.61594\n",
      "[400]\tvalid_0's rmse: 5.61775\n",
      "[500]\tvalid_0's rmse: 5.62329\n",
      "Early stopping, best iteration is:\n",
      "[346]\tvalid_0's rmse: 5.61175\n",
      "[fold 0] training target 35 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 3.35667\n",
      "[200]\tvalid_0's rmse: 3.27675\n",
      "[300]\tvalid_0's rmse: 3.26652\n",
      "[400]\tvalid_0's rmse: 3.26005\n",
      "[500]\tvalid_0's rmse: 3.26023\n",
      "[600]\tvalid_0's rmse: 3.26551\n",
      "Early stopping, best iteration is:\n",
      "[475]\tvalid_0's rmse: 3.25821\n",
      "✅ Saved fold 0 meta‐model → output_folder/rank-spot/realign/no_pretrain/3_encoder/filtered_directly_rank/k-fold/realign_all/Macenko_masked/fold0/meta_model.pkl\n",
      "⏭️ Skipping fold 1\n",
      "⏭️ Skipping fold 2\n",
      "⏭️ Skipping fold 3\n",
      "⏭️ Skipping fold 4\n",
      "⏭️ Skipping fold 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import rankdata\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.operate_model import predict\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from python_scripts.pretrain_model import PretrainedEncoderRegressor\n",
    "# ---------------- Settings ----------------\n",
    "trained_oof_model_folder = 'output_folder/rank-spot/realign/no_pretrain/3_encoder/filtered_directly_rank/k-fold/realign_all/Macenko_masked/'\n",
    "n_folds    = len([d for d in os.listdir(trained_oof_model_folder) if d.startswith('fold')])\n",
    "n_samples  = len(full_dataset)\n",
    "C          = 35\n",
    "BATCH_SIZE = 64\n",
    "start_fold = 0\n",
    "\n",
    "tile_dim = 128\n",
    "center_dim = 128\n",
    "neighbor_dim = 128\n",
    "fusion_dim = tile_dim + center_dim + neighbor_dim\n",
    "\n",
    "pretrained_ae_name = 'AE_Center_noaug'\n",
    "pretrained_ae_path = f\"AE_model/128/{pretrained_ae_name}/best.pt\"\n",
    "ae_type = 'center'\n",
    "\n",
    "# Ground truth label (全 dataset)\n",
    "y_true = np.vstack([ full_dataset[i]['label'].cpu().numpy() for i in range(n_samples) ])\n",
    "\n",
    "# Build CV splitter (must match first stage splits)\n",
    "logo = LeaveOneGroupOut()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "lgb_base = lgb.LGBMRegressor(\n",
    "    objective='l2',\n",
    "    metric='rmse',\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=20000,\n",
    "    max_depth=15,\n",
    "    num_leaves=127,\n",
    "    colsample_bytree=0.7619407413363416,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    min_data_in_leaf=20,\n",
    "    reg_alpha=0.7480401395491829,\n",
    "    reg_lambda=0.2589860348178542,\n",
    "    verbosity=-1\n",
    "    )\n",
    "\n",
    "\n",
    "slide_idx = np.array(grouped_data['slide_idx'])   # shape (N,)\n",
    "\n",
    "\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "    logo.split(X=np.zeros(n_samples), y=None, groups=slide_idx)):\n",
    "\n",
    "    if fold_id > start_fold:\n",
    "        print(f\"⏭️ Skipping fold {fold_id}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🚀 Starting fold {fold_id}...\")\n",
    "    ckpt_path = os.path.join(trained_oof_model_folder, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "\n",
    "    # === Load model and predict OOF ===\n",
    "    net = VisionMLP_MultiTask(tile_dim=tile_dim, subtile_dim=center_dim, output_dim=C)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net = net.to(device).eval()\n",
    "\n",
    "    val_ds = Subset(full_dataset, va_idx)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    preds, latents = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            tiles = batch['tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "            center = subtiles[:, 4].contiguous()\n",
    "\n",
    "            f_c = net.encoder_center(center)\n",
    "            f_n = net.encoder_subtile(subtiles)\n",
    "            f_t = net.encoder_tile(tiles)\n",
    "            fuse = torch.cat([f_c, f_n, f_t], dim=1).contiguous()\n",
    "            output = net.decoder(fuse)\n",
    "\n",
    "            preds.append(output.cpu())\n",
    "            latents.append(fuse.cpu())\n",
    "\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    latents = torch.cat(latents, dim=0).numpy()\n",
    "\n",
    "    # === AE model reconstruction loss ===\n",
    "    recon_model = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=pretrained_ae_path,\n",
    "        ae_type=ae_type,\n",
    "        tile_dim=tile_dim,\n",
    "        center_dim=center_dim,\n",
    "        neighbor_dim=neighbor_dim,\n",
    "        output_dim=C,\n",
    "        mode='reconstruction'\n",
    "    ).to(device)\n",
    "\n",
    "    meta, name = generate_meta_features(\n",
    "        dataset = val_ds,\n",
    "        oof_preds = preds,\n",
    "        model_for_recon = recon_model,\n",
    "        latents = latents,\n",
    "        device = device,\n",
    "        ae_type = ae_type,\n",
    "    )\n",
    "    stats = diagnose_meta_nonfinite(meta, name)\n",
    "\n",
    "    \n",
    "    y_val      = y_true[va_idx]           # (n_val, 35)\n",
    "    resid_val  = y_val - preds            # (n_val, 35)\n",
    "    # 2) 再對這個 fold 的 meta 做 train/val 切分\n",
    "    X_train_tab, X_val_tab, y_train_tab, y_val_tab = train_test_split(\n",
    "        meta, resid_val, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # 3) train MultiOutputRegressor with early stopping\n",
    "    meta_model = MultiOutputRegressor(lgb_base)\n",
    "    meta_model.estimators_ = []\n",
    "\n",
    "    for i in range(y_train_tab.shape[1]):\n",
    "        cell_num = i +1\n",
    "        print(f\"[fold {fold_id}] training target {cell_num} on meta features …\")\n",
    "        model = lgb.LGBMRegressor(**lgb_base.get_params())\n",
    "        model.fit(\n",
    "            X_train_tab, y_train_tab[:, i], # 这里 y_train_tab 是 residual\n",
    "            eval_set=[(X_val_tab, y_val_tab[:, i])], \n",
    "            callbacks=[\n",
    "                early_stopping(stopping_rounds=200),\n",
    "                log_evaluation(period=100)\n",
    "            ]\n",
    "        )\n",
    "        meta_model.estimators_.append(model)\n",
    "\n",
    "    # 4) 存下這個 fold 的 meta model\n",
    "    save_path = os.path.join(trained_oof_model_folder, f\"fold{fold_id}\",\"meta_model.pkl\")\n",
    "    joblib.dump(meta_model, save_path)\n",
    "    print(f\"✅ Saved fold {fold_id} meta‐model → {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting fold 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_3581/3837923258.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting fold 1...\n",
      "\n",
      "🚀 Starting fold 2...\n",
      "\n",
      "🚀 Starting fold 3...\n",
      "\n",
      "🚀 Starting fold 4...\n",
      "\n",
      "🚀 Starting fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:310: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "Computing AE recon loss: 100%|██████████| 131/131 [00:12<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae-recon-loss -> cols:    1, names:    1 OK\n",
      "trained-latents -> cols:  384, names:  384 OK\n",
      "subtile4     -> cols:   12, names:   12 OK\n",
      "exsubtiles   -> cols:   12, names:   12 OK\n",
      "tile         -> cols:   12, names:   12 OK\n",
      "contrast     -> cols:    3, names:    3 OK\n",
      "wavelet-tile -> cols:  280, names:  280 OK\n",
      "sobel-tile   -> cols:   40, names:   40 OK\n",
      "hsv-tile     -> cols:  120, names:  120 OK\n",
      "he-tile      -> cols:   80, names:   80 OK\n",
      "oof          -> cols:   35, names:   35 OK\n",
      "adj          -> cols:   34, names:   34 OK\n",
      "last         -> cols:  136, names:  136 OK\n",
      "mad          -> cols:    1, names:    1 OK\n",
      "skewness     -> cols:    2, names:    2 OK\n",
      "autocorr     -> cols:    5, names:    5 OK\n",
      "diff2        -> cols:   33, names:   33 OK\n",
      "diff3        -> cols:   32, names:   32 OK\n",
      "✅ Generated meta-features with shape: (8348, 1222)\n",
      "→ Training meta target 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 5.50746\n",
      "[200]\tvalid_0's rmse: 5.07442\n",
      "[300]\tvalid_0's rmse: 5.005\n",
      "[400]\tvalid_0's rmse: 4.98823\n",
      "[500]\tvalid_0's rmse: 4.98602\n",
      "[600]\tvalid_0's rmse: 4.98674\n",
      "Early stopping, best iteration is:\n",
      "[497]\tvalid_0's rmse: 4.98523\n",
      "→ Training meta target 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 2.55075\n",
      "[200]\tvalid_0's rmse: 2.31746\n",
      "[300]\tvalid_0's rmse: 2.26258\n",
      "[400]\tvalid_0's rmse: 2.24507\n",
      "[500]\tvalid_0's rmse: 2.23736\n",
      "[600]\tvalid_0's rmse: 2.23362\n",
      "[700]\tvalid_0's rmse: 2.23128\n",
      "[800]\tvalid_0's rmse: 2.22895\n",
      "[900]\tvalid_0's rmse: 2.2275\n",
      "[1000]\tvalid_0's rmse: 2.22718\n",
      "[1100]\tvalid_0's rmse: 2.2263\n",
      "[1200]\tvalid_0's rmse: 2.22598\n",
      "[1300]\tvalid_0's rmse: 2.22539\n",
      "[1400]\tvalid_0's rmse: 2.22502\n",
      "[1500]\tvalid_0's rmse: 2.22507\n",
      "[1600]\tvalid_0's rmse: 2.22478\n",
      "[1700]\tvalid_0's rmse: 2.22475\n",
      "[1800]\tvalid_0's rmse: 2.22458\n",
      "[1900]\tvalid_0's rmse: 2.22461\n",
      "[2000]\tvalid_0's rmse: 2.22459\n",
      "[2100]\tvalid_0's rmse: 2.22462\n",
      "Early stopping, best iteration is:\n",
      "[1969]\tvalid_0's rmse: 2.22454\n",
      "→ Training meta target 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 3.82044\n",
      "[200]\tvalid_0's rmse: 3.39814\n",
      "[300]\tvalid_0's rmse: 3.32805\n",
      "[400]\tvalid_0's rmse: 3.31374\n",
      "[500]\tvalid_0's rmse: 3.31101\n",
      "[600]\tvalid_0's rmse: 3.30985\n",
      "[700]\tvalid_0's rmse: 3.31031\n",
      "Early stopping, best iteration is:\n",
      "[530]\tvalid_0's rmse: 3.30863\n",
      "→ Training meta target 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.60011\n",
      "[200]\tvalid_0's rmse: 6.72922\n",
      "[300]\tvalid_0's rmse: 6.58872\n",
      "[400]\tvalid_0's rmse: 6.56233\n",
      "[500]\tvalid_0's rmse: 6.55305\n",
      "[600]\tvalid_0's rmse: 6.55016\n",
      "[700]\tvalid_0's rmse: 6.54814\n",
      "[800]\tvalid_0's rmse: 6.54735\n",
      "[900]\tvalid_0's rmse: 6.54883\n",
      "Early stopping, best iteration is:\n",
      "[785]\tvalid_0's rmse: 6.54643\n",
      "→ Training meta target 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.19158\n",
      "[200]\tvalid_0's rmse: 6.40524\n",
      "[300]\tvalid_0's rmse: 6.23925\n",
      "[400]\tvalid_0's rmse: 6.19149\n",
      "[500]\tvalid_0's rmse: 6.16662\n",
      "[600]\tvalid_0's rmse: 6.14861\n",
      "[700]\tvalid_0's rmse: 6.14156\n",
      "[800]\tvalid_0's rmse: 6.13665\n",
      "[900]\tvalid_0's rmse: 6.13086\n",
      "[1000]\tvalid_0's rmse: 6.12728\n",
      "[1100]\tvalid_0's rmse: 6.12491\n",
      "[1200]\tvalid_0's rmse: 6.12331\n",
      "[1300]\tvalid_0's rmse: 6.12269\n",
      "[1400]\tvalid_0's rmse: 6.12035\n",
      "[1500]\tvalid_0's rmse: 6.11893\n",
      "[1600]\tvalid_0's rmse: 6.1187\n",
      "[1700]\tvalid_0's rmse: 6.11818\n",
      "[1800]\tvalid_0's rmse: 6.11764\n",
      "[1900]\tvalid_0's rmse: 6.11705\n",
      "[2000]\tvalid_0's rmse: 6.11665\n",
      "[2100]\tvalid_0's rmse: 6.11659\n",
      "[2200]\tvalid_0's rmse: 6.11649\n",
      "[2300]\tvalid_0's rmse: 6.11623\n",
      "[2400]\tvalid_0's rmse: 6.11615\n",
      "[2500]\tvalid_0's rmse: 6.11623\n",
      "Early stopping, best iteration is:\n",
      "[2365]\tvalid_0's rmse: 6.116\n",
      "→ Training meta target 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.90384\n",
      "[200]\tvalid_0's rmse: 7.11565\n",
      "[300]\tvalid_0's rmse: 6.90921\n",
      "[400]\tvalid_0's rmse: 6.84304\n",
      "[500]\tvalid_0's rmse: 6.81655\n",
      "[600]\tvalid_0's rmse: 6.80287\n",
      "[700]\tvalid_0's rmse: 6.78888\n",
      "[800]\tvalid_0's rmse: 6.77809\n",
      "[900]\tvalid_0's rmse: 6.77341\n",
      "[1000]\tvalid_0's rmse: 6.76781\n",
      "[1100]\tvalid_0's rmse: 6.76398\n",
      "[1200]\tvalid_0's rmse: 6.76153\n",
      "[1300]\tvalid_0's rmse: 6.75896\n",
      "[1400]\tvalid_0's rmse: 6.75748\n",
      "[1500]\tvalid_0's rmse: 6.75605\n",
      "[1600]\tvalid_0's rmse: 6.75571\n",
      "[1700]\tvalid_0's rmse: 6.75462\n",
      "[1800]\tvalid_0's rmse: 6.75405\n",
      "[1900]\tvalid_0's rmse: 6.75359\n",
      "[2000]\tvalid_0's rmse: 6.75299\n",
      "[2100]\tvalid_0's rmse: 6.75269\n",
      "[2200]\tvalid_0's rmse: 6.75252\n",
      "[2300]\tvalid_0's rmse: 6.75228\n",
      "[2400]\tvalid_0's rmse: 6.75206\n",
      "[2500]\tvalid_0's rmse: 6.75201\n",
      "[2600]\tvalid_0's rmse: 6.75194\n",
      "[2700]\tvalid_0's rmse: 6.75192\n",
      "Early stopping, best iteration is:\n",
      "[2570]\tvalid_0's rmse: 6.7519\n",
      "→ Training meta target 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 3.63563\n",
      "[200]\tvalid_0's rmse: 3.44846\n",
      "[300]\tvalid_0's rmse: 3.40135\n",
      "[400]\tvalid_0's rmse: 3.38574\n",
      "[500]\tvalid_0's rmse: 3.37626\n",
      "[600]\tvalid_0's rmse: 3.3715\n",
      "[700]\tvalid_0's rmse: 3.37043\n",
      "[800]\tvalid_0's rmse: 3.36964\n",
      "[900]\tvalid_0's rmse: 3.3686\n",
      "[1000]\tvalid_0's rmse: 3.36792\n",
      "[1100]\tvalid_0's rmse: 3.36736\n",
      "[1200]\tvalid_0's rmse: 3.3659\n",
      "[1300]\tvalid_0's rmse: 3.36618\n",
      "[1400]\tvalid_0's rmse: 3.36573\n",
      "[1500]\tvalid_0's rmse: 3.36588\n",
      "[1600]\tvalid_0's rmse: 3.36596\n",
      "Early stopping, best iteration is:\n",
      "[1484]\tvalid_0's rmse: 3.36566\n",
      "→ Training meta target 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 5.65146\n",
      "[200]\tvalid_0's rmse: 5.6042\n",
      "[300]\tvalid_0's rmse: 5.60367\n",
      "[400]\tvalid_0's rmse: 5.60163\n",
      "Early stopping, best iteration is:\n",
      "[245]\tvalid_0's rmse: 5.59687\n",
      "→ Training meta target 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 8.46418\n",
      "[200]\tvalid_0's rmse: 7.59008\n",
      "[300]\tvalid_0's rmse: 7.37897\n",
      "[400]\tvalid_0's rmse: 7.30516\n",
      "[500]\tvalid_0's rmse: 7.27639\n",
      "[600]\tvalid_0's rmse: 7.24647\n",
      "[700]\tvalid_0's rmse: 7.22758\n",
      "[800]\tvalid_0's rmse: 7.21381\n",
      "[900]\tvalid_0's rmse: 7.20366\n",
      "[1000]\tvalid_0's rmse: 7.19609\n",
      "[1100]\tvalid_0's rmse: 7.191\n",
      "[1200]\tvalid_0's rmse: 7.18893\n",
      "[1300]\tvalid_0's rmse: 7.18619\n",
      "[1400]\tvalid_0's rmse: 7.18376\n",
      "[1500]\tvalid_0's rmse: 7.18085\n",
      "[1600]\tvalid_0's rmse: 7.17966\n",
      "[1700]\tvalid_0's rmse: 7.17812\n",
      "[1800]\tvalid_0's rmse: 7.17721\n",
      "[1900]\tvalid_0's rmse: 7.1763\n",
      "[2000]\tvalid_0's rmse: 7.17548\n",
      "[2100]\tvalid_0's rmse: 7.17492\n",
      "[2200]\tvalid_0's rmse: 7.17471\n",
      "[2300]\tvalid_0's rmse: 7.17435\n",
      "[2400]\tvalid_0's rmse: 7.17383\n",
      "[2500]\tvalid_0's rmse: 7.17359\n",
      "[2600]\tvalid_0's rmse: 7.17347\n",
      "[2700]\tvalid_0's rmse: 7.17331\n",
      "[2800]\tvalid_0's rmse: 7.17327\n",
      "[2900]\tvalid_0's rmse: 7.17312\n",
      "[3000]\tvalid_0's rmse: 7.17303\n",
      "[3100]\tvalid_0's rmse: 7.17295\n",
      "[3200]\tvalid_0's rmse: 7.17292\n",
      "[3300]\tvalid_0's rmse: 7.17285\n",
      "[3400]\tvalid_0's rmse: 7.17277\n",
      "[3500]\tvalid_0's rmse: 7.17274\n",
      "[3600]\tvalid_0's rmse: 7.17273\n",
      "[3700]\tvalid_0's rmse: 7.1727\n",
      "[3800]\tvalid_0's rmse: 7.17269\n",
      "[3900]\tvalid_0's rmse: 7.17268\n",
      "[4000]\tvalid_0's rmse: 7.17268\n",
      "[4100]\tvalid_0's rmse: 7.17267\n",
      "[4200]\tvalid_0's rmse: 7.17266\n",
      "[4300]\tvalid_0's rmse: 7.17267\n",
      "Early stopping, best iteration is:\n",
      "[4181]\tvalid_0's rmse: 7.17265\n",
      "→ Training meta target 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 6.73733\n",
      "[200]\tvalid_0's rmse: 5.97082\n",
      "[300]\tvalid_0's rmse: 5.81598\n",
      "[400]\tvalid_0's rmse: 5.76691\n",
      "[500]\tvalid_0's rmse: 5.75109\n",
      "[600]\tvalid_0's rmse: 5.74102\n",
      "[700]\tvalid_0's rmse: 5.73711\n",
      "[800]\tvalid_0's rmse: 5.73332\n",
      "[900]\tvalid_0's rmse: 5.73099\n",
      "[1000]\tvalid_0's rmse: 5.72786\n",
      "[1100]\tvalid_0's rmse: 5.72747\n",
      "[1200]\tvalid_0's rmse: 5.72728\n",
      "[1300]\tvalid_0's rmse: 5.7265\n",
      "[1400]\tvalid_0's rmse: 5.72606\n",
      "[1500]\tvalid_0's rmse: 5.7252\n",
      "[1600]\tvalid_0's rmse: 5.725\n",
      "[1700]\tvalid_0's rmse: 5.72544\n",
      "Early stopping, best iteration is:\n",
      "[1588]\tvalid_0's rmse: 5.72485\n",
      "→ Training meta target 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.38368\n",
      "[200]\tvalid_0's rmse: 6.79837\n",
      "[300]\tvalid_0's rmse: 6.67105\n",
      "[400]\tvalid_0's rmse: 6.62957\n",
      "[500]\tvalid_0's rmse: 6.611\n",
      "[600]\tvalid_0's rmse: 6.59954\n",
      "[700]\tvalid_0's rmse: 6.5931\n",
      "[800]\tvalid_0's rmse: 6.58768\n",
      "[900]\tvalid_0's rmse: 6.58481\n",
      "[1000]\tvalid_0's rmse: 6.58267\n",
      "[1100]\tvalid_0's rmse: 6.58097\n",
      "[1200]\tvalid_0's rmse: 6.57888\n",
      "[1300]\tvalid_0's rmse: 6.57706\n",
      "[1400]\tvalid_0's rmse: 6.57691\n",
      "[1500]\tvalid_0's rmse: 6.57724\n",
      "[1600]\tvalid_0's rmse: 6.57624\n",
      "[1700]\tvalid_0's rmse: 6.57572\n",
      "[1800]\tvalid_0's rmse: 6.5759\n",
      "Early stopping, best iteration is:\n",
      "[1665]\tvalid_0's rmse: 6.57558\n",
      "→ Training meta target 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 4.10887\n",
      "[200]\tvalid_0's rmse: 3.61534\n",
      "[300]\tvalid_0's rmse: 3.50997\n",
      "[400]\tvalid_0's rmse: 3.48371\n",
      "[500]\tvalid_0's rmse: 3.47558\n",
      "[600]\tvalid_0's rmse: 3.47114\n",
      "[700]\tvalid_0's rmse: 3.46763\n",
      "[800]\tvalid_0's rmse: 3.46557\n",
      "[900]\tvalid_0's rmse: 3.46418\n",
      "[1000]\tvalid_0's rmse: 3.46338\n",
      "[1100]\tvalid_0's rmse: 3.46232\n",
      "[1200]\tvalid_0's rmse: 3.46148\n",
      "[1300]\tvalid_0's rmse: 3.46094\n",
      "[1400]\tvalid_0's rmse: 3.46074\n",
      "[1500]\tvalid_0's rmse: 3.46027\n",
      "[1600]\tvalid_0's rmse: 3.46045\n",
      "Early stopping, best iteration is:\n",
      "[1446]\tvalid_0's rmse: 3.46021\n",
      "→ Training meta target 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 6.11447\n",
      "[200]\tvalid_0's rmse: 6.08253\n",
      "[300]\tvalid_0's rmse: 6.08878\n",
      "[400]\tvalid_0's rmse: 6.09904\n",
      "Early stopping, best iteration is:\n",
      "[236]\tvalid_0's rmse: 6.07878\n",
      "→ Training meta target 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 5.33481\n",
      "[200]\tvalid_0's rmse: 5.1257\n",
      "[300]\tvalid_0's rmse: 5.08907\n",
      "[400]\tvalid_0's rmse: 5.08301\n",
      "[500]\tvalid_0's rmse: 5.07972\n",
      "[600]\tvalid_0's rmse: 5.07898\n",
      "[700]\tvalid_0's rmse: 5.07984\n",
      "[800]\tvalid_0's rmse: 5.07705\n",
      "[900]\tvalid_0's rmse: 5.07581\n",
      "[1000]\tvalid_0's rmse: 5.07536\n",
      "[1100]\tvalid_0's rmse: 5.07576\n",
      "[1200]\tvalid_0's rmse: 5.07556\n",
      "Early stopping, best iteration is:\n",
      "[1036]\tvalid_0's rmse: 5.07486\n",
      "→ Training meta target 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 6.08815\n",
      "[200]\tvalid_0's rmse: 5.39882\n",
      "[300]\tvalid_0's rmse: 5.24138\n",
      "[400]\tvalid_0's rmse: 5.19868\n",
      "[500]\tvalid_0's rmse: 5.17811\n",
      "[600]\tvalid_0's rmse: 5.16223\n",
      "[700]\tvalid_0's rmse: 5.15694\n",
      "[800]\tvalid_0's rmse: 5.15139\n",
      "[900]\tvalid_0's rmse: 5.1452\n",
      "[1000]\tvalid_0's rmse: 5.1422\n",
      "[1100]\tvalid_0's rmse: 5.13995\n",
      "[1200]\tvalid_0's rmse: 5.13882\n",
      "[1300]\tvalid_0's rmse: 5.13701\n",
      "[1400]\tvalid_0's rmse: 5.13592\n",
      "[1500]\tvalid_0's rmse: 5.13531\n",
      "[1600]\tvalid_0's rmse: 5.13479\n",
      "[1700]\tvalid_0's rmse: 5.13421\n",
      "[1800]\tvalid_0's rmse: 5.13373\n",
      "[1900]\tvalid_0's rmse: 5.13339\n",
      "[2000]\tvalid_0's rmse: 5.13322\n",
      "[2100]\tvalid_0's rmse: 5.13319\n",
      "[2200]\tvalid_0's rmse: 5.13319\n",
      "[2300]\tvalid_0's rmse: 5.13295\n",
      "[2400]\tvalid_0's rmse: 5.13283\n",
      "[2500]\tvalid_0's rmse: 5.13271\n",
      "[2600]\tvalid_0's rmse: 5.13268\n",
      "[2700]\tvalid_0's rmse: 5.13266\n",
      "[2800]\tvalid_0's rmse: 5.1327\n",
      "[2900]\tvalid_0's rmse: 5.13255\n",
      "[3000]\tvalid_0's rmse: 5.13254\n",
      "[3100]\tvalid_0's rmse: 5.13255\n",
      "[3200]\tvalid_0's rmse: 5.13253\n",
      "[3300]\tvalid_0's rmse: 5.13252\n",
      "[3400]\tvalid_0's rmse: 5.13246\n",
      "[3500]\tvalid_0's rmse: 5.13244\n",
      "[3600]\tvalid_0's rmse: 5.13244\n",
      "[3700]\tvalid_0's rmse: 5.13243\n",
      "[3800]\tvalid_0's rmse: 5.13244\n",
      "[3900]\tvalid_0's rmse: 5.13244\n",
      "Early stopping, best iteration is:\n",
      "[3757]\tvalid_0's rmse: 5.13242\n",
      "→ Training meta target 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 6.13883\n",
      "[200]\tvalid_0's rmse: 5.72808\n",
      "[300]\tvalid_0's rmse: 5.63963\n",
      "[400]\tvalid_0's rmse: 5.59262\n",
      "[500]\tvalid_0's rmse: 5.57925\n",
      "[600]\tvalid_0's rmse: 5.56641\n",
      "[700]\tvalid_0's rmse: 5.55677\n",
      "[800]\tvalid_0's rmse: 5.5502\n",
      "[900]\tvalid_0's rmse: 5.54376\n",
      "[1000]\tvalid_0's rmse: 5.53932\n",
      "[1100]\tvalid_0's rmse: 5.53574\n",
      "[1200]\tvalid_0's rmse: 5.53252\n",
      "[1300]\tvalid_0's rmse: 5.53222\n",
      "[1400]\tvalid_0's rmse: 5.52991\n",
      "[1500]\tvalid_0's rmse: 5.52897\n",
      "[1600]\tvalid_0's rmse: 5.52869\n",
      "[1700]\tvalid_0's rmse: 5.52805\n",
      "[1800]\tvalid_0's rmse: 5.52785\n",
      "[1900]\tvalid_0's rmse: 5.52769\n",
      "[2000]\tvalid_0's rmse: 5.52763\n",
      "[2100]\tvalid_0's rmse: 5.52769\n",
      "Early stopping, best iteration is:\n",
      "[1947]\tvalid_0's rmse: 5.52754\n",
      "→ Training meta target 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.01094\n",
      "[200]\tvalid_0's rmse: 6.27011\n",
      "[300]\tvalid_0's rmse: 6.09859\n",
      "[400]\tvalid_0's rmse: 6.05293\n",
      "[500]\tvalid_0's rmse: 6.03535\n",
      "[600]\tvalid_0's rmse: 6.02296\n",
      "[700]\tvalid_0's rmse: 6.01706\n",
      "[800]\tvalid_0's rmse: 6.01212\n",
      "[900]\tvalid_0's rmse: 6.00843\n",
      "[1000]\tvalid_0's rmse: 6.00696\n",
      "[1100]\tvalid_0's rmse: 6.00548\n",
      "[1200]\tvalid_0's rmse: 6.00408\n",
      "[1300]\tvalid_0's rmse: 6.00328\n",
      "[1400]\tvalid_0's rmse: 6.0024\n",
      "[1500]\tvalid_0's rmse: 6.00159\n",
      "[1600]\tvalid_0's rmse: 6.00082\n",
      "[1700]\tvalid_0's rmse: 6.00052\n",
      "[1800]\tvalid_0's rmse: 6.00027\n",
      "[1900]\tvalid_0's rmse: 6.00013\n",
      "[2000]\tvalid_0's rmse: 5.99984\n",
      "[2100]\tvalid_0's rmse: 5.9998\n",
      "[2200]\tvalid_0's rmse: 5.99976\n",
      "[2300]\tvalid_0's rmse: 5.99969\n",
      "[2400]\tvalid_0's rmse: 5.99959\n",
      "[2500]\tvalid_0's rmse: 5.99954\n",
      "[2600]\tvalid_0's rmse: 5.99944\n",
      "[2700]\tvalid_0's rmse: 5.9994\n",
      "[2800]\tvalid_0's rmse: 5.99927\n",
      "[2900]\tvalid_0's rmse: 5.99926\n",
      "[3000]\tvalid_0's rmse: 5.99925\n",
      "[3100]\tvalid_0's rmse: 5.99925\n",
      "Early stopping, best iteration is:\n",
      "[2955]\tvalid_0's rmse: 5.99923\n",
      "→ Training meta target 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 3.63405\n",
      "[200]\tvalid_0's rmse: 3.20334\n",
      "[300]\tvalid_0's rmse: 3.12603\n",
      "[400]\tvalid_0's rmse: 3.11125\n",
      "[500]\tvalid_0's rmse: 3.10677\n",
      "[600]\tvalid_0's rmse: 3.10793\n",
      "Early stopping, best iteration is:\n",
      "[483]\tvalid_0's rmse: 3.10621\n",
      "→ Training meta target 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.32233\n",
      "[200]\tvalid_0's rmse: 7.09538\n",
      "[300]\tvalid_0's rmse: 7.04986\n",
      "[400]\tvalid_0's rmse: 7.03711\n",
      "[500]\tvalid_0's rmse: 7.03382\n",
      "[600]\tvalid_0's rmse: 7.02616\n",
      "[700]\tvalid_0's rmse: 7.02089\n",
      "[800]\tvalid_0's rmse: 7.02117\n",
      "[900]\tvalid_0's rmse: 7.01761\n",
      "[1000]\tvalid_0's rmse: 7.01641\n",
      "[1100]\tvalid_0's rmse: 7.01424\n",
      "[1200]\tvalid_0's rmse: 7.01392\n",
      "[1300]\tvalid_0's rmse: 7.01418\n",
      "[1400]\tvalid_0's rmse: 7.01381\n",
      "Early stopping, best iteration is:\n",
      "[1254]\tvalid_0's rmse: 7.01336\n",
      "→ Training meta target 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 4.82198\n",
      "[200]\tvalid_0's rmse: 4.6122\n",
      "[300]\tvalid_0's rmse: 4.56327\n",
      "[400]\tvalid_0's rmse: 4.55082\n",
      "[500]\tvalid_0's rmse: 4.54444\n",
      "[600]\tvalid_0's rmse: 4.54013\n",
      "[700]\tvalid_0's rmse: 4.53831\n",
      "[800]\tvalid_0's rmse: 4.53832\n",
      "[900]\tvalid_0's rmse: 4.53617\n",
      "[1000]\tvalid_0's rmse: 4.53479\n",
      "[1100]\tvalid_0's rmse: 4.53417\n",
      "[1200]\tvalid_0's rmse: 4.53276\n",
      "[1300]\tvalid_0's rmse: 4.53179\n",
      "[1400]\tvalid_0's rmse: 4.53113\n",
      "[1500]\tvalid_0's rmse: 4.53101\n",
      "[1600]\tvalid_0's rmse: 4.53086\n",
      "[1700]\tvalid_0's rmse: 4.53055\n",
      "[1800]\tvalid_0's rmse: 4.5306\n",
      "[1900]\tvalid_0's rmse: 4.53061\n",
      "Early stopping, best iteration is:\n",
      "[1727]\tvalid_0's rmse: 4.53047\n",
      "→ Training meta target 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 6.4471\n",
      "[200]\tvalid_0's rmse: 6.06781\n",
      "[300]\tvalid_0's rmse: 5.99441\n",
      "[400]\tvalid_0's rmse: 5.97144\n",
      "[500]\tvalid_0's rmse: 5.96484\n",
      "[600]\tvalid_0's rmse: 5.9628\n",
      "[700]\tvalid_0's rmse: 5.95825\n",
      "[800]\tvalid_0's rmse: 5.95441\n",
      "[900]\tvalid_0's rmse: 5.95336\n",
      "[1000]\tvalid_0's rmse: 5.95238\n",
      "[1100]\tvalid_0's rmse: 5.95289\n",
      "[1200]\tvalid_0's rmse: 5.95214\n",
      "[1300]\tvalid_0's rmse: 5.95275\n",
      "Early stopping, best iteration is:\n",
      "[1140]\tvalid_0's rmse: 5.95185\n",
      "→ Training meta target 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.52108\n",
      "[200]\tvalid_0's rmse: 7.218\n",
      "[300]\tvalid_0's rmse: 7.17954\n",
      "[400]\tvalid_0's rmse: 7.16649\n",
      "[500]\tvalid_0's rmse: 7.16342\n",
      "[600]\tvalid_0's rmse: 7.16175\n",
      "[700]\tvalid_0's rmse: 7.16241\n",
      "[800]\tvalid_0's rmse: 7.15785\n",
      "[900]\tvalid_0's rmse: 7.15881\n",
      "Early stopping, best iteration is:\n",
      "[758]\tvalid_0's rmse: 7.15768\n",
      "→ Training meta target 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 2.81773\n",
      "[200]\tvalid_0's rmse: 2.66514\n",
      "[300]\tvalid_0's rmse: 2.63081\n",
      "[400]\tvalid_0's rmse: 2.62679\n",
      "[500]\tvalid_0's rmse: 2.62175\n",
      "[600]\tvalid_0's rmse: 2.62022\n",
      "[700]\tvalid_0's rmse: 2.61923\n",
      "[800]\tvalid_0's rmse: 2.61882\n",
      "[900]\tvalid_0's rmse: 2.61878\n",
      "Early stopping, best iteration is:\n",
      "[792]\tvalid_0's rmse: 2.61845\n",
      "→ Training meta target 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 6.17624\n",
      "[200]\tvalid_0's rmse: 5.69799\n",
      "[300]\tvalid_0's rmse: 5.58812\n",
      "[400]\tvalid_0's rmse: 5.56156\n",
      "[500]\tvalid_0's rmse: 5.54956\n",
      "[600]\tvalid_0's rmse: 5.54072\n",
      "[700]\tvalid_0's rmse: 5.53535\n",
      "[800]\tvalid_0's rmse: 5.53015\n",
      "[900]\tvalid_0's rmse: 5.52836\n",
      "[1000]\tvalid_0's rmse: 5.52902\n",
      "[1100]\tvalid_0's rmse: 5.52808\n",
      "Early stopping, best iteration is:\n",
      "[918]\tvalid_0's rmse: 5.52713\n",
      "→ Training meta target 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.25433\n",
      "[200]\tvalid_0's rmse: 6.70211\n",
      "[300]\tvalid_0's rmse: 6.59334\n",
      "[400]\tvalid_0's rmse: 6.55872\n",
      "[500]\tvalid_0's rmse: 6.54457\n",
      "[600]\tvalid_0's rmse: 6.54383\n",
      "[700]\tvalid_0's rmse: 6.53973\n",
      "[800]\tvalid_0's rmse: 6.53829\n",
      "[900]\tvalid_0's rmse: 6.53446\n",
      "[1000]\tvalid_0's rmse: 6.53467\n",
      "[1100]\tvalid_0's rmse: 6.53369\n",
      "[1200]\tvalid_0's rmse: 6.53247\n",
      "[1300]\tvalid_0's rmse: 6.53145\n",
      "[1400]\tvalid_0's rmse: 6.53082\n",
      "[1500]\tvalid_0's rmse: 6.5303\n",
      "[1600]\tvalid_0's rmse: 6.52933\n",
      "[1700]\tvalid_0's rmse: 6.52883\n",
      "[1800]\tvalid_0's rmse: 6.52823\n",
      "[1900]\tvalid_0's rmse: 6.52767\n",
      "[2000]\tvalid_0's rmse: 6.52731\n",
      "[2100]\tvalid_0's rmse: 6.52703\n",
      "[2200]\tvalid_0's rmse: 6.52688\n",
      "[2300]\tvalid_0's rmse: 6.52686\n",
      "Early stopping, best iteration is:\n",
      "[2144]\tvalid_0's rmse: 6.52683\n",
      "→ Training meta target 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 6.87585\n",
      "[200]\tvalid_0's rmse: 6.64787\n",
      "[300]\tvalid_0's rmse: 6.59832\n",
      "[400]\tvalid_0's rmse: 6.58028\n",
      "[500]\tvalid_0's rmse: 6.57846\n",
      "[600]\tvalid_0's rmse: 6.58025\n",
      "Early stopping, best iteration is:\n",
      "[438]\tvalid_0's rmse: 6.57501\n",
      "→ Training meta target 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.98643\n",
      "[200]\tvalid_0's rmse: 7.01308\n",
      "[300]\tvalid_0's rmse: 6.76184\n",
      "[400]\tvalid_0's rmse: 6.68172\n",
      "[500]\tvalid_0's rmse: 6.64483\n",
      "[600]\tvalid_0's rmse: 6.62746\n",
      "[700]\tvalid_0's rmse: 6.61483\n",
      "[800]\tvalid_0's rmse: 6.60451\n",
      "[900]\tvalid_0's rmse: 6.59837\n",
      "[1000]\tvalid_0's rmse: 6.59312\n",
      "[1100]\tvalid_0's rmse: 6.58679\n",
      "[1200]\tvalid_0's rmse: 6.58336\n",
      "[1300]\tvalid_0's rmse: 6.58018\n",
      "[1400]\tvalid_0's rmse: 6.57832\n",
      "[1500]\tvalid_0's rmse: 6.57529\n",
      "[1600]\tvalid_0's rmse: 6.57396\n",
      "[1700]\tvalid_0's rmse: 6.5723\n",
      "[1800]\tvalid_0's rmse: 6.57117\n",
      "[1900]\tvalid_0's rmse: 6.57033\n",
      "[2000]\tvalid_0's rmse: 6.56971\n",
      "[2100]\tvalid_0's rmse: 6.56956\n",
      "[2200]\tvalid_0's rmse: 6.5694\n",
      "[2300]\tvalid_0's rmse: 6.56916\n",
      "[2400]\tvalid_0's rmse: 6.56915\n",
      "[2500]\tvalid_0's rmse: 6.56909\n",
      "[2600]\tvalid_0's rmse: 6.56912\n",
      "Early stopping, best iteration is:\n",
      "[2489]\tvalid_0's rmse: 6.56906\n",
      "→ Training meta target 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 5.84713\n",
      "[200]\tvalid_0's rmse: 5.61489\n",
      "[300]\tvalid_0's rmse: 5.57633\n",
      "[400]\tvalid_0's rmse: 5.57549\n",
      "[500]\tvalid_0's rmse: 5.57405\n",
      "Early stopping, best iteration is:\n",
      "[347]\tvalid_0's rmse: 5.57055\n",
      "→ Training meta target 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 6.33046\n",
      "[200]\tvalid_0's rmse: 5.9663\n",
      "[300]\tvalid_0's rmse: 5.89427\n",
      "[400]\tvalid_0's rmse: 5.87844\n",
      "[500]\tvalid_0's rmse: 5.86875\n",
      "[600]\tvalid_0's rmse: 5.86735\n",
      "[700]\tvalid_0's rmse: 5.8698\n",
      "Early stopping, best iteration is:\n",
      "[563]\tvalid_0's rmse: 5.86605\n",
      "→ Training meta target 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 6.90531\n",
      "[200]\tvalid_0's rmse: 6.11547\n",
      "[300]\tvalid_0's rmse: 5.97871\n",
      "[400]\tvalid_0's rmse: 5.94903\n",
      "[500]\tvalid_0's rmse: 5.9413\n",
      "[600]\tvalid_0's rmse: 5.93815\n",
      "[700]\tvalid_0's rmse: 5.93934\n",
      "[800]\tvalid_0's rmse: 5.93954\n",
      "Early stopping, best iteration is:\n",
      "[649]\tvalid_0's rmse: 5.93714\n",
      "→ Training meta target 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 2.82916\n",
      "[200]\tvalid_0's rmse: 2.54427\n",
      "[300]\tvalid_0's rmse: 2.48123\n",
      "[400]\tvalid_0's rmse: 2.46448\n",
      "[500]\tvalid_0's rmse: 2.45515\n",
      "[600]\tvalid_0's rmse: 2.44973\n",
      "[700]\tvalid_0's rmse: 2.44687\n",
      "[800]\tvalid_0's rmse: 2.44353\n",
      "[900]\tvalid_0's rmse: 2.44221\n",
      "[1000]\tvalid_0's rmse: 2.44178\n",
      "[1100]\tvalid_0's rmse: 2.44108\n",
      "[1200]\tvalid_0's rmse: 2.44054\n",
      "[1300]\tvalid_0's rmse: 2.44039\n",
      "[1400]\tvalid_0's rmse: 2.44012\n",
      "[1500]\tvalid_0's rmse: 2.43993\n",
      "[1600]\tvalid_0's rmse: 2.43995\n",
      "[1700]\tvalid_0's rmse: 2.43984\n",
      "[1800]\tvalid_0's rmse: 2.43973\n",
      "[1900]\tvalid_0's rmse: 2.43959\n",
      "[2000]\tvalid_0's rmse: 2.43939\n",
      "[2100]\tvalid_0's rmse: 2.43928\n",
      "[2200]\tvalid_0's rmse: 2.43932\n",
      "[2300]\tvalid_0's rmse: 2.43931\n",
      "Early stopping, best iteration is:\n",
      "[2146]\tvalid_0's rmse: 2.43925\n",
      "→ Training meta target 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 2.38322\n",
      "[200]\tvalid_0's rmse: 2.32413\n",
      "[300]\tvalid_0's rmse: 2.30712\n",
      "[400]\tvalid_0's rmse: 2.30385\n",
      "[500]\tvalid_0's rmse: 2.30353\n",
      "[600]\tvalid_0's rmse: 2.30093\n",
      "[700]\tvalid_0's rmse: 2.30114\n",
      "[800]\tvalid_0's rmse: 2.30026\n",
      "[900]\tvalid_0's rmse: 2.30065\n",
      "Early stopping, best iteration is:\n",
      "[795]\tvalid_0's rmse: 2.30019\n",
      "→ Training meta target 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 7.31176\n",
      "[200]\tvalid_0's rmse: 7.21761\n",
      "[300]\tvalid_0's rmse: 7.18736\n",
      "[400]\tvalid_0's rmse: 7.18747\n",
      "[500]\tvalid_0's rmse: 7.1788\n",
      "[600]\tvalid_0's rmse: 7.18067\n",
      "[700]\tvalid_0's rmse: 7.18211\n",
      "Early stopping, best iteration is:\n",
      "[509]\tvalid_0's rmse: 7.17822\n",
      "→ Training meta target 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 5.58386\n",
      "[200]\tvalid_0's rmse: 5.37563\n",
      "[300]\tvalid_0's rmse: 5.35572\n",
      "[400]\tvalid_0's rmse: 5.35776\n",
      "[500]\tvalid_0's rmse: 5.35498\n",
      "Early stopping, best iteration is:\n",
      "[336]\tvalid_0's rmse: 5.35341\n",
      "→ Training meta target 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 2.89242\n",
      "[200]\tvalid_0's rmse: 2.80777\n",
      "[300]\tvalid_0's rmse: 2.78715\n",
      "[400]\tvalid_0's rmse: 2.78106\n",
      "[500]\tvalid_0's rmse: 2.77924\n",
      "[600]\tvalid_0's rmse: 2.77783\n",
      "[700]\tvalid_0's rmse: 2.77521\n",
      "[800]\tvalid_0's rmse: 2.77496\n",
      "[900]\tvalid_0's rmse: 2.77561\n",
      "Early stopping, best iteration is:\n",
      "[752]\tvalid_0's rmse: 2.77487\n",
      "✅ Meta‐model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import rankdata\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.operate_model import predict\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from python_scripts.pretrain_model import PretrainedEncoderRegressor\n",
    "# ---------------- Settings ----------------\n",
    "trained_oof_model_folder = 'output_folder/rank-spot/realign/no_pretrain/3_encoder/filtered_directly_rank/k-fold/realign_all/Macenko_masked/'\n",
    "n_folds    = len([d for d in os.listdir(trained_oof_model_folder) if d.startswith('fold')])\n",
    "n_samples  = len(full_dataset)\n",
    "C          = 35\n",
    "BATCH_SIZE = 64\n",
    "start_fold = 0\n",
    "\n",
    "tile_dim = 128\n",
    "center_dim = 128\n",
    "neighbor_dim = 128\n",
    "fusion_dim = tile_dim + center_dim + neighbor_dim\n",
    "\n",
    "pretrained_ae_name = 'AE_Center_noaug'\n",
    "pretrained_ae_path = f\"AE_model/128/{pretrained_ae_name}/best.pt\"\n",
    "ae_type = 'center'\n",
    "\n",
    "# Ground truth label (全 dataset)\n",
    "y_true = np.vstack([ full_dataset[i]['label'].cpu().numpy() for i in range(n_samples) ])\n",
    "\n",
    "# Build CV splitter (must match first stage splits)\n",
    "logo = LeaveOneGroupOut()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "lgb_base = lgb.LGBMRegressor(\n",
    "    objective='l2',\n",
    "    metric='rmse',\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=20000,\n",
    "    max_depth=15,\n",
    "    num_leaves=127,\n",
    "    colsample_bytree=0.7619407413363416,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    min_data_in_leaf=20,\n",
    "    reg_alpha=0.7480401395491829,\n",
    "    reg_lambda=0.2589860348178542,\n",
    "    verbosity=-1\n",
    "    )\n",
    "\n",
    "\n",
    "slide_idx = np.array(grouped_data['slide_idx'])   # shape (N,)\n",
    "\n",
    "N, C = len(full_dataset), 35\n",
    "oof_preds = np.zeros((N, C), dtype=float)\n",
    "oof_latents = np.zeros((N, 128*3), dtype=float)\n",
    "\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "    logo.split(X=np.zeros(n_samples), y=None, groups=slide_idx)):\n",
    "\n",
    "    # if fold_id > start_fold:\n",
    "    #     print(f\"⏭️ Skipping fold {fold_id}\")\n",
    "    #     continue\n",
    "\n",
    "    print(f\"\\n🚀 Starting fold {fold_id}...\")\n",
    "    ckpt_path = os.path.join(trained_oof_model_folder, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "\n",
    "    # === Load model and predict OOF ===\n",
    "    net = VisionMLP_MultiTask(tile_dim=tile_dim, subtile_dim=center_dim, output_dim=C)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net = net.to(device).eval()\n",
    "\n",
    "    val_ds = Subset(full_dataset, va_idx)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    preds, latents = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            tiles = batch['tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "            center = subtiles[:, 4].contiguous()\n",
    "\n",
    "            f_c = net.encoder_center(center)\n",
    "            f_n = net.encoder_subtile(subtiles)\n",
    "            f_t = net.encoder_tile(tiles)\n",
    "            fuse = torch.cat([f_c, f_n, f_t], dim=1).contiguous()\n",
    "            output = net.decoder(fuse)\n",
    "\n",
    "            preds.append(output.cpu())\n",
    "            latents.append(fuse.cpu())\n",
    "\n",
    "\n",
    "    preds = torch.cat(preds, dim=0).numpy()        # (n_val, C)\n",
    "    oof_preds[va_idx] = preds                       # 按索引写回 numpy 数组\n",
    "    latents = torch.cat(latents, dim=0).numpy()\n",
    "    oof_latents[va_idx] = latents                       # 按索引写回 numpy 数组\n",
    "\n",
    "# 2) 用完整的 oof_preds 一次生成所有 meta-features\n",
    "recon = PretrainedEncoderRegressor(\n",
    "    ae_checkpoint=pretrained_ae_path,\n",
    "    ae_type=ae_type,\n",
    "    tile_dim=tile_dim,\n",
    "    center_dim=center_dim,\n",
    "    neighbor_dim=neighbor_dim,\n",
    "    output_dim=C,\n",
    "    mode='reconstruction'\n",
    ").to(device)\n",
    "\n",
    "meta_all, meta_names = generate_meta_features(\n",
    "    dataset=full_dataset,\n",
    "    oof_preds=oof_preds,\n",
    "    model_for_recon=recon,\n",
    "    latents = oof_latents,\n",
    "    device=device,\n",
    "    ae_type=ae_type\n",
    ")  # meta_all.shape == (N, D_meta)\n",
    "diagnose_meta_nonfinite(meta_all, meta_names)\n",
    "# 3) 构造 y_true，与 oof_preds 顺序一致\n",
    "y_true = np.vstack([ full_dataset[i]['label'].cpu().numpy() for i in range(N) ])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 假设 X_meta 的 shape 是 (N, 35 + D_image)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 之后就把 X_meta_scaled 丢给你的 meta-model\n",
    "\n",
    "# 划 train/val\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    meta_all, y_true, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_tr)\n",
    "X_tr_scaled  = scaler.transform(X_tr)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "# 5) 训练 MultiOutputRegressor\n",
    "meta_model = MultiOutputRegressor(lgb_base)\n",
    "meta_model.estimators_ = []\n",
    "\n",
    "for i in range(C):\n",
    "    print(f\"→ Training meta target {i}\")\n",
    "    m = lgb.LGBMRegressor(**lgb_base.get_params())\n",
    "    m.fit(\n",
    "        X_tr_scaled, y_tr[:, i],\n",
    "        eval_set=[(X_val_scaled, y_val[:, i])],\n",
    "        callbacks=[early_stopping(stopping_rounds=200), log_evaluation(period=100)]\n",
    "    )\n",
    "    meta_model.estimators_.append(m)\n",
    "\n",
    "# 6) 保存一次训练好的 meta-model\n",
    "joblib.dump(meta_model, os.path.join(trained_oof_model_folder, \"meta_model.pkl\"))\n",
    "print(\"✅ Meta‐model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual vs. base pred correlation: 0.1801649507479534\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 残差分布\n",
    "sns.histplot(residual.flatten(), bins=50)\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# 残差和基线预测的相关性\n",
    "corr = np.corrcoef(residual.flatten(), oof_preds.flatten())[0,1]\n",
    "print(\"Residual vs. base pred correlation:\", corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting fold 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_35616/4165331370.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:310: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "Computing AE recon loss: 100%|██████████| 35/35 [00:02<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated meta-features with shape: (2197, 307)\n",
      "[fold 0] training target 0 on meta features …\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
      "  _log_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 6.4752\n",
      "[200]\tvalid_0's rmse: 6.28582\n",
      "[300]\tvalid_0's rmse: 6.20383\n",
      "[400]\tvalid_0's rmse: 6.16166\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[43], line 132\u001b[0m\n",
      "\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] training target \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on meta features …\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m    131\u001b[0m     model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlgb_base\u001b[38;5;241m.\u001b[39mget_params())\n",
      "\u001b[0;32m--> 132\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train_tab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tab\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_tab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_tab\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n",
      "\u001b[1;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    140\u001b[0m     meta_model\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# 4) 存下這個 fold 的 meta model\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/sklearn.py:1398\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n",
      "\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n",
      "\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[1;32m   1383\u001b[0m     X: _LGBM_ScikitMatrixLike,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1395\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[1;32m   1396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m   1397\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n",
      "\u001b[1;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n",
      "\u001b[0;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n",
      "\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n",
      "\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n",
      "\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n",
      "\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n",
      "\u001b[1;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/engine.py:322\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n",
      "\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n",
      "\u001b[1;32m    311\u001b[0m     cb(\n",
      "\u001b[1;32m    312\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n",
      "\u001b[1;32m    313\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    319\u001b[0m         )\n",
      "\u001b[1;32m    320\u001b[0m     )\n",
      "\u001b[0;32m--> 322\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    324\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/lightgbm/basic.py:4155\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n",
      "\u001b[1;32m   4152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n",
      "\u001b[1;32m   4153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m   4154\u001b[0m _safe_call(\n",
      "\u001b[0;32m-> 4155\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   4156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   4157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   4158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   4159\u001b[0m )\n",
      "\u001b[1;32m   4160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n",
      "\u001b[1;32m   4161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import rankdata\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.operate_model import predict\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from python_scripts.pretrain_model import PretrainedEncoderRegressor\n",
    "# ---------------- Settings ----------------\n",
    "trained_oof_model_folder = 'output_folder/rank-spot/realign/no_pretrain/3_encoder/filtered_directly_rank/k-fold/realign_all/Macenko_masked/'\n",
    "n_folds    = len([d for d in os.listdir(trained_oof_model_folder) if d.startswith('fold')])\n",
    "n_samples  = len(full_dataset)\n",
    "C          = 35\n",
    "BATCH_SIZE = 64\n",
    "start_fold = 0\n",
    "\n",
    "tile_dim = 128\n",
    "center_dim = 128\n",
    "neighbor_dim = 128\n",
    "fusion_dim = tile_dim + center_dim + neighbor_dim\n",
    "\n",
    "pretrained_ae_name = 'AE_Center_noaug'\n",
    "pretrained_ae_path = f\"AE_model/128/{pretrained_ae_name}/best.pt\"\n",
    "ae_type = 'center'\n",
    "\n",
    "# Ground truth label (全 dataset)\n",
    "y_true = np.vstack([ full_dataset[i]['label'].cpu().numpy() for i in range(n_samples) ])\n",
    "\n",
    "# Build CV splitter (must match first stage splits)\n",
    "logo = LeaveOneGroupOut()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "lgb_base = lgb.LGBMRegressor(\n",
    "    objective='l2',\n",
    "    metric='rmse',\n",
    "    learning_rate=0.007522970004049377,\n",
    "    n_estimators=12000,\n",
    "    max_depth=11,\n",
    "    num_leaves=20,\n",
    "    colsample_bytree=0.7619407413363416,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    min_data_in_leaf=20,\n",
    "    reg_alpha=0.7480401395491829,\n",
    "    reg_lambda=0.2589860348178542,\n",
    "    verbosity=-1\n",
    "    )\n",
    "\n",
    "slide_idx = np.array(grouped_data['slide_idx'])   # shape (N,)\n",
    "\n",
    "\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "    logo.split(X=np.zeros(n_samples), y=None, groups=slide_idx)):\n",
    "\n",
    "    if fold_id > start_fold:\n",
    "        print(f\"⏭️ Skipping fold {fold_id}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🚀 Starting fold {fold_id}...\")\n",
    "    ckpt_path = os.path.join(trained_oof_model_folder, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "\n",
    "    # === Load model and predict OOF ===\n",
    "    net = VisionMLP_MultiTask(tile_dim=tile_dim, subtile_dim=center_dim, output_dim=C)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net = net.to(device).eval()\n",
    "\n",
    "    val_ds = Subset(full_dataset, va_idx)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    preds, latents = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            tiles = batch['tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "            center = subtiles[:, 4].contiguous()\n",
    "\n",
    "            f_c = net.encoder_center(center)\n",
    "            f_n = net.encoder_subtile(subtiles)\n",
    "            f_t = net.encoder_tile(tiles)\n",
    "            fuse = torch.cat([f_c, f_n, f_t], dim=1).contiguous()\n",
    "            output = net.decoder(fuse)\n",
    "\n",
    "            preds.append(output.cpu())\n",
    "            latents.append(fuse.cpu())\n",
    "\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    latents = torch.cat(latents, dim=0).numpy()\n",
    "\n",
    "    # === AE model reconstruction loss ===\n",
    "    recon_model = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=pretrained_ae_path,\n",
    "        ae_type=ae_type,\n",
    "        tile_dim=tile_dim,\n",
    "        center_dim=center_dim,\n",
    "        neighbor_dim=neighbor_dim,\n",
    "        output_dim=C,\n",
    "        mode='reconstruction'\n",
    "    ).to(device)\n",
    "\n",
    "    meta = generate_meta_features(\n",
    "        dataset = val_ds,\n",
    "        oof_preds = preds,\n",
    "        image_latents = latents,\n",
    "        model_for_recon = recon_model,\n",
    "        device = device,\n",
    "        ae_type = ae_type,\n",
    "        use_clusters=\"both\"\n",
    "    )\n",
    "    \n",
    "    y_val = y_true[va_idx]   # shape = (len(va_idx), 35)\n",
    "\n",
    "    # 2) 再對這個 fold 的 meta 做 train/val 切分\n",
    "    X_train_tab, X_val_tab, y_train_tab, y_val_tab = train_test_split(\n",
    "        meta, y_val, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # 3) train MultiOutputRegressor with early stopping\n",
    "    meta_model = MultiOutputRegressor(lgb_base)\n",
    "    meta_model.estimators_ = []\n",
    "\n",
    "    for i in range(y_train_tab.shape[1]):\n",
    "        print(f\"[fold {fold_id}] training target {i} on meta features …\")\n",
    "        model = lgb.LGBMRegressor(**lgb_base.get_params())\n",
    "        model.fit(\n",
    "            X_train_tab, y_train_tab[:, i],\n",
    "            eval_set=[(X_val_tab, y_val_tab[:, i])],\n",
    "            callbacks=[\n",
    "                early_stopping(stopping_rounds=200),\n",
    "                log_evaluation(period=100)\n",
    "            ]\n",
    "        )\n",
    "        meta_model.estimators_.append(model)\n",
    "\n",
    "    # 4) 存下這個 fold 的 meta model\n",
    "    save_path = os.path.join(trained_oof_model_folder, f\"meta_model_fold{fold_id}.pkl\")\n",
    "    joblib.dump(meta_model, save_path)\n",
    "    print(f\"✅ Saved fold {fold_id} meta‐model → {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/import_data.py:280: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  raw = torch.load(pt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 從 '<class 'list'>' 推斷樣本數量: 2088\n",
      "Model forward signature: (tile, subtiles)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from python_scripts.import_data import load_node_feature_data\n",
    "\n",
    "\n",
    "image_keys = [ 'tile', 'subtiles']\n",
    "\n",
    "model = VisionMLP_MultiTask(tile_dim=tile_dim, subtile_dim=center_dim, output_dim=C)\n",
    "\n",
    "# 用法示例\n",
    "from python_scripts.import_data import importDataset\n",
    "# 假设你的 model 已经定义好并实例化为 `model`\n",
    "test_dataset = load_node_feature_data(\"dataset/spot-rank/filtered_directly_rank/masked/test/Macenko/test_dataset.pt\", model)\n",
    "test_dataset = importDataset(\n",
    "        data_dict=test_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_3581/3306017216.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:310: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "Computing AE recon loss: 100%|██████████| 33/33 [00:02<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae-recon-loss -> cols:    1, names:    1 OK\n",
      "trained-latents -> cols:  384, names:  384 OK\n",
      "subtile4     -> cols:   12, names:   12 OK\n",
      "exsubtiles   -> cols:   12, names:   12 OK\n",
      "tile         -> cols:   12, names:   12 OK\n",
      "contrast     -> cols:    3, names:    3 OK\n",
      "wavelet-tile -> cols:  280, names:  280 OK\n",
      "sobel-tile   -> cols:   40, names:   40 OK\n",
      "hsv-tile     -> cols:  120, names:  120 OK\n",
      "he-tile      -> cols:   80, names:   80 OK\n",
      "oof          -> cols:   35, names:   35 OK\n",
      "adj          -> cols:   34, names:   34 OK\n",
      "last         -> cols:  136, names:  136 OK\n",
      "mad          -> cols:    1, names:    1 OK\n",
      "skewness     -> cols:    2, names:    2 OK\n",
      "autocorr     -> cols:    5, names:    5 OK\n",
      "diff2        -> cols:   33, names:   33 OK\n",
      "diff3        -> cols:   32, names:   32 OK\n",
      "✅ Generated meta-features with shape: (2088, 1222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_3581/3306017216.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:310: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "Computing AE recon loss: 100%|██████████| 33/33 [00:03<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae-recon-loss -> cols:    1, names:    1 OK\n",
      "trained-latents -> cols:  384, names:  384 OK\n",
      "subtile4     -> cols:   12, names:   12 OK\n",
      "exsubtiles   -> cols:   12, names:   12 OK\n",
      "tile         -> cols:   12, names:   12 OK\n",
      "contrast     -> cols:    3, names:    3 OK\n",
      "wavelet-tile -> cols:  280, names:  280 OK\n",
      "sobel-tile   -> cols:   40, names:   40 OK\n",
      "hsv-tile     -> cols:  120, names:  120 OK\n",
      "he-tile      -> cols:   80, names:   80 OK\n",
      "oof          -> cols:   35, names:   35 OK\n",
      "adj          -> cols:   34, names:   34 OK\n",
      "last         -> cols:  136, names:  136 OK\n",
      "mad          -> cols:    1, names:    1 OK\n",
      "skewness     -> cols:    2, names:    2 OK\n",
      "autocorr     -> cols:    5, names:    5 OK\n",
      "diff2        -> cols:   33, names:   33 OK\n",
      "diff3        -> cols:   32, names:   32 OK\n",
      "✅ Generated meta-features with shape: (2088, 1222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_3581/3306017216.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:310: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "Computing AE recon loss: 100%|██████████| 33/33 [00:02<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae-recon-loss -> cols:    1, names:    1 OK\n",
      "trained-latents -> cols:  384, names:  384 OK\n",
      "subtile4     -> cols:   12, names:   12 OK\n",
      "exsubtiles   -> cols:   12, names:   12 OK\n",
      "tile         -> cols:   12, names:   12 OK\n",
      "contrast     -> cols:    3, names:    3 OK\n",
      "wavelet-tile -> cols:  280, names:  280 OK\n",
      "sobel-tile   -> cols:   40, names:   40 OK\n",
      "hsv-tile     -> cols:  120, names:  120 OK\n",
      "he-tile      -> cols:   80, names:   80 OK\n",
      "oof          -> cols:   35, names:   35 OK\n",
      "adj          -> cols:   34, names:   34 OK\n",
      "last         -> cols:  136, names:  136 OK\n",
      "mad          -> cols:    1, names:    1 OK\n",
      "skewness     -> cols:    2, names:    2 OK\n",
      "autocorr     -> cols:    5, names:    5 OK\n",
      "diff2        -> cols:   33, names:   33 OK\n",
      "diff3        -> cols:   32, names:   32 OK\n",
      "✅ Generated meta-features with shape: (2088, 1222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_3581/3306017216.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:310: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "Computing AE recon loss: 100%|██████████| 33/33 [00:02<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae-recon-loss -> cols:    1, names:    1 OK\n",
      "trained-latents -> cols:  384, names:  384 OK\n",
      "subtile4     -> cols:   12, names:   12 OK\n",
      "exsubtiles   -> cols:   12, names:   12 OK\n",
      "tile         -> cols:   12, names:   12 OK\n",
      "contrast     -> cols:    3, names:    3 OK\n",
      "wavelet-tile -> cols:  280, names:  280 OK\n",
      "sobel-tile   -> cols:   40, names:   40 OK\n",
      "hsv-tile     -> cols:  120, names:  120 OK\n",
      "he-tile      -> cols:   80, names:   80 OK\n",
      "oof          -> cols:   35, names:   35 OK\n",
      "adj          -> cols:   34, names:   34 OK\n",
      "last         -> cols:  136, names:  136 OK\n",
      "mad          -> cols:    1, names:    1 OK\n",
      "skewness     -> cols:    2, names:    2 OK\n",
      "autocorr     -> cols:    5, names:    5 OK\n",
      "diff2        -> cols:   33, names:   33 OK\n",
      "diff3        -> cols:   32, names:   32 OK\n",
      "✅ Generated meta-features with shape: (2088, 1222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_3581/3306017216.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:310: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "Computing AE recon loss: 100%|██████████| 33/33 [00:02<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae-recon-loss -> cols:    1, names:    1 OK\n",
      "trained-latents -> cols:  384, names:  384 OK\n",
      "subtile4     -> cols:   12, names:   12 OK\n",
      "exsubtiles   -> cols:   12, names:   12 OK\n",
      "tile         -> cols:   12, names:   12 OK\n",
      "contrast     -> cols:    3, names:    3 OK\n",
      "wavelet-tile -> cols:  280, names:  280 OK\n",
      "sobel-tile   -> cols:   40, names:   40 OK\n",
      "hsv-tile     -> cols:  120, names:  120 OK\n",
      "he-tile      -> cols:   80, names:   80 OK\n",
      "oof          -> cols:   35, names:   35 OK\n",
      "adj          -> cols:   34, names:   34 OK\n",
      "last         -> cols:  136, names:  136 OK\n",
      "mad          -> cols:    1, names:    1 OK\n",
      "skewness     -> cols:    2, names:    2 OK\n",
      "autocorr     -> cols:    5, names:    5 OK\n",
      "diff2        -> cols:   33, names:   33 OK\n",
      "diff3        -> cols:   32, names:   32 OK\n",
      "✅ Generated meta-features with shape: (2088, 1222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_3581/3306017216.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:310: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "Computing AE recon loss: 100%|██████████| 33/33 [00:02<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae-recon-loss -> cols:    1, names:    1 OK\n",
      "trained-latents -> cols:  384, names:  384 OK\n",
      "subtile4     -> cols:   12, names:   12 OK\n",
      "exsubtiles   -> cols:   12, names:   12 OK\n",
      "tile         -> cols:   12, names:   12 OK\n",
      "contrast     -> cols:    3, names:    3 OK\n",
      "wavelet-tile -> cols:  280, names:  280 OK\n",
      "sobel-tile   -> cols:   40, names:   40 OK\n",
      "hsv-tile     -> cols:  120, names:  120 OK\n",
      "he-tile      -> cols:   80, names:   80 OK\n",
      "oof          -> cols:   35, names:   35 OK\n",
      "adj          -> cols:   34, names:   34 OK\n",
      "last         -> cols:  136, names:  136 OK\n",
      "mad          -> cols:    1, names:    1 OK\n",
      "skewness     -> cols:    2, names:    2 OK\n",
      "autocorr     -> cols:    5, names:    5 OK\n",
      "diff2        -> cols:   33, names:   33 OK\n",
      "diff3        -> cols:   32, names:   32 OK\n",
      "✅ Generated meta-features with shape: (2088, 1222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved stacked submission in output_folder/rank-spot/realign/no_pretrain/3_encoder/filtered_directly_rank/k-fold/realign_all/Macenko_masked/\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。\n",
      "\u001b[1;31m請檢閱儲存格中的程式碼，找出失敗的可能原因。\n",
      "\u001b[1;31m如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。\n",
      "\u001b[1;31m如需詳細資料，請檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>。"
     ]
    }
   ],
   "source": [
    "# --- 3) Prepare test meta-features ---\n",
    "n_test = len(test_dataset)\n",
    "\n",
    "all_final = []\n",
    "\n",
    "for fold_id in range(n_folds):\n",
    "    # if fold_id > start_fold:\n",
    "    #     print(f\"⏭️ Skipping fold {fold_id}\")\n",
    "    #     continue\n",
    "    ckpt_path = os.path.join(trained_oof_model_folder, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = VisionMLP_MultiTask(tile_dim=tile_dim, subtile_dim=center_dim, output_dim=C)\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_preds = []\n",
    "    test_latents = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            tiles = batch['tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "            center = subtiles[:, 4].contiguous()\n",
    "\n",
    "            f_c = net.encoder_center(center)\n",
    "            f_n = net.encoder_subtile(subtiles)\n",
    "            f_t = net.encoder_tile(tiles)\n",
    "            fuse = torch.cat([f_c, f_n, f_t], dim=1).contiguous()\n",
    "            output = net.decoder(fuse)\n",
    "\n",
    "            test_preds.append(output.cpu())\n",
    "            test_latents.append(fuse.cpu())\n",
    "\n",
    "    test_preds = torch.cat(test_preds, dim=0).numpy()\n",
    "    test_latents = torch.cat(test_latents, dim=0).numpy()\n",
    "# === AE model reconstruction loss ===\n",
    "    recon_model = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=pretrained_ae_path,\n",
    "        ae_type=ae_type,\n",
    "        tile_dim=tile_dim,\n",
    "        center_dim=center_dim,\n",
    "        neighbor_dim=neighbor_dim,\n",
    "        output_dim=C,\n",
    "        mode='reconstruction'\n",
    "    ).to(device)\n",
    "\n",
    "    meta, name = generate_meta_features(\n",
    "        dataset = test_dataset,\n",
    "        oof_preds = test_preds,\n",
    "        latents = test_latents,\n",
    "        model_for_recon = recon_model,\n",
    "        device = device,\n",
    "        ae_type = ae_type,\n",
    "    )\n",
    "    # ** 用训练时同一个 scaler 来标准化 **  \n",
    "    meta_scaled = scaler.transform(meta)  # (n_test, n_meta_feats)\n",
    "    # 1) 直接載入整個 MultiOutputRegressor\n",
    "    meta_model_path =  os.path.join(trained_oof_model_folder, \"meta_model.pkl\")\n",
    "    meta_model = joblib.load(meta_model_path)\n",
    "    # 用 meta_model 预测 residual  \n",
    "    pred = meta_model.predict(meta_scaled)  #  (n_test, C)\n",
    "\n",
    "\n",
    "    all_final.append(pred)\n",
    "\n",
    "\n",
    "final_preds = np.mean(all_final, axis=0)  # (n_test, C)\n",
    "\n",
    "# --- Save submission ---\n",
    "import h5py\n",
    "import pandas as pd\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spot_ids = pd.DataFrame(np.array(f[\"spots/Test\"][\"S_7\"]))\n",
    "sub = pd.DataFrame(final_preds, columns=[f\"C{i+1}\" for i in range(C)])\n",
    "sub.insert(0, 'ID', test_spot_ids.index)\n",
    "sub.to_csv(os.path.join(trained_oof_model_folder, 'submission_stacked.csv'), index=False)\n",
    "print(f\"✅ Saved stacked submission in {trained_oof_model_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/j5s0yzcj34l3v043s7znkplc0000gn/T/ipykernel_3581/2907721651.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
      "/Users/deweywang/Desktop/GitHub/HEVisum/python_scripts/pretrain_model.py:310: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae.load_state_dict(torch.load(ae_checkpoint, map_location=\"cpu\"))\n",
      "Computing AE recon loss: 100%|██████████| 33/33 [00:02<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae-recon-loss -> cols:    1, names:    1 OK\n",
      "ae           -> cols:  384, names:  384 OK\n",
      "subtile4     -> cols:   12, names:   12 OK\n",
      "exsubtiles   -> cols:   12, names:   12 OK\n",
      "tile         -> cols:   12, names:   12 OK\n",
      "contrast     -> cols:    3, names:    3 OK\n",
      "wavelet-tile -> cols:  280, names:  280 OK\n",
      "oof          -> cols:   35, names:   35 OK\n",
      "mad          -> cols:    1, names:    1 OK\n",
      "✅ Generated meta-features with shape: (2088, 740)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️ Skipping fold 1\n",
      "⏭️ Skipping fold 2\n",
      "⏭️ Skipping fold 3\n",
      "⏭️ Skipping fold 4\n",
      "⏭️ Skipping fold 5\n",
      "✅ Saved stacked submission in output_folder/rank-spot/realign/no_pretrain/3_encoder/filtered_directly_rank/k-fold/realign_all/Macenko_masked/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spatialhackathon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- 3) Prepare test meta-features ---\n",
    "n_test = len(test_dataset)\n",
    "\n",
    "\n",
    "for fold_id in range(n_folds):\n",
    "    if fold_id > start_fold:\n",
    "        print(f\"⏭️ Skipping fold {fold_id}\")\n",
    "        continue\n",
    "    ckpt_path = os.path.join(trained_oof_model_folder, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = VisionMLP_MultiTask(tile_dim=tile_dim, subtile_dim=center_dim, output_dim=C)\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_preds = []\n",
    "    test_latents = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            tiles = batch['tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "            center = subtiles[:, 4].contiguous()\n",
    "\n",
    "            f_c = net.encoder_center(center)\n",
    "            f_n = net.encoder_subtile(subtiles)\n",
    "            f_t = net.encoder_tile(tiles)\n",
    "            fuse = torch.cat([f_c, f_n, f_t], dim=1).contiguous()\n",
    "            output = net.decoder(fuse)\n",
    "\n",
    "            test_preds.append(output.cpu())\n",
    "            test_latents.append(fuse.cpu())\n",
    "\n",
    "\n",
    "    test_preds = torch.cat(test_preds, dim=0).numpy()\n",
    "    test_latents = torch.cat(test_latents, dim=0).numpy()\n",
    "# === AE model reconstruction loss ===\n",
    "    recon_model = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=pretrained_ae_path,\n",
    "        ae_type=ae_type,\n",
    "        tile_dim=tile_dim,\n",
    "        center_dim=center_dim,\n",
    "        neighbor_dim=neighbor_dim,\n",
    "        output_dim=C,\n",
    "        mode='reconstruction'\n",
    "    ).to(device)\n",
    "\n",
    "    meta, name = generate_meta_features(\n",
    "        dataset = test_dataset,\n",
    "        oof_preds = test_preds,\n",
    "        model_for_recon = recon_model,\n",
    "        device = device,\n",
    "        ae_type = ae_type,\n",
    "    )\n",
    "    # meta, name = groupwise_reduce(\n",
    "    #     features=test_meta_initial,\n",
    "    #     names=test_name_initial,\n",
    "    # )\n",
    "        \n",
    "    # 1) 直接載入整個 MultiOutputRegressor\n",
    "    meta_model_path = os.path.join(trained_oof_model_folder, f\"fold{fold_id}\",\"meta_model.pkl\")\n",
    "    meta_model = joblib.load(meta_model_path)\n",
    "\n",
    "    # 2) 用剛剛算出的 meta features 做預測\n",
    "    resid_pred = meta_model.predict(meta)\n",
    "    final_preds = test_preds + resid_pred\n",
    "\n",
    "\n",
    "\n",
    "# --- Save submission ---\n",
    "import h5py\n",
    "import pandas as pd\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spot_ids = pd.DataFrame(np.array(f[\"spots/Test\"][\"S_7\"]))\n",
    "sub = pd.DataFrame(final_preds, columns=[f\"C{i+1}\" for i in range(C)])\n",
    "sub.insert(0, 'ID', test_spot_ids.index)\n",
    "sub.to_csv(os.path.join(trained_oof_model_folder, 'submission_stacked.csv'), index=False)\n",
    "print(f\"✅ Saved stacked submission in {trained_oof_model_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "sobel-tile         223.985714\n",
      "sobel-subtile      204.554762\n",
      "wavelet-subtile    200.403968\n",
      "wavelet            194.213810\n",
      "hsv-tile           178.371429\n",
      "ae                 142.711310\n",
      "he-tile            130.703571\n",
      "oof                118.499592\n",
      "hsv-subtile        116.662169\n",
      "he-subtile         110.588889\n",
      "ae-recon-loss       83.028571\n",
      "exsubtiles          81.842857\n",
      "tile                79.740476\n",
      "subtile4            79.554762\n",
      "wavelet-tile        71.435714\n",
      "trained-latents     65.637277\n",
      "contrast            58.733333\n",
      "Name: importance, dtype: float64\n",
      "                                  feature  importance         category\n",
      "0                    ae-recon-loss_center   83.028571    ae-recon-loss\n",
      "1                                 ae_emb0   59.771429               ae\n",
      "2                                 ae_emb1   75.057143               ae\n",
      "3                                 ae_emb2  106.942857               ae\n",
      "4                                 ae_emb3   69.057143               ae\n",
      "5                                 ae_emb4   43.400000               ae\n",
      "6                                 ae_emb5   77.971429               ae\n",
      "7                                 ae_emb6   49.514286               ae\n",
      "8                                 ae_emb7   46.685714               ae\n",
      "9                                 ae_emb8   47.742857               ae\n",
      "10                                ae_emb9   55.085714               ae\n",
      "11                               ae_emb10   67.057143               ae\n",
      "12                               ae_emb11   41.714286               ae\n",
      "13                               ae_emb12   35.571429               ae\n",
      "14                               ae_emb13   60.485714               ae\n",
      "15                               ae_emb14   48.742857               ae\n",
      "16                               ae_emb15   87.342857               ae\n",
      "17                               ae_emb16  113.057143               ae\n",
      "18                               ae_emb17   91.457143               ae\n",
      "19                               ae_emb18   73.314286               ae\n",
      "20                               ae_emb19   70.628571               ae\n",
      "21                               ae_emb20   70.971429               ae\n",
      "22                               ae_emb21   55.171429               ae\n",
      "23                               ae_emb22  100.800000               ae\n",
      "24                               ae_emb23   48.971429               ae\n",
      "25                               ae_emb24   54.285714               ae\n",
      "26                               ae_emb25   44.542857               ae\n",
      "27                               ae_emb26   59.657143               ae\n",
      "28                               ae_emb27   78.142857               ae\n",
      "29                               ae_emb28   63.714286               ae\n",
      "30                               ae_emb29   76.285714               ae\n",
      "31                               ae_emb30   57.885714               ae\n",
      "32                               ae_emb31  112.000000               ae\n",
      "33                               ae_emb32  122.285714               ae\n",
      "34                               ae_emb33   73.457143               ae\n",
      "35                               ae_emb34  100.542857               ae\n",
      "36                               ae_emb35  240.400000               ae\n",
      "37                               ae_emb36  236.285714               ae\n",
      "38                               ae_emb37  240.371429               ae\n",
      "39                               ae_emb38   62.571429               ae\n",
      "40                               ae_emb39   74.457143               ae\n",
      "41                               ae_emb40  144.771429               ae\n",
      "42                               ae_emb41   62.971429               ae\n",
      "43                               ae_emb42  107.428571               ae\n",
      "44                               ae_emb43   74.285714               ae\n",
      "45                               ae_emb44   60.942857               ae\n",
      "46                               ae_emb45  228.342857               ae\n",
      "47                               ae_emb46  205.714286               ae\n",
      "48                               ae_emb47  233.371429               ae\n",
      "49                               ae_emb48   74.000000               ae\n",
      "50                               ae_emb49  188.742857               ae\n",
      "51                               ae_emb50   70.114286               ae\n",
      "52                               ae_emb51  226.714286               ae\n",
      "53                               ae_emb52  225.857143               ae\n",
      "54                               ae_emb53  206.914286               ae\n",
      "55                               ae_emb54  179.628571               ae\n",
      "56                               ae_emb55  192.828571               ae\n",
      "57                               ae_emb56  119.314286               ae\n",
      "58                               ae_emb57  199.657143               ae\n",
      "59                               ae_emb58   70.400000               ae\n",
      "60                               ae_emb59  219.600000               ae\n",
      "61                               ae_emb60   47.028571               ae\n",
      "62                               ae_emb61  230.885714               ae\n",
      "63                               ae_emb62  124.457143               ae\n",
      "64                               ae_emb63  233.628571               ae\n",
      "65                               ae_emb64  242.200000               ae\n",
      "66                               ae_emb65  102.685714               ae\n",
      "67                               ae_emb66  227.428571               ae\n",
      "68                               ae_emb67   99.485714               ae\n",
      "69                               ae_emb68  163.657143               ae\n",
      "70                               ae_emb69   80.657143               ae\n",
      "71                               ae_emb70  159.428571               ae\n",
      "72                               ae_emb71   63.085714               ae\n",
      "73                               ae_emb72  244.400000               ae\n",
      "74                               ae_emb73  163.742857               ae\n",
      "75                               ae_emb74  262.742857               ae\n",
      "76                               ae_emb75  225.657143               ae\n",
      "77                               ae_emb76  164.171429               ae\n",
      "78                               ae_emb77  238.171429               ae\n",
      "79                               ae_emb78  105.142857               ae\n",
      "80                               ae_emb79   96.685714               ae\n",
      "81                               ae_emb80   42.971429               ae\n",
      "82                               ae_emb81  219.371429               ae\n",
      "83                               ae_emb82  222.085714               ae\n",
      "84                               ae_emb83  216.914286               ae\n",
      "85                               ae_emb84  215.914286               ae\n",
      "86                               ae_emb85  223.200000               ae\n",
      "87                               ae_emb86   85.885714               ae\n",
      "88                               ae_emb87  181.514286               ae\n",
      "89                               ae_emb88  231.342857               ae\n",
      "90                               ae_emb89  230.942857               ae\n",
      "91                               ae_emb90  261.685714               ae\n",
      "92                               ae_emb91  101.085714               ae\n",
      "93                               ae_emb92  227.742857               ae\n",
      "94                               ae_emb93   76.571429               ae\n",
      "95                               ae_emb94  146.485714               ae\n",
      "96                               ae_emb95  217.428571               ae\n",
      "97                               ae_emb96  217.714286               ae\n",
      "98                               ae_emb97  194.085714               ae\n",
      "99                               ae_emb98  179.114286               ae\n",
      "100                              ae_emb99  256.600000               ae\n",
      "101                             ae_emb100   62.571429               ae\n",
      "102                             ae_emb101   65.371429               ae\n",
      "103                             ae_emb102   68.885714               ae\n",
      "104                             ae_emb103  188.971429               ae\n",
      "105                             ae_emb104  163.885714               ae\n",
      "106                             ae_emb105  218.428571               ae\n",
      "107                             ae_emb106   66.485714               ae\n",
      "108                             ae_emb107   60.257143               ae\n",
      "109                             ae_emb108   70.257143               ae\n",
      "110                             ae_emb109  233.942857               ae\n",
      "111                             ae_emb110  118.600000               ae\n",
      "112                             ae_emb111  195.428571               ae\n",
      "113                             ae_emb112   54.428571               ae\n",
      "114                             ae_emb113   77.428571               ae\n",
      "115                             ae_emb114   75.285714               ae\n",
      "116                             ae_emb115  205.600000               ae\n",
      "117                             ae_emb116   93.342857               ae\n",
      "118                             ae_emb117  204.371429               ae\n",
      "119                             ae_emb118  147.400000               ae\n",
      "120                             ae_emb119  250.028571               ae\n",
      "121                             ae_emb120  246.828571               ae\n",
      "122                             ae_emb121  155.000000               ae\n",
      "123                             ae_emb122   89.942857               ae\n",
      "124                             ae_emb123   63.885714               ae\n",
      "125                             ae_emb124   98.885714               ae\n",
      "126                             ae_emb125   54.514286               ae\n",
      "127                             ae_emb126  172.942857               ae\n",
      "128                             ae_emb127  201.314286               ae\n",
      "129                             ae_emb128   36.171429               ae\n",
      "130                             ae_emb129  215.371429               ae\n",
      "131                             ae_emb130   73.171429               ae\n",
      "132                             ae_emb131   52.028571               ae\n",
      "133                             ae_emb132  217.314286               ae\n",
      "134                             ae_emb133  232.371429               ae\n",
      "135                             ae_emb134  162.285714               ae\n",
      "136                             ae_emb135   36.685714               ae\n",
      "137                             ae_emb136   71.371429               ae\n",
      "138                             ae_emb137  254.914286               ae\n",
      "139                             ae_emb138  202.942857               ae\n",
      "140                             ae_emb139  236.657143               ae\n",
      "141                             ae_emb140   62.971429               ae\n",
      "142                             ae_emb141  221.828571               ae\n",
      "143                             ae_emb142  171.657143               ae\n",
      "144                             ae_emb143  232.628571               ae\n",
      "145                             ae_emb144  108.714286               ae\n",
      "146                             ae_emb145   45.828571               ae\n",
      "147                             ae_emb146  191.685714               ae\n",
      "148                             ae_emb147  216.914286               ae\n",
      "149                             ae_emb148  255.228571               ae\n",
      "150                             ae_emb149  220.714286               ae\n",
      "151                             ae_emb150  206.000000               ae\n",
      "152                             ae_emb151  244.085714               ae\n",
      "153                             ae_emb152  244.857143               ae\n",
      "154                             ae_emb153   94.457143               ae\n",
      "155                             ae_emb154   95.657143               ae\n",
      "156                             ae_emb155  236.000000               ae\n",
      "157                             ae_emb156  231.114286               ae\n",
      "158                             ae_emb157  205.628571               ae\n",
      "159                             ae_emb158   74.885714               ae\n",
      "160                             ae_emb159  172.742857               ae\n",
      "161                             ae_emb160   52.971429               ae\n",
      "162                             ae_emb161   58.600000               ae\n",
      "163                             ae_emb162  101.142857               ae\n",
      "164                             ae_emb163   42.400000               ae\n",
      "165                             ae_emb164  120.057143               ae\n",
      "166                             ae_emb165  160.800000               ae\n",
      "167                             ae_emb166  194.514286               ae\n",
      "168                             ae_emb167   27.628571               ae\n",
      "169                             ae_emb168   88.800000               ae\n",
      "170                             ae_emb169   81.485714               ae\n",
      "171                             ae_emb170   91.571429               ae\n",
      "172                             ae_emb171   83.485714               ae\n",
      "173                             ae_emb172   43.057143               ae\n",
      "174                             ae_emb173  177.800000               ae\n",
      "175                             ae_emb174   37.342857               ae\n",
      "176                             ae_emb175  193.914286               ae\n",
      "177                             ae_emb176   46.114286               ae\n",
      "178                             ae_emb177  199.200000               ae\n",
      "179                             ae_emb178  128.085714               ae\n",
      "180                             ae_emb179   83.857143               ae\n",
      "181                             ae_emb180  111.057143               ae\n",
      "182                             ae_emb181  142.771429               ae\n",
      "183                             ae_emb182  213.200000               ae\n",
      "184                             ae_emb183   91.057143               ae\n",
      "185                             ae_emb184   98.028571               ae\n",
      "186                             ae_emb185   98.600000               ae\n",
      "187                             ae_emb186  178.657143               ae\n",
      "188                             ae_emb187  145.171429               ae\n",
      "189                             ae_emb188  191.514286               ae\n",
      "190                             ae_emb189   76.142857               ae\n",
      "191                             ae_emb190  120.428571               ae\n",
      "192                             ae_emb191  110.885714               ae\n",
      "193                             ae_emb192  103.685714               ae\n",
      "194                             ae_emb193  202.685714               ae\n",
      "195                             ae_emb194   52.057143               ae\n",
      "196                             ae_emb195   88.171429               ae\n",
      "197                             ae_emb196   88.828571               ae\n",
      "198                             ae_emb197   63.942857               ae\n",
      "199                             ae_emb198  167.314286               ae\n",
      "200                             ae_emb199  117.742857               ae\n",
      "201                             ae_emb200  143.142857               ae\n",
      "202                             ae_emb201  175.914286               ae\n",
      "203                             ae_emb202  187.200000               ae\n",
      "204                             ae_emb203  187.828571               ae\n",
      "205                             ae_emb204   52.228571               ae\n",
      "206                             ae_emb205  174.085714               ae\n",
      "207                             ae_emb206   78.514286               ae\n",
      "208                             ae_emb207  169.457143               ae\n",
      "209                             ae_emb208  172.257143               ae\n",
      "210                             ae_emb209   51.371429               ae\n",
      "211                             ae_emb210   65.885714               ae\n",
      "212                             ae_emb211  109.285714               ae\n",
      "213                             ae_emb212   61.971429               ae\n",
      "214                             ae_emb213   93.771429               ae\n",
      "215                             ae_emb214  183.657143               ae\n",
      "216                             ae_emb215  132.400000               ae\n",
      "217                             ae_emb216  112.800000               ae\n",
      "218                             ae_emb217   86.285714               ae\n",
      "219                             ae_emb218   94.942857               ae\n",
      "220                             ae_emb219   72.257143               ae\n",
      "221                             ae_emb220   85.428571               ae\n",
      "222                             ae_emb221   39.742857               ae\n",
      "223                             ae_emb222  141.257143               ae\n",
      "224                             ae_emb223  117.228571               ae\n",
      "225                             ae_emb224   64.828571               ae\n",
      "226                             ae_emb225   83.028571               ae\n",
      "227                             ae_emb226   53.114286               ae\n",
      "228                             ae_emb227   59.628571               ae\n",
      "229                             ae_emb228   49.428571               ae\n",
      "230                             ae_emb229   60.742857               ae\n",
      "231                             ae_emb230   53.171429               ae\n",
      "232                             ae_emb231  186.657143               ae\n",
      "233                             ae_emb232  171.771429               ae\n",
      "234                             ae_emb233  141.257143               ae\n",
      "235                             ae_emb234  173.771429               ae\n",
      "236                             ae_emb235   50.400000               ae\n",
      "237                             ae_emb236   63.514286               ae\n",
      "238                             ae_emb237  135.857143               ae\n",
      "239                             ae_emb238  184.000000               ae\n",
      "240                             ae_emb239   66.742857               ae\n",
      "241                             ae_emb240  187.257143               ae\n",
      "242                             ae_emb241  122.142857               ae\n",
      "243                             ae_emb242  127.285714               ae\n",
      "244                             ae_emb243   26.857143               ae\n",
      "245                             ae_emb244  124.571429               ae\n",
      "246                             ae_emb245   73.857143               ae\n",
      "247                             ae_emb246  182.828571               ae\n",
      "248                             ae_emb247   99.800000               ae\n",
      "249                             ae_emb248  147.685714               ae\n",
      "250                             ae_emb249   59.628571               ae\n",
      "251                             ae_emb250  160.971429               ae\n",
      "252                             ae_emb251   89.000000               ae\n",
      "253                             ae_emb252   56.628571               ae\n",
      "254                             ae_emb253  173.000000               ae\n",
      "255                             ae_emb254  149.342857               ae\n",
      "256                             ae_emb255   56.285714               ae\n",
      "257                             ae_emb256   53.542857               ae\n",
      "258                             ae_emb257  133.657143               ae\n",
      "259                             ae_emb258  115.600000               ae\n",
      "260                             ae_emb259   89.171429               ae\n",
      "261                             ae_emb260  105.771429               ae\n",
      "262                             ae_emb261  107.828571               ae\n",
      "263                             ae_emb262  175.114286               ae\n",
      "264                             ae_emb263   72.685714               ae\n",
      "265                             ae_emb264   42.657143               ae\n",
      "266                             ae_emb265  122.971429               ae\n",
      "267                             ae_emb266   90.600000               ae\n",
      "268                             ae_emb267  191.657143               ae\n",
      "269                             ae_emb268   27.285714               ae\n",
      "270                             ae_emb269   88.171429               ae\n",
      "271                             ae_emb270  162.800000               ae\n",
      "272                             ae_emb271  143.942857               ae\n",
      "273                             ae_emb272  141.257143               ae\n",
      "274                             ae_emb273  218.771429               ae\n",
      "275                             ae_emb274  176.828571               ae\n",
      "276                             ae_emb275   25.857143               ae\n",
      "277                             ae_emb276   61.885714               ae\n",
      "278                             ae_emb277  188.685714               ae\n",
      "279                             ae_emb278  117.285714               ae\n",
      "280                             ae_emb279  172.114286               ae\n",
      "281                             ae_emb280   68.371429               ae\n",
      "282                             ae_emb281  121.514286               ae\n",
      "283                             ae_emb282  179.600000               ae\n",
      "284                             ae_emb283  141.085714               ae\n",
      "285                             ae_emb284  105.400000               ae\n",
      "286                             ae_emb285  157.742857               ae\n",
      "287                             ae_emb286  127.400000               ae\n",
      "288                             ae_emb287   28.457143               ae\n",
      "289                             ae_emb288  157.257143               ae\n",
      "290                             ae_emb289   44.885714               ae\n",
      "291                             ae_emb290   52.457143               ae\n",
      "292                             ae_emb291  208.657143               ae\n",
      "293                             ae_emb292  203.371429               ae\n",
      "294                             ae_emb293  191.857143               ae\n",
      "295                             ae_emb294  191.571429               ae\n",
      "296                             ae_emb295  165.571429               ae\n",
      "297                             ae_emb296  205.514286               ae\n",
      "298                             ae_emb297  220.085714               ae\n",
      "299                             ae_emb298  179.000000               ae\n",
      "300                             ae_emb299  148.657143               ae\n",
      "301                             ae_emb300  236.400000               ae\n",
      "302                             ae_emb301  187.885714               ae\n",
      "303                             ae_emb302  135.828571               ae\n",
      "304                             ae_emb303  176.914286               ae\n",
      "305                             ae_emb304  166.085714               ae\n",
      "306                             ae_emb305  198.542857               ae\n",
      "307                             ae_emb306  159.542857               ae\n",
      "308                             ae_emb307  175.314286               ae\n",
      "309                             ae_emb308  135.657143               ae\n",
      "310                             ae_emb309  174.485714               ae\n",
      "311                             ae_emb310  197.257143               ae\n",
      "312                             ae_emb311  235.000000               ae\n",
      "313                             ae_emb312  208.885714               ae\n",
      "314                             ae_emb313  184.628571               ae\n",
      "315                             ae_emb314  202.571429               ae\n",
      "316                             ae_emb315  164.885714               ae\n",
      "317                             ae_emb316  210.971429               ae\n",
      "318                             ae_emb317  186.428571               ae\n",
      "319                             ae_emb318  186.400000               ae\n",
      "320                             ae_emb319  227.314286               ae\n",
      "321                             ae_emb320  210.714286               ae\n",
      "322                             ae_emb321  140.800000               ae\n",
      "323                             ae_emb322  131.314286               ae\n",
      "324                             ae_emb323  193.342857               ae\n",
      "325                             ae_emb324  180.342857               ae\n",
      "326                             ae_emb325  155.571429               ae\n",
      "327                             ae_emb326  232.457143               ae\n",
      "328                             ae_emb327  191.657143               ae\n",
      "329                             ae_emb328  201.400000               ae\n",
      "330                             ae_emb329  233.085714               ae\n",
      "331                             ae_emb330  203.257143               ae\n",
      "332                             ae_emb331  188.857143               ae\n",
      "333                             ae_emb332  154.828571               ae\n",
      "334                             ae_emb333  177.085714               ae\n",
      "335                             ae_emb334  182.542857               ae\n",
      "336                             ae_emb335  216.000000               ae\n",
      "337                             ae_emb336  192.685714               ae\n",
      "338                             ae_emb337  230.057143               ae\n",
      "339                             ae_emb338  217.057143               ae\n",
      "340                             ae_emb339  210.685714               ae\n",
      "341                             ae_emb340  154.628571               ae\n",
      "342                             ae_emb341  170.142857               ae\n",
      "343                             ae_emb342  243.571429               ae\n",
      "344                             ae_emb343  175.828571               ae\n",
      "345                             ae_emb344  161.828571               ae\n",
      "346                             ae_emb345  219.685714               ae\n",
      "347                             ae_emb346  209.485714               ae\n",
      "348                             ae_emb347  195.085714               ae\n",
      "349                             ae_emb348  184.200000               ae\n",
      "350                             ae_emb349  180.800000               ae\n",
      "351                             ae_emb350  208.742857               ae\n",
      "352                             ae_emb351  134.057143               ae\n",
      "353                             ae_emb352  201.428571               ae\n",
      "354                             ae_emb353  157.342857               ae\n",
      "355                             ae_emb354  217.942857               ae\n",
      "356                             ae_emb355  206.828571               ae\n",
      "357                             ae_emb356  238.000000               ae\n",
      "358                             ae_emb357  192.257143               ae\n",
      "359                             ae_emb358  185.542857               ae\n",
      "360                             ae_emb359  231.028571               ae\n",
      "361                             ae_emb360  188.000000               ae\n",
      "362                             ae_emb361  193.400000               ae\n",
      "363                             ae_emb362  164.142857               ae\n",
      "364                             ae_emb363  153.371429               ae\n",
      "365                             ae_emb364  197.342857               ae\n",
      "366                             ae_emb365  239.342857               ae\n",
      "367                             ae_emb366  197.600000               ae\n",
      "368                             ae_emb367  142.971429               ae\n",
      "369                             ae_emb368  167.742857               ae\n",
      "370                             ae_emb369  161.857143               ae\n",
      "371                             ae_emb370  232.571429               ae\n",
      "372                             ae_emb371  235.457143               ae\n",
      "373                             ae_emb372  186.571429               ae\n",
      "374                             ae_emb373  156.200000               ae\n",
      "375                             ae_emb374  243.457143               ae\n",
      "376                             ae_emb375  154.885714               ae\n",
      "377                             ae_emb376  157.600000               ae\n",
      "378                             ae_emb377  174.114286               ae\n",
      "379                             ae_emb378  213.200000               ae\n",
      "380                             ae_emb379  182.600000               ae\n",
      "381                             ae_emb380  185.657143               ae\n",
      "382                             ae_emb381  177.142857               ae\n",
      "383                             ae_emb382  214.000000               ae\n",
      "384                             ae_emb383  157.228571               ae\n",
      "385                     trained-latents_0  170.028571  trained-latents\n",
      "386                     trained-latents_1  190.771429  trained-latents\n",
      "387                     trained-latents_2  180.228571  trained-latents\n",
      "388                     trained-latents_3  156.828571  trained-latents\n",
      "389                     trained-latents_4  154.971429  trained-latents\n",
      "390                     trained-latents_5  175.857143  trained-latents\n",
      "391                     trained-latents_6  171.285714  trained-latents\n",
      "392                     trained-latents_7  203.171429  trained-latents\n",
      "393                     trained-latents_8  156.028571  trained-latents\n",
      "394                     trained-latents_9  206.971429  trained-latents\n",
      "395                    trained-latents_10  175.400000  trained-latents\n",
      "396                    trained-latents_11  192.057143  trained-latents\n",
      "397                    trained-latents_12  139.800000  trained-latents\n",
      "398                    trained-latents_13  177.685714  trained-latents\n",
      "399                    trained-latents_14  198.428571  trained-latents\n",
      "400                    trained-latents_15  171.571429  trained-latents\n",
      "401                    trained-latents_16  179.400000  trained-latents\n",
      "402                    trained-latents_17  152.114286  trained-latents\n",
      "403                    trained-latents_18  178.028571  trained-latents\n",
      "404                    trained-latents_19  164.742857  trained-latents\n",
      "405                    trained-latents_20  178.428571  trained-latents\n",
      "406                    trained-latents_21  210.828571  trained-latents\n",
      "407                    trained-latents_22  198.428571  trained-latents\n",
      "408                    trained-latents_23  227.828571  trained-latents\n",
      "409                    trained-latents_24  182.228571  trained-latents\n",
      "410                    trained-latents_25  200.885714  trained-latents\n",
      "411                    trained-latents_26  219.600000  trained-latents\n",
      "412                    trained-latents_27  190.028571  trained-latents\n",
      "413                    trained-latents_28  220.828571  trained-latents\n",
      "414                    trained-latents_29  175.314286  trained-latents\n",
      "415                    trained-latents_30  187.742857  trained-latents\n",
      "416                    trained-latents_31  167.742857  trained-latents\n",
      "417                    trained-latents_32  172.742857  trained-latents\n",
      "418                    trained-latents_33  190.371429  trained-latents\n",
      "419                    trained-latents_34  209.228571  trained-latents\n",
      "420                    trained-latents_35   39.142857  trained-latents\n",
      "421                    trained-latents_36   67.257143  trained-latents\n",
      "422                    trained-latents_37   48.885714  trained-latents\n",
      "423                    trained-latents_38   39.314286  trained-latents\n",
      "424                    trained-latents_39   44.885714  trained-latents\n",
      "425                    trained-latents_40   43.714286  trained-latents\n",
      "426                    trained-latents_41   56.885714  trained-latents\n",
      "427                    trained-latents_42   62.285714  trained-latents\n",
      "428                    trained-latents_43   63.142857  trained-latents\n",
      "429                    trained-latents_44   44.314286  trained-latents\n",
      "430                    trained-latents_45   30.857143  trained-latents\n",
      "431                    trained-latents_46   53.057143  trained-latents\n",
      "432                    trained-latents_47   56.428571  trained-latents\n",
      "433                    trained-latents_48   68.314286  trained-latents\n",
      "434                    trained-latents_49   50.771429  trained-latents\n",
      "435                    trained-latents_50   27.171429  trained-latents\n",
      "436                    trained-latents_51   27.400000  trained-latents\n",
      "437                    trained-latents_52   46.428571  trained-latents\n",
      "438                    trained-latents_53   65.885714  trained-latents\n",
      "439                    trained-latents_54   39.571429  trained-latents\n",
      "440                    trained-latents_55   35.257143  trained-latents\n",
      "441                    trained-latents_56   35.057143  trained-latents\n",
      "442                    trained-latents_57   41.885714  trained-latents\n",
      "443                    trained-latents_58   67.828571  trained-latents\n",
      "444                    trained-latents_59   48.057143  trained-latents\n",
      "445                    trained-latents_60   48.200000  trained-latents\n",
      "446                    trained-latents_61   48.400000  trained-latents\n",
      "447                    trained-latents_62   35.514286  trained-latents\n",
      "448                    trained-latents_63   49.514286  trained-latents\n",
      "449                    trained-latents_64   53.485714  trained-latents\n",
      "450                    trained-latents_65   56.428571  trained-latents\n",
      "451                    trained-latents_66   53.714286  trained-latents\n",
      "452                    trained-latents_67   53.457143  trained-latents\n",
      "453                    trained-latents_68   34.457143  trained-latents\n",
      "454                    trained-latents_69   52.400000  trained-latents\n",
      "455                    trained-latents_70   47.114286  trained-latents\n",
      "456                    trained-latents_71   37.200000  trained-latents\n",
      "457                    trained-latents_72   31.942857  trained-latents\n",
      "458                    trained-latents_73   57.171429  trained-latents\n",
      "459                    trained-latents_74   54.457143  trained-latents\n",
      "460                    trained-latents_75   56.028571  trained-latents\n",
      "461                    trained-latents_76   41.314286  trained-latents\n",
      "462                    trained-latents_77   58.057143  trained-latents\n",
      "463                    trained-latents_78   56.257143  trained-latents\n",
      "464                    trained-latents_79   47.771429  trained-latents\n",
      "465                    trained-latents_80   43.000000  trained-latents\n",
      "466                    trained-latents_81   24.285714  trained-latents\n",
      "467                    trained-latents_82   34.085714  trained-latents\n",
      "468                    trained-latents_83   81.857143  trained-latents\n",
      "469                    trained-latents_84   40.914286  trained-latents\n",
      "470                    trained-latents_85   42.885714  trained-latents\n",
      "471                    trained-latents_86   29.200000  trained-latents\n",
      "472                    trained-latents_87   47.800000  trained-latents\n",
      "473                    trained-latents_88   37.857143  trained-latents\n",
      "474                    trained-latents_89   46.200000  trained-latents\n",
      "475                    trained-latents_90   65.200000  trained-latents\n",
      "476                    trained-latents_91   36.514286  trained-latents\n",
      "477                    trained-latents_92   32.971429  trained-latents\n",
      "478                    trained-latents_93   60.628571  trained-latents\n",
      "479                    trained-latents_94   63.457143  trained-latents\n",
      "480                    trained-latents_95   47.857143  trained-latents\n",
      "481                    trained-latents_96   58.371429  trained-latents\n",
      "482                    trained-latents_97   33.228571  trained-latents\n",
      "483                    trained-latents_98   44.285714  trained-latents\n",
      "484                    trained-latents_99   80.228571  trained-latents\n",
      "485                   trained-latents_100   41.457143  trained-latents\n",
      "486                   trained-latents_101   48.142857  trained-latents\n",
      "487                   trained-latents_102   40.885714  trained-latents\n",
      "488                   trained-latents_103   36.485714  trained-latents\n",
      "489                   trained-latents_104   40.428571  trained-latents\n",
      "490                   trained-latents_105   47.800000  trained-latents\n",
      "491                   trained-latents_106   40.657143  trained-latents\n",
      "492                   trained-latents_107   41.828571  trained-latents\n",
      "493                   trained-latents_108   50.685714  trained-latents\n",
      "494                   trained-latents_109   26.914286  trained-latents\n",
      "495                   trained-latents_110   53.657143  trained-latents\n",
      "496                   trained-latents_111   44.200000  trained-latents\n",
      "497                   trained-latents_112   57.028571  trained-latents\n",
      "498                   trained-latents_113   48.571429  trained-latents\n",
      "499                   trained-latents_114   37.657143  trained-latents\n",
      "500                   trained-latents_115   34.542857  trained-latents\n",
      "501                   trained-latents_116   44.085714  trained-latents\n",
      "502                   trained-latents_117   58.657143  trained-latents\n",
      "503                   trained-latents_118   25.400000  trained-latents\n",
      "504                   trained-latents_119   46.085714  trained-latents\n",
      "505                   trained-latents_120   58.771429  trained-latents\n",
      "506                   trained-latents_121   50.342857  trained-latents\n",
      "507                   trained-latents_122   51.885714  trained-latents\n",
      "508                   trained-latents_123   33.571429  trained-latents\n",
      "509                   trained-latents_124   37.628571  trained-latents\n",
      "510                   trained-latents_125   42.571429  trained-latents\n",
      "511                   trained-latents_126   40.742857  trained-latents\n",
      "512                   trained-latents_127   52.228571  trained-latents\n",
      "513                   trained-latents_128   47.685714  trained-latents\n",
      "514                   trained-latents_129   33.600000  trained-latents\n",
      "515                   trained-latents_130   50.542857  trained-latents\n",
      "516                   trained-latents_131   63.971429  trained-latents\n",
      "517                   trained-latents_132   43.428571  trained-latents\n",
      "518                   trained-latents_133   34.457143  trained-latents\n",
      "519                   trained-latents_134   52.657143  trained-latents\n",
      "520                   trained-latents_135   52.571429  trained-latents\n",
      "521                   trained-latents_136   26.171429  trained-latents\n",
      "522                   trained-latents_137   79.685714  trained-latents\n",
      "523                   trained-latents_138   40.714286  trained-latents\n",
      "524                   trained-latents_139   35.057143  trained-latents\n",
      "525                   trained-latents_140   63.228571  trained-latents\n",
      "526                   trained-latents_141   55.885714  trained-latents\n",
      "527                   trained-latents_142   21.400000  trained-latents\n",
      "528                   trained-latents_143   56.685714  trained-latents\n",
      "529                   trained-latents_144   40.657143  trained-latents\n",
      "530                   trained-latents_145   16.685714  trained-latents\n",
      "531                   trained-latents_146   53.057143  trained-latents\n",
      "532                   trained-latents_147   52.800000  trained-latents\n",
      "533                   trained-latents_148   67.742857  trained-latents\n",
      "534                   trained-latents_149   52.742857  trained-latents\n",
      "535                   trained-latents_150   29.600000  trained-latents\n",
      "536                   trained-latents_151   52.057143  trained-latents\n",
      "537                   trained-latents_152   60.428571  trained-latents\n",
      "538                   trained-latents_153   42.428571  trained-latents\n",
      "539                   trained-latents_154   58.171429  trained-latents\n",
      "540                   trained-latents_155   33.828571  trained-latents\n",
      "541                   trained-latents_156   38.171429  trained-latents\n",
      "542                   trained-latents_157   30.600000  trained-latents\n",
      "543                   trained-latents_158   57.200000  trained-latents\n",
      "544                   trained-latents_159   58.571429  trained-latents\n",
      "545                   trained-latents_160   50.400000  trained-latents\n",
      "546                   trained-latents_161   68.885714  trained-latents\n",
      "547                   trained-latents_162   25.914286  trained-latents\n",
      "548                   trained-latents_163   38.828571  trained-latents\n",
      "549                   trained-latents_164   35.171429  trained-latents\n",
      "550                   trained-latents_165   47.885714  trained-latents\n",
      "551                   trained-latents_166   36.714286  trained-latents\n",
      "552                   trained-latents_167   20.942857  trained-latents\n",
      "553                   trained-latents_168   39.657143  trained-latents\n",
      "554                   trained-latents_169   51.514286  trained-latents\n",
      "555                   trained-latents_170   33.971429  trained-latents\n",
      "556                   trained-latents_171   23.800000  trained-latents\n",
      "557                   trained-latents_172   37.400000  trained-latents\n",
      "558                   trained-latents_173   39.514286  trained-latents\n",
      "559                   trained-latents_174   44.885714  trained-latents\n",
      "560                   trained-latents_175   38.200000  trained-latents\n",
      "561                   trained-latents_176   38.028571  trained-latents\n",
      "562                   trained-latents_177   62.114286  trained-latents\n",
      "563                   trained-latents_178   19.342857  trained-latents\n",
      "564                   trained-latents_179   32.771429  trained-latents\n",
      "565                   trained-latents_180   39.200000  trained-latents\n",
      "566                   trained-latents_181   63.514286  trained-latents\n",
      "567                   trained-latents_182   41.971429  trained-latents\n",
      "568                   trained-latents_183   34.428571  trained-latents\n",
      "569                   trained-latents_184   32.514286  trained-latents\n",
      "570                   trained-latents_185   54.542857  trained-latents\n",
      "571                   trained-latents_186   43.942857  trained-latents\n",
      "572                   trained-latents_187   29.971429  trained-latents\n",
      "573                   trained-latents_188   43.771429  trained-latents\n",
      "574                   trained-latents_189   31.057143  trained-latents\n",
      "575                   trained-latents_190   41.714286  trained-latents\n",
      "576                   trained-latents_191   30.771429  trained-latents\n",
      "577                   trained-latents_192   26.114286  trained-latents\n",
      "578                   trained-latents_193   40.371429  trained-latents\n",
      "579                   trained-latents_194   67.085714  trained-latents\n",
      "580                   trained-latents_195   51.314286  trained-latents\n",
      "581                   trained-latents_196   37.885714  trained-latents\n",
      "582                   trained-latents_197   31.800000  trained-latents\n",
      "583                   trained-latents_198   29.171429  trained-latents\n",
      "584                   trained-latents_199   56.885714  trained-latents\n",
      "585                   trained-latents_200   41.485714  trained-latents\n",
      "586                   trained-latents_201   39.971429  trained-latents\n",
      "587                   trained-latents_202   69.828571  trained-latents\n",
      "588                   trained-latents_203   47.257143  trained-latents\n",
      "589                   trained-latents_204   39.371429  trained-latents\n",
      "590                   trained-latents_205   29.857143  trained-latents\n",
      "591                   trained-latents_206   34.800000  trained-latents\n",
      "592                   trained-latents_207   39.657143  trained-latents\n",
      "593                   trained-latents_208   49.285714  trained-latents\n",
      "594                   trained-latents_209   39.800000  trained-latents\n",
      "595                   trained-latents_210   42.485714  trained-latents\n",
      "596                   trained-latents_211   21.971429  trained-latents\n",
      "597                   trained-latents_212   45.942857  trained-latents\n",
      "598                   trained-latents_213   51.000000  trained-latents\n",
      "599                   trained-latents_214   44.257143  trained-latents\n",
      "600                   trained-latents_215   40.685714  trained-latents\n",
      "601                   trained-latents_216   43.342857  trained-latents\n",
      "602                   trained-latents_217   47.342857  trained-latents\n",
      "603                   trained-latents_218   40.514286  trained-latents\n",
      "604                   trained-latents_219   26.428571  trained-latents\n",
      "605                   trained-latents_220   26.857143  trained-latents\n",
      "606                   trained-latents_221   30.114286  trained-latents\n",
      "607                   trained-latents_222   48.371429  trained-latents\n",
      "608                   trained-latents_223   47.257143  trained-latents\n",
      "609                   trained-latents_224   28.571429  trained-latents\n",
      "610                   trained-latents_225   45.885714  trained-latents\n",
      "611                   trained-latents_226   69.428571  trained-latents\n",
      "612                   trained-latents_227   66.114286  trained-latents\n",
      "613                   trained-latents_228   39.857143  trained-latents\n",
      "614                   trained-latents_229   31.571429  trained-latents\n",
      "615                   trained-latents_230   20.000000  trained-latents\n",
      "616                   trained-latents_231   41.828571  trained-latents\n",
      "617                   trained-latents_232   49.914286  trained-latents\n",
      "618                   trained-latents_233   37.142857  trained-latents\n",
      "619                   trained-latents_234   19.657143  trained-latents\n",
      "620                   trained-latents_235   22.828571  trained-latents\n",
      "621                   trained-latents_236   33.514286  trained-latents\n",
      "622                   trained-latents_237   26.028571  trained-latents\n",
      "623                   trained-latents_238   53.657143  trained-latents\n",
      "624                   trained-latents_239   46.314286  trained-latents\n",
      "625                   trained-latents_240   33.857143  trained-latents\n",
      "626                   trained-latents_241   67.742857  trained-latents\n",
      "627                   trained-latents_242   21.485714  trained-latents\n",
      "628                   trained-latents_243   40.885714  trained-latents\n",
      "629                   trained-latents_244   50.514286  trained-latents\n",
      "630                   trained-latents_245   31.171429  trained-latents\n",
      "631                   trained-latents_246   37.085714  trained-latents\n",
      "632                   trained-latents_247   60.057143  trained-latents\n",
      "633                   trained-latents_248   40.000000  trained-latents\n",
      "634                   trained-latents_249   32.714286  trained-latents\n",
      "635                   trained-latents_250   20.828571  trained-latents\n",
      "636                   trained-latents_251  110.342857  trained-latents\n",
      "637                   trained-latents_252   55.314286  trained-latents\n",
      "638                   trained-latents_253   27.428571  trained-latents\n",
      "639                   trained-latents_254   41.771429  trained-latents\n",
      "640                   trained-latents_255   42.400000  trained-latents\n",
      "641                   trained-latents_256   32.314286  trained-latents\n",
      "642                   trained-latents_257   47.771429  trained-latents\n",
      "643                   trained-latents_258   38.857143  trained-latents\n",
      "644                   trained-latents_259   34.400000  trained-latents\n",
      "645                   trained-latents_260   39.914286  trained-latents\n",
      "646                   trained-latents_261   25.457143  trained-latents\n",
      "647                   trained-latents_262   38.228571  trained-latents\n",
      "648                   trained-latents_263   49.028571  trained-latents\n",
      "649                   trained-latents_264   38.028571  trained-latents\n",
      "650                   trained-latents_265   25.400000  trained-latents\n",
      "651                   trained-latents_266   32.600000  trained-latents\n",
      "652                   trained-latents_267   22.828571  trained-latents\n",
      "653                   trained-latents_268   39.800000  trained-latents\n",
      "654                   trained-latents_269   55.971429  trained-latents\n",
      "655                   trained-latents_270   45.571429  trained-latents\n",
      "656                   trained-latents_271   37.000000  trained-latents\n",
      "657                   trained-latents_272   42.085714  trained-latents\n",
      "658                   trained-latents_273   45.857143  trained-latents\n",
      "659                   trained-latents_274   78.828571  trained-latents\n",
      "660                   trained-latents_275   65.885714  trained-latents\n",
      "661                   trained-latents_276   58.257143  trained-latents\n",
      "662                   trained-latents_277   29.857143  trained-latents\n",
      "663                   trained-latents_278   68.800000  trained-latents\n",
      "664                   trained-latents_279   19.885714  trained-latents\n",
      "665                   trained-latents_280   75.314286  trained-latents\n",
      "666                   trained-latents_281   22.400000  trained-latents\n",
      "667                   trained-latents_282   33.828571  trained-latents\n",
      "668                   trained-latents_283   30.400000  trained-latents\n",
      "669                   trained-latents_284   60.657143  trained-latents\n",
      "670                   trained-latents_285   41.085714  trained-latents\n",
      "671                   trained-latents_286   41.257143  trained-latents\n",
      "672                   trained-latents_287   19.514286  trained-latents\n",
      "673                   trained-latents_288   73.342857  trained-latents\n",
      "674                   trained-latents_289   42.514286  trained-latents\n",
      "675                   trained-latents_290   24.542857  trained-latents\n",
      "676                   trained-latents_291   90.342857  trained-latents\n",
      "677                   trained-latents_292  104.628571  trained-latents\n",
      "678                   trained-latents_293   80.342857  trained-latents\n",
      "679                   trained-latents_294  120.342857  trained-latents\n",
      "680                   trained-latents_295   63.314286  trained-latents\n",
      "681                   trained-latents_296   58.942857  trained-latents\n",
      "682                   trained-latents_297   82.714286  trained-latents\n",
      "683                   trained-latents_298   83.371429  trained-latents\n",
      "684                   trained-latents_299  108.857143  trained-latents\n",
      "685                   trained-latents_300   98.571429  trained-latents\n",
      "686                   trained-latents_301   52.914286  trained-latents\n",
      "687                   trained-latents_302  152.457143  trained-latents\n",
      "688                   trained-latents_303  111.371429  trained-latents\n",
      "689                   trained-latents_304   73.285714  trained-latents\n",
      "690                   trained-latents_305  118.285714  trained-latents\n",
      "691                   trained-latents_306   61.000000  trained-latents\n",
      "692                   trained-latents_307   74.371429  trained-latents\n",
      "693                   trained-latents_308  131.771429  trained-latents\n",
      "694                   trained-latents_309   88.400000  trained-latents\n",
      "695                   trained-latents_310   55.342857  trained-latents\n",
      "696                   trained-latents_311   94.885714  trained-latents\n",
      "697                   trained-latents_312   62.142857  trained-latents\n",
      "698                   trained-latents_313   92.200000  trained-latents\n",
      "699                   trained-latents_314  114.257143  trained-latents\n",
      "700                   trained-latents_315   68.828571  trained-latents\n",
      "701                   trained-latents_316   71.285714  trained-latents\n",
      "702                   trained-latents_317   85.400000  trained-latents\n",
      "703                   trained-latents_318   85.542857  trained-latents\n",
      "704                   trained-latents_319   77.142857  trained-latents\n",
      "705                   trained-latents_320   80.600000  trained-latents\n",
      "706                   trained-latents_321   46.771429  trained-latents\n",
      "707                   trained-latents_322   88.971429  trained-latents\n",
      "708                   trained-latents_323   88.028571  trained-latents\n",
      "709                   trained-latents_324   94.885714  trained-latents\n",
      "710                   trained-latents_325  100.428571  trained-latents\n",
      "711                   trained-latents_326   51.571429  trained-latents\n",
      "712                   trained-latents_327  101.914286  trained-latents\n",
      "713                   trained-latents_328   83.714286  trained-latents\n",
      "714                   trained-latents_329  138.171429  trained-latents\n",
      "715                   trained-latents_330  100.114286  trained-latents\n",
      "716                   trained-latents_331   54.914286  trained-latents\n",
      "717                   trained-latents_332   68.342857  trained-latents\n",
      "718                   trained-latents_333   83.028571  trained-latents\n",
      "719                   trained-latents_334   77.085714  trained-latents\n",
      "720                   trained-latents_335   61.428571  trained-latents\n",
      "721                   trained-latents_336   74.028571  trained-latents\n",
      "722                   trained-latents_337   58.114286  trained-latents\n",
      "723                   trained-latents_338   53.371429  trained-latents\n",
      "724                   trained-latents_339   74.771429  trained-latents\n",
      "725                   trained-latents_340   68.285714  trained-latents\n",
      "726                   trained-latents_341   63.742857  trained-latents\n",
      "727                   trained-latents_342   81.828571  trained-latents\n",
      "728                   trained-latents_343  120.371429  trained-latents\n",
      "729                   trained-latents_344   64.914286  trained-latents\n",
      "730                   trained-latents_345  131.742857  trained-latents\n",
      "731                   trained-latents_346   66.085714  trained-latents\n",
      "732                   trained-latents_347   70.257143  trained-latents\n",
      "733                   trained-latents_348   56.457143  trained-latents\n",
      "734                   trained-latents_349   92.257143  trained-latents\n",
      "735                   trained-latents_350   71.485714  trained-latents\n",
      "736                   trained-latents_351   87.400000  trained-latents\n",
      "737                   trained-latents_352   84.314286  trained-latents\n",
      "738                   trained-latents_353  100.542857  trained-latents\n",
      "739                   trained-latents_354   65.857143  trained-latents\n",
      "740                   trained-latents_355   75.914286  trained-latents\n",
      "741                   trained-latents_356   74.571429  trained-latents\n",
      "742                   trained-latents_357   37.657143  trained-latents\n",
      "743                   trained-latents_358  106.485714  trained-latents\n",
      "744                   trained-latents_359   45.228571  trained-latents\n",
      "745                   trained-latents_360   78.571429  trained-latents\n",
      "746                   trained-latents_361   66.457143  trained-latents\n",
      "747                   trained-latents_362   40.628571  trained-latents\n",
      "748                   trained-latents_363   80.714286  trained-latents\n",
      "749                   trained-latents_364   46.342857  trained-latents\n",
      "750                   trained-latents_365   45.600000  trained-latents\n",
      "751                   trained-latents_366   60.657143  trained-latents\n",
      "752                   trained-latents_367   58.000000  trained-latents\n",
      "753                   trained-latents_368   84.457143  trained-latents\n",
      "754                   trained-latents_369  119.571429  trained-latents\n",
      "755                   trained-latents_370   89.114286  trained-latents\n",
      "756                   trained-latents_371   82.342857  trained-latents\n",
      "757                   trained-latents_372   98.400000  trained-latents\n",
      "758                   trained-latents_373   58.571429  trained-latents\n",
      "759                   trained-latents_374   66.857143  trained-latents\n",
      "760                   trained-latents_375   73.714286  trained-latents\n",
      "761                   trained-latents_376   83.142857  trained-latents\n",
      "762                   trained-latents_377   80.142857  trained-latents\n",
      "763                   trained-latents_378   97.257143  trained-latents\n",
      "764                   trained-latents_379   83.428571  trained-latents\n",
      "765                   trained-latents_380   76.828571  trained-latents\n",
      "766                   trained-latents_381   60.600000  trained-latents\n",
      "767                   trained-latents_382   68.828571  trained-latents\n",
      "768                   trained-latents_383   80.742857  trained-latents\n",
      "769                     subtile4_ch0_mean   89.371429         subtile4\n",
      "770                      subtile4_ch0_std   84.742857         subtile4\n",
      "771                      subtile4_ch0_min  111.542857         subtile4\n",
      "772                      subtile4_ch0_max   89.057143         subtile4\n",
      "773                     subtile4_ch1_mean   74.828571         subtile4\n",
      "774                      subtile4_ch1_std   70.400000         subtile4\n",
      "775                      subtile4_ch1_min   55.142857         subtile4\n",
      "776                      subtile4_ch1_max   72.600000         subtile4\n",
      "777                     subtile4_ch2_mean   64.771429         subtile4\n",
      "778                      subtile4_ch2_std   81.171429         subtile4\n",
      "779                      subtile4_ch2_min   99.571429         subtile4\n",
      "780                      subtile4_ch2_max   61.457143         subtile4\n",
      "781                   exsubtiles_ch0_mean   94.685714       exsubtiles\n",
      "782                    exsubtiles_ch0_std   42.771429       exsubtiles\n",
      "783                    exsubtiles_ch0_min   97.571429       exsubtiles\n",
      "784                    exsubtiles_ch0_max   93.771429       exsubtiles\n",
      "785                   exsubtiles_ch1_mean   78.400000       exsubtiles\n",
      "786                    exsubtiles_ch1_std   45.257143       exsubtiles\n",
      "787                    exsubtiles_ch1_min   90.942857       exsubtiles\n",
      "788                    exsubtiles_ch1_max  145.800000       exsubtiles\n",
      "789                   exsubtiles_ch2_mean   61.000000       exsubtiles\n",
      "790                    exsubtiles_ch2_std   72.028571       exsubtiles\n",
      "791                    exsubtiles_ch2_min   68.485714       exsubtiles\n",
      "792                    exsubtiles_ch2_max   91.400000       exsubtiles\n",
      "793                         tile_ch0_mean   50.828571             tile\n",
      "794                          tile_ch0_std   70.514286             tile\n",
      "795                          tile_ch0_min   95.057143             tile\n",
      "796                          tile_ch0_max   61.514286             tile\n",
      "797                         tile_ch1_mean   84.885714             tile\n",
      "798                          tile_ch1_std   87.828571             tile\n",
      "799                          tile_ch1_min   86.400000             tile\n",
      "800                          tile_ch1_max  107.942857             tile\n",
      "801                         tile_ch2_mean   90.542857             tile\n",
      "802                          tile_ch2_std   99.714286             tile\n",
      "803                          tile_ch2_min   99.371429             tile\n",
      "804                          tile_ch2_max   22.285714             tile\n",
      "805                     contrast_diff_ch0   78.971429         contrast\n",
      "806                     contrast_diff_ch1   56.457143         contrast\n",
      "807                     contrast_diff_ch2   40.771429         contrast\n",
      "808              wavelet-tile_approx_mean   32.771429     wavelet-tile\n",
      "809               wavelet-tile_approx_std  108.400000     wavelet-tile\n",
      "810               wavelet-tile_approx_min   77.200000     wavelet-tile\n",
      "811               wavelet-tile_approx_max   67.371429     wavelet-tile\n",
      "812        wavelet_tile_level1_band0_mean   39.000000          wavelet\n",
      "813         wavelet_tile_level1_band0_std   81.428571          wavelet\n",
      "814         wavelet_tile_level1_band0_min   87.314286          wavelet\n",
      "815         wavelet_tile_level1_band0_max   42.171429          wavelet\n",
      "816        wavelet_tile_level1_band1_mean   36.914286          wavelet\n",
      "817         wavelet_tile_level1_band1_std   59.628571          wavelet\n",
      "818         wavelet_tile_level1_band1_min   67.828571          wavelet\n",
      "819         wavelet_tile_level1_band1_max   24.085714          wavelet\n",
      "820        wavelet_tile_level1_band2_mean   42.628571          wavelet\n",
      "821         wavelet_tile_level1_band2_std  101.542857          wavelet\n",
      "822         wavelet_tile_level1_band2_min   81.314286          wavelet\n",
      "823         wavelet_tile_level1_band2_max   38.142857          wavelet\n",
      "824        wavelet_tile_level2_band0_mean   45.657143          wavelet\n",
      "825         wavelet_tile_level2_band0_std   60.400000          wavelet\n",
      "826         wavelet_tile_level2_band0_min   86.657143          wavelet\n",
      "827         wavelet_tile_level2_band0_max   53.228571          wavelet\n",
      "828        wavelet_tile_level2_band1_mean   26.571429          wavelet\n",
      "829         wavelet_tile_level2_band1_std   49.114286          wavelet\n",
      "830         wavelet_tile_level2_band1_min   51.714286          wavelet\n",
      "831         wavelet_tile_level2_band1_max   18.571429          wavelet\n",
      "832        wavelet_tile_level2_band2_mean   40.114286          wavelet\n",
      "833         wavelet_tile_level2_band2_std   97.771429          wavelet\n",
      "834         wavelet_tile_level2_band2_min   77.885714          wavelet\n",
      "835         wavelet_tile_level2_band2_max   34.571429          wavelet\n",
      "836         wavelet-subtile_0_approx_mean   35.371429  wavelet-subtile\n",
      "837          wavelet-subtile_0_approx_std   53.085714  wavelet-subtile\n",
      "838          wavelet-subtile_0_approx_min   81.971429  wavelet-subtile\n",
      "839          wavelet-subtile_0_approx_max   66.371429  wavelet-subtile\n",
      "840   wavelet_subtile_0_level1_band0_mean  133.685714          wavelet\n",
      "841    wavelet_subtile_0_level1_band0_std  160.885714          wavelet\n",
      "842    wavelet_subtile_0_level1_band0_min  137.114286          wavelet\n",
      "843    wavelet_subtile_0_level1_band0_max   20.314286          wavelet\n",
      "844   wavelet_subtile_0_level1_band1_mean   74.457143          wavelet\n",
      "845    wavelet_subtile_0_level1_band1_std  167.342857          wavelet\n",
      "846    wavelet_subtile_0_level1_band1_min  259.514286          wavelet\n",
      "847    wavelet_subtile_0_level1_band1_max  278.857143          wavelet\n",
      "848   wavelet_subtile_0_level1_band2_mean  159.114286          wavelet\n",
      "849    wavelet_subtile_0_level1_band2_std  231.914286          wavelet\n",
      "850    wavelet_subtile_0_level1_band2_min  229.628571          wavelet\n",
      "851    wavelet_subtile_0_level1_band2_max  278.400000          wavelet\n",
      "852   wavelet_subtile_0_level2_band0_mean  175.114286          wavelet\n",
      "853    wavelet_subtile_0_level2_band0_std  215.885714          wavelet\n",
      "854    wavelet_subtile_0_level2_band0_min  226.771429          wavelet\n",
      "855    wavelet_subtile_0_level2_band0_max  312.742857          wavelet\n",
      "856   wavelet_subtile_0_level2_band1_mean  117.371429          wavelet\n",
      "857    wavelet_subtile_0_level2_band1_std  173.628571          wavelet\n",
      "858    wavelet_subtile_0_level2_band1_min  163.057143          wavelet\n",
      "859    wavelet_subtile_0_level2_band1_max  323.828571          wavelet\n",
      "860   wavelet_subtile_0_level2_band2_mean  142.571429          wavelet\n",
      "861    wavelet_subtile_0_level2_band2_std  191.857143          wavelet\n",
      "862    wavelet_subtile_0_level2_band2_min  190.342857          wavelet\n",
      "863    wavelet_subtile_0_level2_band2_max  258.485714          wavelet\n",
      "864         wavelet-subtile_1_approx_mean  110.628571  wavelet-subtile\n",
      "865          wavelet-subtile_1_approx_std  189.885714  wavelet-subtile\n",
      "866          wavelet-subtile_1_approx_min  194.742857  wavelet-subtile\n",
      "867          wavelet-subtile_1_approx_max  290.371429  wavelet-subtile\n",
      "868   wavelet_subtile_1_level1_band0_mean  120.571429          wavelet\n",
      "869    wavelet_subtile_1_level1_band0_std  163.000000          wavelet\n",
      "870    wavelet_subtile_1_level1_band0_min  143.371429          wavelet\n",
      "871    wavelet_subtile_1_level1_band0_max   94.028571          wavelet\n",
      "872   wavelet_subtile_1_level1_band1_mean  138.857143          wavelet\n",
      "873    wavelet_subtile_1_level1_band1_std  148.771429          wavelet\n",
      "874    wavelet_subtile_1_level1_band1_min  191.600000          wavelet\n",
      "875    wavelet_subtile_1_level1_band1_max  304.571429          wavelet\n",
      "876   wavelet_subtile_1_level1_band2_mean  200.485714          wavelet\n",
      "877    wavelet_subtile_1_level1_band2_std  231.600000          wavelet\n",
      "878    wavelet_subtile_1_level1_band2_min  248.285714          wavelet\n",
      "879    wavelet_subtile_1_level1_band2_max  300.485714          wavelet\n",
      "880   wavelet_subtile_1_level2_band0_mean  199.257143          wavelet\n",
      "881    wavelet_subtile_1_level2_band0_std  231.457143          wavelet\n",
      "882    wavelet_subtile_1_level2_band0_min  234.428571          wavelet\n",
      "883    wavelet_subtile_1_level2_band0_max  331.485714          wavelet\n",
      "884   wavelet_subtile_1_level2_band1_mean  173.514286          wavelet\n",
      "885    wavelet_subtile_1_level2_band1_std  224.085714          wavelet\n",
      "886    wavelet_subtile_1_level2_band1_min  224.800000          wavelet\n",
      "887    wavelet_subtile_1_level2_band1_max  308.142857          wavelet\n",
      "888   wavelet_subtile_1_level2_band2_mean  143.342857          wavelet\n",
      "889    wavelet_subtile_1_level2_band2_std  208.542857          wavelet\n",
      "890    wavelet_subtile_1_level2_band2_min  198.628571          wavelet\n",
      "891    wavelet_subtile_1_level2_band2_max  308.400000          wavelet\n",
      "892         wavelet-subtile_2_approx_mean  162.342857  wavelet-subtile\n",
      "893          wavelet-subtile_2_approx_std  208.228571  wavelet-subtile\n",
      "894          wavelet-subtile_2_approx_min  201.742857  wavelet-subtile\n",
      "895          wavelet-subtile_2_approx_max  320.428571  wavelet-subtile\n",
      "896   wavelet_subtile_2_level1_band0_mean  132.228571          wavelet\n",
      "897    wavelet_subtile_2_level1_band0_std  162.714286          wavelet\n",
      "898    wavelet_subtile_2_level1_band0_min  169.800000          wavelet\n",
      "899    wavelet_subtile_2_level1_band0_max   85.400000          wavelet\n",
      "900   wavelet_subtile_2_level1_band1_mean  143.514286          wavelet\n",
      "901    wavelet_subtile_2_level1_band1_std  146.428571          wavelet\n",
      "902    wavelet_subtile_2_level1_band1_min  177.514286          wavelet\n",
      "903    wavelet_subtile_2_level1_band1_max  314.457143          wavelet\n",
      "904   wavelet_subtile_2_level1_band2_mean  211.028571          wavelet\n",
      "905    wavelet_subtile_2_level1_band2_std  251.628571          wavelet\n",
      "906    wavelet_subtile_2_level1_band2_min  260.371429          wavelet\n",
      "907    wavelet_subtile_2_level1_band2_max  304.600000          wavelet\n",
      "908   wavelet_subtile_2_level2_band0_mean  186.028571          wavelet\n",
      "909    wavelet_subtile_2_level2_band0_std  224.371429          wavelet\n",
      "910    wavelet_subtile_2_level2_band0_min  241.628571          wavelet\n",
      "911    wavelet_subtile_2_level2_band0_max  317.514286          wavelet\n",
      "912   wavelet_subtile_2_level2_band1_mean  167.457143          wavelet\n",
      "913    wavelet_subtile_2_level2_band1_std  211.171429          wavelet\n",
      "914    wavelet_subtile_2_level2_band1_min  219.571429          wavelet\n",
      "915    wavelet_subtile_2_level2_band1_max  284.200000          wavelet\n",
      "916   wavelet_subtile_2_level2_band2_mean  138.542857          wavelet\n",
      "917    wavelet_subtile_2_level2_band2_std  188.000000          wavelet\n",
      "918    wavelet_subtile_2_level2_band2_min  197.800000          wavelet\n",
      "919    wavelet_subtile_2_level2_band2_max  293.600000          wavelet\n",
      "920         wavelet-subtile_3_approx_mean  147.800000  wavelet-subtile\n",
      "921          wavelet-subtile_3_approx_std  206.085714  wavelet-subtile\n",
      "922          wavelet-subtile_3_approx_min  200.228571  wavelet-subtile\n",
      "923          wavelet-subtile_3_approx_max  303.457143  wavelet-subtile\n",
      "924   wavelet_subtile_3_level1_band0_mean  127.114286          wavelet\n",
      "925    wavelet_subtile_3_level1_band0_std  164.142857          wavelet\n",
      "926    wavelet_subtile_3_level1_band0_min  185.542857          wavelet\n",
      "927    wavelet_subtile_3_level1_band0_max  109.142857          wavelet\n",
      "928   wavelet_subtile_3_level1_band1_mean  152.228571          wavelet\n",
      "929    wavelet_subtile_3_level1_band1_std  159.714286          wavelet\n",
      "930    wavelet_subtile_3_level1_band1_min  215.200000          wavelet\n",
      "931    wavelet_subtile_3_level1_band1_max  311.742857          wavelet\n",
      "932   wavelet_subtile_3_level1_band2_mean  223.971429          wavelet\n",
      "933    wavelet_subtile_3_level1_band2_std  256.828571          wavelet\n",
      "934    wavelet_subtile_3_level1_band2_min  285.200000          wavelet\n",
      "935    wavelet_subtile_3_level1_band2_max  305.314286          wavelet\n",
      "936   wavelet_subtile_3_level2_band0_mean  210.714286          wavelet\n",
      "937    wavelet_subtile_3_level2_band0_std  252.485714          wavelet\n",
      "938    wavelet_subtile_3_level2_band0_min  246.314286          wavelet\n",
      "939    wavelet_subtile_3_level2_band0_max  312.285714          wavelet\n",
      "940   wavelet_subtile_3_level2_band1_mean  166.428571          wavelet\n",
      "941    wavelet_subtile_3_level2_band1_std  223.800000          wavelet\n",
      "942    wavelet_subtile_3_level2_band1_min  226.685714          wavelet\n",
      "943    wavelet_subtile_3_level2_band1_max  315.171429          wavelet\n",
      "944   wavelet_subtile_3_level2_band2_mean  156.914286          wavelet\n",
      "945    wavelet_subtile_3_level2_band2_std  201.885714          wavelet\n",
      "946    wavelet_subtile_3_level2_band2_min  212.228571          wavelet\n",
      "947    wavelet_subtile_3_level2_band2_max  326.228571          wavelet\n",
      "948         wavelet-subtile_4_approx_mean  158.571429  wavelet-subtile\n",
      "949          wavelet-subtile_4_approx_std  212.542857  wavelet-subtile\n",
      "950          wavelet-subtile_4_approx_min  209.085714  wavelet-subtile\n",
      "951          wavelet-subtile_4_approx_max  319.542857  wavelet-subtile\n",
      "952   wavelet_subtile_4_level1_band0_mean  147.514286          wavelet\n",
      "953    wavelet_subtile_4_level1_band0_std  195.828571          wavelet\n",
      "954    wavelet_subtile_4_level1_band0_min  177.800000          wavelet\n",
      "955    wavelet_subtile_4_level1_band0_max   82.342857          wavelet\n",
      "956   wavelet_subtile_4_level1_band1_mean  139.200000          wavelet\n",
      "957    wavelet_subtile_4_level1_band1_std  149.428571          wavelet\n",
      "958    wavelet_subtile_4_level1_band1_min  171.028571          wavelet\n",
      "959    wavelet_subtile_4_level1_band1_max  301.257143          wavelet\n",
      "960   wavelet_subtile_4_level1_band2_mean  185.542857          wavelet\n",
      "961    wavelet_subtile_4_level1_band2_std  246.485714          wavelet\n",
      "962    wavelet_subtile_4_level1_band2_min  246.628571          wavelet\n",
      "963    wavelet_subtile_4_level1_band2_max  311.542857          wavelet\n",
      "964   wavelet_subtile_4_level2_band0_mean  197.228571          wavelet\n",
      "965    wavelet_subtile_4_level2_band0_std  238.971429          wavelet\n",
      "966    wavelet_subtile_4_level2_band0_min  236.571429          wavelet\n",
      "967    wavelet_subtile_4_level2_band0_max  324.571429          wavelet\n",
      "968   wavelet_subtile_4_level2_band1_mean  159.085714          wavelet\n",
      "969    wavelet_subtile_4_level2_band1_std  216.428571          wavelet\n",
      "970    wavelet_subtile_4_level2_band1_min  215.571429          wavelet\n",
      "971    wavelet_subtile_4_level2_band1_max  300.428571          wavelet\n",
      "972   wavelet_subtile_4_level2_band2_mean  143.228571          wavelet\n",
      "973    wavelet_subtile_4_level2_band2_std  200.914286          wavelet\n",
      "974    wavelet_subtile_4_level2_band2_min  208.371429          wavelet\n",
      "975    wavelet_subtile_4_level2_band2_max  300.457143          wavelet\n",
      "976         wavelet-subtile_5_approx_mean  163.885714  wavelet-subtile\n",
      "977          wavelet-subtile_5_approx_std  207.514286  wavelet-subtile\n",
      "978          wavelet-subtile_5_approx_min  203.971429  wavelet-subtile\n",
      "979          wavelet-subtile_5_approx_max  315.314286  wavelet-subtile\n",
      "980   wavelet_subtile_5_level1_band0_mean  128.942857          wavelet\n",
      "981    wavelet_subtile_5_level1_band0_std  163.285714          wavelet\n",
      "982    wavelet_subtile_5_level1_band0_min  162.342857          wavelet\n",
      "983    wavelet_subtile_5_level1_band0_max   26.628571          wavelet\n",
      "984   wavelet_subtile_5_level1_band1_mean  107.971429          wavelet\n",
      "985    wavelet_subtile_5_level1_band1_std  130.714286          wavelet\n",
      "986    wavelet_subtile_5_level1_band1_min  135.171429          wavelet\n",
      "987    wavelet_subtile_5_level1_band1_max  246.942857          wavelet\n",
      "988   wavelet_subtile_5_level1_band2_mean  182.457143          wavelet\n",
      "989    wavelet_subtile_5_level1_band2_std  240.057143          wavelet\n",
      "990    wavelet_subtile_5_level1_band2_min  236.000000          wavelet\n",
      "991    wavelet_subtile_5_level1_band2_max  254.057143          wavelet\n",
      "992   wavelet_subtile_5_level2_band0_mean  172.171429          wavelet\n",
      "993    wavelet_subtile_5_level2_band0_std  248.628571          wavelet\n",
      "994    wavelet_subtile_5_level2_band0_min  235.942857          wavelet\n",
      "995    wavelet_subtile_5_level2_band0_max  324.628571          wavelet\n",
      "996   wavelet_subtile_5_level2_band1_mean  168.800000          wavelet\n",
      "997    wavelet_subtile_5_level2_band1_std  214.685714          wavelet\n",
      "998    wavelet_subtile_5_level2_band1_min  226.714286          wavelet\n",
      "999    wavelet_subtile_5_level2_band1_max  243.257143          wavelet\n",
      "1000  wavelet_subtile_5_level2_band2_mean  148.057143          wavelet\n",
      "1001   wavelet_subtile_5_level2_band2_std  210.371429          wavelet\n",
      "1002   wavelet_subtile_5_level2_band2_min  204.228571          wavelet\n",
      "1003   wavelet_subtile_5_level2_band2_max  249.200000          wavelet\n",
      "1004        wavelet-subtile_6_approx_mean  143.800000  wavelet-subtile\n",
      "1005         wavelet-subtile_6_approx_std  209.171429  wavelet-subtile\n",
      "1006         wavelet-subtile_6_approx_min  201.542857  wavelet-subtile\n",
      "1007         wavelet-subtile_6_approx_max  325.400000  wavelet-subtile\n",
      "1008  wavelet_subtile_6_level1_band0_mean  119.857143          wavelet\n",
      "1009   wavelet_subtile_6_level1_band0_std  170.742857          wavelet\n",
      "1010   wavelet_subtile_6_level1_band0_min  153.342857          wavelet\n",
      "1011   wavelet_subtile_6_level1_band0_max   98.542857          wavelet\n",
      "1012  wavelet_subtile_6_level1_band1_mean  140.914286          wavelet\n",
      "1013   wavelet_subtile_6_level1_band1_std  170.571429          wavelet\n",
      "1014   wavelet_subtile_6_level1_band1_min  213.314286          wavelet\n",
      "1015   wavelet_subtile_6_level1_band1_max  296.885714          wavelet\n",
      "1016  wavelet_subtile_6_level1_band2_mean  192.742857          wavelet\n",
      "1017   wavelet_subtile_6_level1_band2_std  263.142857          wavelet\n",
      "1018   wavelet_subtile_6_level1_band2_min  239.828571          wavelet\n",
      "1019   wavelet_subtile_6_level1_band2_max  300.685714          wavelet\n",
      "1020  wavelet_subtile_6_level2_band0_mean  205.028571          wavelet\n",
      "1021   wavelet_subtile_6_level2_band0_std  255.742857          wavelet\n",
      "1022   wavelet_subtile_6_level2_band0_min  263.428571          wavelet\n",
      "1023   wavelet_subtile_6_level2_band0_max  328.085714          wavelet\n",
      "1024  wavelet_subtile_6_level2_band1_mean  159.257143          wavelet\n",
      "1025   wavelet_subtile_6_level2_band1_std  220.228571          wavelet\n",
      "1026   wavelet_subtile_6_level2_band1_min  236.228571          wavelet\n",
      "1027   wavelet_subtile_6_level2_band1_max  289.885714          wavelet\n",
      "1028  wavelet_subtile_6_level2_band2_mean  157.571429          wavelet\n",
      "1029   wavelet_subtile_6_level2_band2_std  206.428571          wavelet\n",
      "1030   wavelet_subtile_6_level2_band2_min  204.257143          wavelet\n",
      "1031   wavelet_subtile_6_level2_band2_max  279.028571          wavelet\n",
      "1032        wavelet-subtile_7_approx_mean  153.200000  wavelet-subtile\n",
      "1033         wavelet-subtile_7_approx_std  213.257143  wavelet-subtile\n",
      "1034         wavelet-subtile_7_approx_min  193.714286  wavelet-subtile\n",
      "1035         wavelet-subtile_7_approx_max  321.600000  wavelet-subtile\n",
      "1036  wavelet_subtile_7_level1_band0_mean  128.400000          wavelet\n",
      "1037   wavelet_subtile_7_level1_band0_std  170.428571          wavelet\n",
      "1038   wavelet_subtile_7_level1_band0_min  167.971429          wavelet\n",
      "1039   wavelet_subtile_7_level1_band0_max   96.971429          wavelet\n",
      "1040  wavelet_subtile_7_level1_band1_mean  148.514286          wavelet\n",
      "1041   wavelet_subtile_7_level1_band1_std  156.171429          wavelet\n",
      "1042   wavelet_subtile_7_level1_band1_min  192.571429          wavelet\n",
      "1043   wavelet_subtile_7_level1_band1_max  298.971429          wavelet\n",
      "1044  wavelet_subtile_7_level1_band2_mean  188.000000          wavelet\n",
      "1045   wavelet_subtile_7_level1_band2_std  237.228571          wavelet\n",
      "1046   wavelet_subtile_7_level1_band2_min  230.200000          wavelet\n",
      "1047   wavelet_subtile_7_level1_band2_max  329.114286          wavelet\n",
      "1048  wavelet_subtile_7_level2_band0_mean  198.257143          wavelet\n",
      "1049   wavelet_subtile_7_level2_band0_std  246.971429          wavelet\n",
      "1050   wavelet_subtile_7_level2_band0_min  241.657143          wavelet\n",
      "1051   wavelet_subtile_7_level2_band0_max  314.657143          wavelet\n",
      "1052  wavelet_subtile_7_level2_band1_mean  169.800000          wavelet\n",
      "1053   wavelet_subtile_7_level2_band1_std  208.000000          wavelet\n",
      "1054   wavelet_subtile_7_level2_band1_min  218.971429          wavelet\n",
      "1055   wavelet_subtile_7_level2_band1_max  318.000000          wavelet\n",
      "1056  wavelet_subtile_7_level2_band2_mean  162.914286          wavelet\n",
      "1057   wavelet_subtile_7_level2_band2_std  218.200000          wavelet\n",
      "1058   wavelet_subtile_7_level2_band2_min  209.685714          wavelet\n",
      "1059   wavelet_subtile_7_level2_band2_max  305.942857          wavelet\n",
      "1060        wavelet-subtile_8_approx_mean  162.857143  wavelet-subtile\n",
      "1061         wavelet-subtile_8_approx_std  209.085714  wavelet-subtile\n",
      "1062         wavelet-subtile_8_approx_min  205.342857  wavelet-subtile\n",
      "1063         wavelet-subtile_8_approx_max  312.400000  wavelet-subtile\n",
      "1064  wavelet_subtile_8_level1_band0_mean  126.485714          wavelet\n",
      "1065   wavelet_subtile_8_level1_band0_std  178.800000          wavelet\n",
      "1066   wavelet_subtile_8_level1_band0_min  189.457143          wavelet\n",
      "1067   wavelet_subtile_8_level1_band0_max   95.342857          wavelet\n",
      "1068  wavelet_subtile_8_level1_band1_mean  151.800000          wavelet\n",
      "1069   wavelet_subtile_8_level1_band1_std  148.257143          wavelet\n",
      "1070   wavelet_subtile_8_level1_band1_min  190.857143          wavelet\n",
      "1071   wavelet_subtile_8_level1_band1_max  301.114286          wavelet\n",
      "1072  wavelet_subtile_8_level1_band2_mean  188.257143          wavelet\n",
      "1073   wavelet_subtile_8_level1_band2_std  258.914286          wavelet\n",
      "1074   wavelet_subtile_8_level1_band2_min  232.200000          wavelet\n",
      "1075   wavelet_subtile_8_level1_band2_max  314.085714          wavelet\n",
      "1076  wavelet_subtile_8_level2_band0_mean  192.371429          wavelet\n",
      "1077   wavelet_subtile_8_level2_band0_std  239.771429          wavelet\n",
      "1078   wavelet_subtile_8_level2_band0_min  243.971429          wavelet\n",
      "1079   wavelet_subtile_8_level2_band0_max  320.428571          wavelet\n",
      "1080  wavelet_subtile_8_level2_band1_mean  167.657143          wavelet\n",
      "1081   wavelet_subtile_8_level2_band1_std  214.942857          wavelet\n",
      "1082   wavelet_subtile_8_level2_band1_min  217.057143          wavelet\n",
      "1083   wavelet_subtile_8_level2_band1_max  285.514286          wavelet\n",
      "1084  wavelet_subtile_8_level2_band2_mean  155.228571          wavelet\n",
      "1085   wavelet_subtile_8_level2_band2_std  218.257143          wavelet\n",
      "1086   wavelet_subtile_8_level2_band2_min  197.371429          wavelet\n",
      "1087   wavelet_subtile_8_level2_band2_max  286.428571          wavelet\n",
      "1088                      sobel-tile_mean  149.514286       sobel-tile\n",
      "1089                       sobel-tile_std  217.914286       sobel-tile\n",
      "1090                       sobel-tile_min  209.914286       sobel-tile\n",
      "1091                       sobel-tile_max  318.600000       sobel-tile\n",
      "1092                 sobel-subtile_0_mean  124.142857    sobel-subtile\n",
      "1093                  sobel-subtile_0_std  174.542857    sobel-subtile\n",
      "1094                  sobel-subtile_0_min  166.514286    sobel-subtile\n",
      "1095                  sobel-subtile_0_max  104.400000    sobel-subtile\n",
      "1096                 sobel-subtile_1_mean  154.828571    sobel-subtile\n",
      "1097                  sobel-subtile_1_std  171.742857    sobel-subtile\n",
      "1098                  sobel-subtile_1_min  210.971429    sobel-subtile\n",
      "1099                  sobel-subtile_1_max  296.028571    sobel-subtile\n",
      "1100                 sobel-subtile_2_mean  201.057143    sobel-subtile\n",
      "1101                  sobel-subtile_2_std  244.657143    sobel-subtile\n",
      "1102                  sobel-subtile_2_min  229.285714    sobel-subtile\n",
      "1103                  sobel-subtile_2_max  312.085714    sobel-subtile\n",
      "1104                 sobel-subtile_3_mean  210.942857    sobel-subtile\n",
      "1105                  sobel-subtile_3_std  258.514286    sobel-subtile\n",
      "1106                  sobel-subtile_3_min  234.428571    sobel-subtile\n",
      "1107                  sobel-subtile_3_max  299.314286    sobel-subtile\n",
      "1108                 sobel-subtile_4_mean  159.371429    sobel-subtile\n",
      "1109                  sobel-subtile_4_std  205.057143    sobel-subtile\n",
      "1110                  sobel-subtile_4_min  222.057143    sobel-subtile\n",
      "1111                  sobel-subtile_4_max  298.142857    sobel-subtile\n",
      "1112                 sobel-subtile_5_mean  160.314286    sobel-subtile\n",
      "1113                  sobel-subtile_5_std  220.485714    sobel-subtile\n",
      "1114                  sobel-subtile_5_min  210.685714    sobel-subtile\n",
      "1115                  sobel-subtile_5_max  311.628571    sobel-subtile\n",
      "1116                 sobel-subtile_6_mean  136.942857    sobel-subtile\n",
      "1117                  sobel-subtile_6_std  205.428571    sobel-subtile\n",
      "1118                  sobel-subtile_6_min  209.342857    sobel-subtile\n",
      "1119                  sobel-subtile_6_max  328.000000    sobel-subtile\n",
      "1120                 sobel-subtile_7_mean  136.314286    sobel-subtile\n",
      "1121                  sobel-subtile_7_std  169.342857    sobel-subtile\n",
      "1122                  sobel-subtile_7_min  161.285714    sobel-subtile\n",
      "1123                  sobel-subtile_7_max  150.657143    sobel-subtile\n",
      "1124                 sobel-subtile_8_mean   89.857143    sobel-subtile\n",
      "1125                  sobel-subtile_8_std  249.228571    sobel-subtile\n",
      "1126                  sobel-subtile_8_min  204.171429    sobel-subtile\n",
      "1127                  sobel-subtile_8_max  142.200000    sobel-subtile\n",
      "1128                      hsv-tile_H_mean  110.514286         hsv-tile\n",
      "1129                       hsv-tile_H_std  309.200000         hsv-tile\n",
      "1130                       hsv-tile_H_min  142.628571         hsv-tile\n",
      "1131                       hsv-tile_H_max  134.085714         hsv-tile\n",
      "1132                      hsv-tile_S_mean  110.200000         hsv-tile\n",
      "1133                       hsv-tile_S_std  296.942857         hsv-tile\n",
      "1134                       hsv-tile_S_min  149.714286         hsv-tile\n",
      "1135                       hsv-tile_S_max  151.028571         hsv-tile\n",
      "1136                      hsv-tile_V_mean  120.742857         hsv-tile\n",
      "1137                       hsv-tile_V_std  320.885714         hsv-tile\n",
      "1138                       hsv-tile_V_min  157.971429         hsv-tile\n",
      "1139                       hsv-tile_V_max  136.542857         hsv-tile\n",
      "1140                 hsv-subtile_0_H_mean  105.628571      hsv-subtile\n",
      "1141                  hsv-subtile_0_H_std  290.971429      hsv-subtile\n",
      "1142                  hsv-subtile_0_H_min  134.314286      hsv-subtile\n",
      "1143                  hsv-subtile_0_H_max  123.142857      hsv-subtile\n",
      "1144                 hsv-subtile_0_S_mean   93.342857      hsv-subtile\n",
      "1145                  hsv-subtile_0_S_std  291.000000      hsv-subtile\n",
      "1146                  hsv-subtile_0_S_min  147.057143      hsv-subtile\n",
      "1147                  hsv-subtile_0_S_max  134.885714      hsv-subtile\n",
      "1148                 hsv-subtile_0_V_mean  119.542857      hsv-subtile\n",
      "1149                  hsv-subtile_0_V_std  310.571429      hsv-subtile\n",
      "1150                  hsv-subtile_0_V_min  155.571429      hsv-subtile\n",
      "1151                  hsv-subtile_0_V_max  144.400000      hsv-subtile\n",
      "1152                 hsv-subtile_1_H_mean  111.685714      hsv-subtile\n",
      "1153                  hsv-subtile_1_H_std  295.142857      hsv-subtile\n",
      "1154                  hsv-subtile_1_H_min  138.771429      hsv-subtile\n",
      "1155                  hsv-subtile_1_H_max  125.171429      hsv-subtile\n",
      "1156                 hsv-subtile_1_S_mean  106.800000      hsv-subtile\n",
      "1157                  hsv-subtile_1_S_std  301.857143      hsv-subtile\n",
      "1158                  hsv-subtile_1_S_min  143.885714      hsv-subtile\n",
      "1159                  hsv-subtile_1_S_max  123.142857      hsv-subtile\n",
      "1160                 hsv-subtile_1_V_mean  109.400000      hsv-subtile\n",
      "1161                  hsv-subtile_1_V_std  300.571429      hsv-subtile\n",
      "1162                  hsv-subtile_1_V_min  157.000000      hsv-subtile\n",
      "1163                  hsv-subtile_1_V_max   79.028571      hsv-subtile\n",
      "1164                 hsv-subtile_2_H_mean  113.857143      hsv-subtile\n",
      "1165                  hsv-subtile_2_H_std   99.914286      hsv-subtile\n",
      "1166                  hsv-subtile_2_H_min   94.285714      hsv-subtile\n",
      "1167                  hsv-subtile_2_H_max   73.000000      hsv-subtile\n",
      "1168                 hsv-subtile_2_S_mean  112.828571      hsv-subtile\n",
      "1169                  hsv-subtile_2_S_std   81.485714      hsv-subtile\n",
      "1170                  hsv-subtile_2_S_min  187.200000      hsv-subtile\n",
      "1171                  hsv-subtile_2_S_max   24.200000      hsv-subtile\n",
      "1172                 hsv-subtile_2_V_mean   43.742857      hsv-subtile\n",
      "1173                  hsv-subtile_2_V_std   21.628571      hsv-subtile\n",
      "1174                  hsv-subtile_2_V_min    3.542857      hsv-subtile\n",
      "1175                  hsv-subtile_2_V_max  111.285714      hsv-subtile\n",
      "1176                 hsv-subtile_3_H_mean  117.914286      hsv-subtile\n",
      "1177                  hsv-subtile_3_H_std  135.857143      hsv-subtile\n",
      "1178                  hsv-subtile_3_H_min   86.971429      hsv-subtile\n",
      "1179                  hsv-subtile_3_H_max  136.400000      hsv-subtile\n",
      "1180                 hsv-subtile_3_S_mean  137.828571      hsv-subtile\n",
      "1181                  hsv-subtile_3_S_std  150.714286      hsv-subtile\n",
      "1182                  hsv-subtile_3_S_min  140.771429      hsv-subtile\n",
      "1183                  hsv-subtile_3_S_max  115.457143      hsv-subtile\n",
      "1184                 hsv-subtile_3_V_mean  124.628571      hsv-subtile\n",
      "1185                  hsv-subtile_3_V_std  102.857143      hsv-subtile\n",
      "1186                  hsv-subtile_3_V_min   37.028571      hsv-subtile\n",
      "1187                  hsv-subtile_3_V_max   92.314286      hsv-subtile\n",
      "1188                 hsv-subtile_4_H_mean  107.342857      hsv-subtile\n",
      "1189                  hsv-subtile_4_H_std  115.800000      hsv-subtile\n",
      "1190                  hsv-subtile_4_H_min   85.714286      hsv-subtile\n",
      "1191                  hsv-subtile_4_H_max  127.171429      hsv-subtile\n",
      "1192                 hsv-subtile_4_S_mean  129.142857      hsv-subtile\n",
      "1193                  hsv-subtile_4_S_std  123.542857      hsv-subtile\n",
      "1194                  hsv-subtile_4_S_min  142.200000      hsv-subtile\n",
      "1195                  hsv-subtile_4_S_max   97.942857      hsv-subtile\n",
      "1196                 hsv-subtile_4_V_mean  132.285714      hsv-subtile\n",
      "1197                  hsv-subtile_4_V_std  100.028571      hsv-subtile\n",
      "1198                  hsv-subtile_4_V_min   43.657143      hsv-subtile\n",
      "1199                  hsv-subtile_4_V_max  108.542857      hsv-subtile\n",
      "1200                 hsv-subtile_5_H_mean  124.371429      hsv-subtile\n",
      "1201                  hsv-subtile_5_H_std  123.542857      hsv-subtile\n",
      "1202                  hsv-subtile_5_H_min   91.457143      hsv-subtile\n",
      "1203                  hsv-subtile_5_H_max  144.314286      hsv-subtile\n",
      "1204                 hsv-subtile_5_S_mean  142.342857      hsv-subtile\n",
      "1205                  hsv-subtile_5_S_std  142.028571      hsv-subtile\n",
      "1206                  hsv-subtile_5_S_min  155.685714      hsv-subtile\n",
      "1207                  hsv-subtile_5_S_max  128.257143      hsv-subtile\n",
      "1208                 hsv-subtile_5_V_mean  127.828571      hsv-subtile\n",
      "1209                  hsv-subtile_5_V_std  115.628571      hsv-subtile\n",
      "1210                  hsv-subtile_5_V_min   38.457143      hsv-subtile\n",
      "1211                  hsv-subtile_5_V_max   94.485714      hsv-subtile\n",
      "1212                 hsv-subtile_6_H_mean   99.857143      hsv-subtile\n",
      "1213                  hsv-subtile_6_H_std  134.600000      hsv-subtile\n",
      "1214                  hsv-subtile_6_H_min   78.057143      hsv-subtile\n",
      "1215                  hsv-subtile_6_H_max  115.000000      hsv-subtile\n",
      "1216                 hsv-subtile_6_S_mean  123.057143      hsv-subtile\n",
      "1217                  hsv-subtile_6_S_std  130.171429      hsv-subtile\n",
      "1218                  hsv-subtile_6_S_min  137.514286      hsv-subtile\n",
      "1219                  hsv-subtile_6_S_max   92.828571      hsv-subtile\n",
      "1220                 hsv-subtile_6_V_mean  122.257143      hsv-subtile\n",
      "1221                  hsv-subtile_6_V_std   91.028571      hsv-subtile\n",
      "1222                  hsv-subtile_6_V_min   39.371429      hsv-subtile\n",
      "1223                  hsv-subtile_6_V_max   78.542857      hsv-subtile\n",
      "1224                 hsv-subtile_7_H_mean   84.914286      hsv-subtile\n",
      "1225                  hsv-subtile_7_H_std  110.028571      hsv-subtile\n",
      "1226                  hsv-subtile_7_H_min   68.771429      hsv-subtile\n",
      "1227                  hsv-subtile_7_H_max   64.857143      hsv-subtile\n",
      "1228                 hsv-subtile_7_S_mean  105.942857      hsv-subtile\n",
      "1229                  hsv-subtile_7_S_std  103.371429      hsv-subtile\n",
      "1230                  hsv-subtile_7_S_min  126.428571      hsv-subtile\n",
      "1231                  hsv-subtile_7_S_max   21.314286      hsv-subtile\n",
      "1232                 hsv-subtile_7_V_mean   62.457143      hsv-subtile\n",
      "1233                  hsv-subtile_7_V_std   23.000000      hsv-subtile\n",
      "1234                  hsv-subtile_7_V_min   32.171429      hsv-subtile\n",
      "1235                  hsv-subtile_7_V_max  100.171429      hsv-subtile\n",
      "1236                 hsv-subtile_8_H_mean  109.371429      hsv-subtile\n",
      "1237                  hsv-subtile_8_H_std  130.971429      hsv-subtile\n",
      "1238                  hsv-subtile_8_H_min   86.485714      hsv-subtile\n",
      "1239                  hsv-subtile_8_H_max  112.514286      hsv-subtile\n",
      "1240                 hsv-subtile_8_S_mean  126.800000      hsv-subtile\n",
      "1241                  hsv-subtile_8_S_std  146.457143      hsv-subtile\n",
      "1242                  hsv-subtile_8_S_min  130.542857      hsv-subtile\n",
      "1243                  hsv-subtile_8_S_max  106.171429      hsv-subtile\n",
      "1244                 hsv-subtile_8_V_mean  114.142857      hsv-subtile\n",
      "1245                  hsv-subtile_8_V_std  106.371429      hsv-subtile\n",
      "1246                  hsv-subtile_8_V_min   40.028571      hsv-subtile\n",
      "1247                  hsv-subtile_8_V_max  113.571429      hsv-subtile\n",
      "1248                       he-tile_H_mean  128.371429          he-tile\n",
      "1249                        he-tile_H_std  137.542857          he-tile\n",
      "1250                        he-tile_H_min   84.171429          he-tile\n",
      "1251                        he-tile_H_max  142.885714          he-tile\n",
      "1252                       he-tile_E_mean  142.142857          he-tile\n",
      "1253                        he-tile_E_std  144.600000          he-tile\n",
      "1254                        he-tile_E_min  152.828571          he-tile\n",
      "1255                        he-tile_E_max  113.085714          he-tile\n",
      "1256                  he-subtile_0_H_mean  127.542857       he-subtile\n",
      "1257                   he-subtile_0_H_std  108.942857       he-subtile\n",
      "1258                   he-subtile_0_H_min   35.857143       he-subtile\n",
      "1259                   he-subtile_0_H_max  103.857143       he-subtile\n",
      "1260                  he-subtile_0_E_mean  118.200000       he-subtile\n",
      "1261                   he-subtile_0_E_std  126.428571       he-subtile\n",
      "1262                   he-subtile_0_E_min   86.142857       he-subtile\n",
      "1263                   he-subtile_0_E_max  122.200000       he-subtile\n",
      "1264                  he-subtile_1_H_mean  131.285714       he-subtile\n",
      "1265                   he-subtile_1_H_std  138.342857       he-subtile\n",
      "1266                   he-subtile_1_H_min  147.600000       he-subtile\n",
      "1267                   he-subtile_1_H_max  107.114286       he-subtile\n",
      "1268                  he-subtile_1_E_mean  119.142857       he-subtile\n",
      "1269                   he-subtile_1_E_std  109.285714       he-subtile\n",
      "1270                   he-subtile_1_E_min   37.485714       he-subtile\n",
      "1271                   he-subtile_1_E_max  126.257143       he-subtile\n",
      "1272                  he-subtile_2_H_mean  118.257143       he-subtile\n",
      "1273                   he-subtile_2_H_std  134.771429       he-subtile\n",
      "1274                   he-subtile_2_H_min   94.200000       he-subtile\n",
      "1275                   he-subtile_2_H_max  126.114286       he-subtile\n",
      "1276                  he-subtile_2_E_mean  128.400000       he-subtile\n",
      "1277                   he-subtile_2_E_std  140.342857       he-subtile\n",
      "1278                   he-subtile_2_E_min  159.171429       he-subtile\n",
      "1279                   he-subtile_2_E_max  112.971429       he-subtile\n",
      "1280                  he-subtile_3_H_mean  122.828571       he-subtile\n",
      "1281                   he-subtile_3_H_std  110.171429       he-subtile\n",
      "1282                   he-subtile_3_H_min   41.228571       he-subtile\n",
      "1283                   he-subtile_3_H_max   39.542857       he-subtile\n",
      "1284                  he-subtile_3_E_mean   89.571429       he-subtile\n",
      "1285                   he-subtile_3_E_std   31.028571       he-subtile\n",
      "1286                   he-subtile_3_E_min   98.057143       he-subtile\n",
      "1287                   he-subtile_3_E_max  112.171429       he-subtile\n",
      "1288                  he-subtile_4_H_mean  142.514286       he-subtile\n",
      "1289                   he-subtile_4_H_std   17.771429       he-subtile\n",
      "1290                   he-subtile_4_H_min  206.914286       he-subtile\n",
      "1291                   he-subtile_4_H_max   94.857143       he-subtile\n",
      "1292                  he-subtile_4_E_mean  120.485714       he-subtile\n",
      "1293                   he-subtile_4_E_std   91.400000       he-subtile\n",
      "1294                   he-subtile_4_E_min  124.885714       he-subtile\n",
      "1295                   he-subtile_4_E_max  174.485714       he-subtile\n",
      "1296                  he-subtile_5_H_mean  157.114286       he-subtile\n",
      "1297                   he-subtile_5_H_std   52.428571       he-subtile\n",
      "1298                   he-subtile_5_H_min  162.342857       he-subtile\n",
      "1299                   he-subtile_5_H_max   82.171429       he-subtile\n",
      "1300                  he-subtile_5_E_mean  121.371429       he-subtile\n",
      "1301                   he-subtile_5_E_std   91.571429       he-subtile\n",
      "1302                   he-subtile_5_E_min  120.914286       he-subtile\n",
      "1303                   he-subtile_5_E_max  155.028571       he-subtile\n",
      "1304                  he-subtile_6_H_mean  144.600000       he-subtile\n",
      "1305                   he-subtile_6_H_std   48.971429       he-subtile\n",
      "1306                   he-subtile_6_H_min  158.228571       he-subtile\n",
      "1307                   he-subtile_6_H_max  106.028571       he-subtile\n",
      "1308                  he-subtile_6_E_mean  130.657143       he-subtile\n",
      "1309                   he-subtile_6_E_std  103.485714       he-subtile\n",
      "1310                   he-subtile_6_E_min  140.571429       he-subtile\n",
      "1311                   he-subtile_6_E_max  171.800000       he-subtile\n",
      "1312                  he-subtile_7_H_mean  150.200000       he-subtile\n",
      "1313                   he-subtile_7_H_std   62.000000       he-subtile\n",
      "1314                   he-subtile_7_H_min  160.485714       he-subtile\n",
      "1315                   he-subtile_7_H_max   83.485714       he-subtile\n",
      "1316                  he-subtile_7_E_mean  120.600000       he-subtile\n",
      "1317                   he-subtile_7_E_std   90.028571       he-subtile\n",
      "1318                   he-subtile_7_E_min  115.314286       he-subtile\n",
      "1319                   he-subtile_7_E_max  147.742857       he-subtile\n",
      "1320                  he-subtile_8_H_mean  138.085714       he-subtile\n",
      "1321                   he-subtile_8_H_std   51.771429       he-subtile\n",
      "1322                   he-subtile_8_H_min  148.685714       he-subtile\n",
      "1323                   he-subtile_8_H_max   30.314286       he-subtile\n",
      "1324                  he-subtile_8_E_mean   91.457143       he-subtile\n",
      "1325                   he-subtile_8_E_std   53.428571       he-subtile\n",
      "1326                   he-subtile_8_E_min   79.942857       he-subtile\n",
      "1327                   he-subtile_8_E_max  113.742857       he-subtile\n",
      "1328                           oof_pred_0  142.085714              oof\n",
      "1329                           oof_pred_1   50.400000              oof\n",
      "1330                           oof_pred_2  140.971429              oof\n",
      "1331                           oof_pred_3   92.057143              oof\n",
      "1332                           oof_pred_4  122.114286              oof\n",
      "1333                           oof_pred_5  100.942857              oof\n",
      "1334                           oof_pred_6  125.028571              oof\n",
      "1335                           oof_pred_7  138.342857              oof\n",
      "1336                           oof_pred_8  147.285714              oof\n",
      "1337                           oof_pred_9   58.742857              oof\n",
      "1338                          oof_pred_10  152.200000              oof\n",
      "1339                          oof_pred_11   89.542857              oof\n",
      "1340                          oof_pred_12  123.542857              oof\n",
      "1341                          oof_pred_13   90.942857              oof\n",
      "1342                          oof_pred_14  130.828571              oof\n",
      "1343                          oof_pred_15  145.457143              oof\n",
      "1344                          oof_pred_16  157.971429              oof\n",
      "1345                          oof_pred_17   56.371429              oof\n",
      "1346                          oof_pred_18  155.400000              oof\n",
      "1347                          oof_pred_19   86.285714              oof\n",
      "1348                          oof_pred_20  122.685714              oof\n",
      "1349                          oof_pred_21   83.885714              oof\n",
      "1350                          oof_pred_22  134.371429              oof\n",
      "1351                          oof_pred_23  145.571429              oof\n",
      "1352                          oof_pred_24  161.314286              oof\n",
      "1353                          oof_pred_25   51.828571              oof\n",
      "1354                          oof_pred_26  157.714286              oof\n",
      "1355                          oof_pred_27   96.228571              oof\n",
      "1356                          oof_pred_28  130.857143              oof\n",
      "1357                          oof_pred_29   93.685714              oof\n",
      "1358                          oof_pred_30  132.914286              oof\n",
      "1359                          oof_pred_31  145.685714              oof\n",
      "1360                          oof_pred_32  165.457143              oof\n",
      "1361                          oof_pred_33   58.600000              oof\n",
      "1362                          oof_pred_34  160.171429              oof\n",
      "                            feature  importance        category\n",
      "               ae-recon-loss_center   83.028571   ae-recon-loss\n",
      "                            ae_emb0   59.771429              ae\n",
      "                            ae_emb1   75.057143              ae\n",
      "                            ae_emb2  106.942857              ae\n",
      "                            ae_emb3   69.057143              ae\n",
      "                            ae_emb4   43.400000              ae\n",
      "                            ae_emb5   77.971429              ae\n",
      "                            ae_emb6   49.514286              ae\n",
      "                            ae_emb7   46.685714              ae\n",
      "                            ae_emb8   47.742857              ae\n",
      "                            ae_emb9   55.085714              ae\n",
      "                           ae_emb10   67.057143              ae\n",
      "                           ae_emb11   41.714286              ae\n",
      "                           ae_emb12   35.571429              ae\n",
      "                           ae_emb13   60.485714              ae\n",
      "                           ae_emb14   48.742857              ae\n",
      "                           ae_emb15   87.342857              ae\n",
      "                           ae_emb16  113.057143              ae\n",
      "                           ae_emb17   91.457143              ae\n",
      "                           ae_emb18   73.314286              ae\n",
      "                           ae_emb19   70.628571              ae\n",
      "                           ae_emb20   70.971429              ae\n",
      "                           ae_emb21   55.171429              ae\n",
      "                           ae_emb22  100.800000              ae\n",
      "                           ae_emb23   48.971429              ae\n",
      "                           ae_emb24   54.285714              ae\n",
      "                           ae_emb25   44.542857              ae\n",
      "                           ae_emb26   59.657143              ae\n",
      "                           ae_emb27   78.142857              ae\n",
      "                           ae_emb28   63.714286              ae\n",
      "                           ae_emb29   76.285714              ae\n",
      "                           ae_emb30   57.885714              ae\n",
      "                           ae_emb31  112.000000              ae\n",
      "                           ae_emb32  122.285714              ae\n",
      "                           ae_emb33   73.457143              ae\n",
      "                           ae_emb34  100.542857              ae\n",
      "                           ae_emb35  240.400000              ae\n",
      "                           ae_emb36  236.285714              ae\n",
      "                           ae_emb37  240.371429              ae\n",
      "                           ae_emb38   62.571429              ae\n",
      "                           ae_emb39   74.457143              ae\n",
      "                           ae_emb40  144.771429              ae\n",
      "                           ae_emb41   62.971429              ae\n",
      "                           ae_emb42  107.428571              ae\n",
      "                           ae_emb43   74.285714              ae\n",
      "                           ae_emb44   60.942857              ae\n",
      "                           ae_emb45  228.342857              ae\n",
      "                           ae_emb46  205.714286              ae\n",
      "                           ae_emb47  233.371429              ae\n",
      "                           ae_emb48   74.000000              ae\n",
      "                           ae_emb49  188.742857              ae\n",
      "                           ae_emb50   70.114286              ae\n",
      "                           ae_emb51  226.714286              ae\n",
      "                           ae_emb52  225.857143              ae\n",
      "                           ae_emb53  206.914286              ae\n",
      "                           ae_emb54  179.628571              ae\n",
      "                           ae_emb55  192.828571              ae\n",
      "                           ae_emb56  119.314286              ae\n",
      "                           ae_emb57  199.657143              ae\n",
      "                           ae_emb58   70.400000              ae\n",
      "                           ae_emb59  219.600000              ae\n",
      "                           ae_emb60   47.028571              ae\n",
      "                           ae_emb61  230.885714              ae\n",
      "                           ae_emb62  124.457143              ae\n",
      "                           ae_emb63  233.628571              ae\n",
      "                           ae_emb64  242.200000              ae\n",
      "                           ae_emb65  102.685714              ae\n",
      "                           ae_emb66  227.428571              ae\n",
      "                           ae_emb67   99.485714              ae\n",
      "                           ae_emb68  163.657143              ae\n",
      "                           ae_emb69   80.657143              ae\n",
      "                           ae_emb70  159.428571              ae\n",
      "                           ae_emb71   63.085714              ae\n",
      "                           ae_emb72  244.400000              ae\n",
      "                           ae_emb73  163.742857              ae\n",
      "                           ae_emb74  262.742857              ae\n",
      "                           ae_emb75  225.657143              ae\n",
      "                           ae_emb76  164.171429              ae\n",
      "                           ae_emb77  238.171429              ae\n",
      "                           ae_emb78  105.142857              ae\n",
      "                           ae_emb79   96.685714              ae\n",
      "                           ae_emb80   42.971429              ae\n",
      "                           ae_emb81  219.371429              ae\n",
      "                           ae_emb82  222.085714              ae\n",
      "                           ae_emb83  216.914286              ae\n",
      "                           ae_emb84  215.914286              ae\n",
      "                           ae_emb85  223.200000              ae\n",
      "                           ae_emb86   85.885714              ae\n",
      "                           ae_emb87  181.514286              ae\n",
      "                           ae_emb88  231.342857              ae\n",
      "                           ae_emb89  230.942857              ae\n",
      "                           ae_emb90  261.685714              ae\n",
      "                           ae_emb91  101.085714              ae\n",
      "                           ae_emb92  227.742857              ae\n",
      "                           ae_emb93   76.571429              ae\n",
      "                           ae_emb94  146.485714              ae\n",
      "                           ae_emb95  217.428571              ae\n",
      "                           ae_emb96  217.714286              ae\n",
      "                           ae_emb97  194.085714              ae\n",
      "                           ae_emb98  179.114286              ae\n",
      "                           ae_emb99  256.600000              ae\n",
      "                          ae_emb100   62.571429              ae\n",
      "                          ae_emb101   65.371429              ae\n",
      "                          ae_emb102   68.885714              ae\n",
      "                          ae_emb103  188.971429              ae\n",
      "                          ae_emb104  163.885714              ae\n",
      "                          ae_emb105  218.428571              ae\n",
      "                          ae_emb106   66.485714              ae\n",
      "                          ae_emb107   60.257143              ae\n",
      "                          ae_emb108   70.257143              ae\n",
      "                          ae_emb109  233.942857              ae\n",
      "                          ae_emb110  118.600000              ae\n",
      "                          ae_emb111  195.428571              ae\n",
      "                          ae_emb112   54.428571              ae\n",
      "                          ae_emb113   77.428571              ae\n",
      "                          ae_emb114   75.285714              ae\n",
      "                          ae_emb115  205.600000              ae\n",
      "                          ae_emb116   93.342857              ae\n",
      "                          ae_emb117  204.371429              ae\n",
      "                          ae_emb118  147.400000              ae\n",
      "                          ae_emb119  250.028571              ae\n",
      "                          ae_emb120  246.828571              ae\n",
      "                          ae_emb121  155.000000              ae\n",
      "                          ae_emb122   89.942857              ae\n",
      "                          ae_emb123   63.885714              ae\n",
      "                          ae_emb124   98.885714              ae\n",
      "                          ae_emb125   54.514286              ae\n",
      "                          ae_emb126  172.942857              ae\n",
      "                          ae_emb127  201.314286              ae\n",
      "                          ae_emb128   36.171429              ae\n",
      "                          ae_emb129  215.371429              ae\n",
      "                          ae_emb130   73.171429              ae\n",
      "                          ae_emb131   52.028571              ae\n",
      "                          ae_emb132  217.314286              ae\n",
      "                          ae_emb133  232.371429              ae\n",
      "                          ae_emb134  162.285714              ae\n",
      "                          ae_emb135   36.685714              ae\n",
      "                          ae_emb136   71.371429              ae\n",
      "                          ae_emb137  254.914286              ae\n",
      "                          ae_emb138  202.942857              ae\n",
      "                          ae_emb139  236.657143              ae\n",
      "                          ae_emb140   62.971429              ae\n",
      "                          ae_emb141  221.828571              ae\n",
      "                          ae_emb142  171.657143              ae\n",
      "                          ae_emb143  232.628571              ae\n",
      "                          ae_emb144  108.714286              ae\n",
      "                          ae_emb145   45.828571              ae\n",
      "                          ae_emb146  191.685714              ae\n",
      "                          ae_emb147  216.914286              ae\n",
      "                          ae_emb148  255.228571              ae\n",
      "                          ae_emb149  220.714286              ae\n",
      "                          ae_emb150  206.000000              ae\n",
      "                          ae_emb151  244.085714              ae\n",
      "                          ae_emb152  244.857143              ae\n",
      "                          ae_emb153   94.457143              ae\n",
      "                          ae_emb154   95.657143              ae\n",
      "                          ae_emb155  236.000000              ae\n",
      "                          ae_emb156  231.114286              ae\n",
      "                          ae_emb157  205.628571              ae\n",
      "                          ae_emb158   74.885714              ae\n",
      "                          ae_emb159  172.742857              ae\n",
      "                          ae_emb160   52.971429              ae\n",
      "                          ae_emb161   58.600000              ae\n",
      "                          ae_emb162  101.142857              ae\n",
      "                          ae_emb163   42.400000              ae\n",
      "                          ae_emb164  120.057143              ae\n",
      "                          ae_emb165  160.800000              ae\n",
      "                          ae_emb166  194.514286              ae\n",
      "                          ae_emb167   27.628571              ae\n",
      "                          ae_emb168   88.800000              ae\n",
      "                          ae_emb169   81.485714              ae\n",
      "                          ae_emb170   91.571429              ae\n",
      "                          ae_emb171   83.485714              ae\n",
      "                          ae_emb172   43.057143              ae\n",
      "                          ae_emb173  177.800000              ae\n",
      "                          ae_emb174   37.342857              ae\n",
      "                          ae_emb175  193.914286              ae\n",
      "                          ae_emb176   46.114286              ae\n",
      "                          ae_emb177  199.200000              ae\n",
      "                          ae_emb178  128.085714              ae\n",
      "                          ae_emb179   83.857143              ae\n",
      "                          ae_emb180  111.057143              ae\n",
      "                          ae_emb181  142.771429              ae\n",
      "                          ae_emb182  213.200000              ae\n",
      "                          ae_emb183   91.057143              ae\n",
      "                          ae_emb184   98.028571              ae\n",
      "                          ae_emb185   98.600000              ae\n",
      "                          ae_emb186  178.657143              ae\n",
      "                          ae_emb187  145.171429              ae\n",
      "                          ae_emb188  191.514286              ae\n",
      "                          ae_emb189   76.142857              ae\n",
      "                          ae_emb190  120.428571              ae\n",
      "                          ae_emb191  110.885714              ae\n",
      "                          ae_emb192  103.685714              ae\n",
      "                          ae_emb193  202.685714              ae\n",
      "                          ae_emb194   52.057143              ae\n",
      "                          ae_emb195   88.171429              ae\n",
      "                          ae_emb196   88.828571              ae\n",
      "                          ae_emb197   63.942857              ae\n",
      "                          ae_emb198  167.314286              ae\n",
      "                          ae_emb199  117.742857              ae\n",
      "                          ae_emb200  143.142857              ae\n",
      "                          ae_emb201  175.914286              ae\n",
      "                          ae_emb202  187.200000              ae\n",
      "                          ae_emb203  187.828571              ae\n",
      "                          ae_emb204   52.228571              ae\n",
      "                          ae_emb205  174.085714              ae\n",
      "                          ae_emb206   78.514286              ae\n",
      "                          ae_emb207  169.457143              ae\n",
      "                          ae_emb208  172.257143              ae\n",
      "                          ae_emb209   51.371429              ae\n",
      "                          ae_emb210   65.885714              ae\n",
      "                          ae_emb211  109.285714              ae\n",
      "                          ae_emb212   61.971429              ae\n",
      "                          ae_emb213   93.771429              ae\n",
      "                          ae_emb214  183.657143              ae\n",
      "                          ae_emb215  132.400000              ae\n",
      "                          ae_emb216  112.800000              ae\n",
      "                          ae_emb217   86.285714              ae\n",
      "                          ae_emb218   94.942857              ae\n",
      "                          ae_emb219   72.257143              ae\n",
      "                          ae_emb220   85.428571              ae\n",
      "                          ae_emb221   39.742857              ae\n",
      "                          ae_emb222  141.257143              ae\n",
      "                          ae_emb223  117.228571              ae\n",
      "                          ae_emb224   64.828571              ae\n",
      "                          ae_emb225   83.028571              ae\n",
      "                          ae_emb226   53.114286              ae\n",
      "                          ae_emb227   59.628571              ae\n",
      "                          ae_emb228   49.428571              ae\n",
      "                          ae_emb229   60.742857              ae\n",
      "                          ae_emb230   53.171429              ae\n",
      "                          ae_emb231  186.657143              ae\n",
      "                          ae_emb232  171.771429              ae\n",
      "                          ae_emb233  141.257143              ae\n",
      "                          ae_emb234  173.771429              ae\n",
      "                          ae_emb235   50.400000              ae\n",
      "                          ae_emb236   63.514286              ae\n",
      "                          ae_emb237  135.857143              ae\n",
      "                          ae_emb238  184.000000              ae\n",
      "                          ae_emb239   66.742857              ae\n",
      "                          ae_emb240  187.257143              ae\n",
      "                          ae_emb241  122.142857              ae\n",
      "                          ae_emb242  127.285714              ae\n",
      "                          ae_emb243   26.857143              ae\n",
      "                          ae_emb244  124.571429              ae\n",
      "                          ae_emb245   73.857143              ae\n",
      "                          ae_emb246  182.828571              ae\n",
      "                          ae_emb247   99.800000              ae\n",
      "                          ae_emb248  147.685714              ae\n",
      "                          ae_emb249   59.628571              ae\n",
      "                          ae_emb250  160.971429              ae\n",
      "                          ae_emb251   89.000000              ae\n",
      "                          ae_emb252   56.628571              ae\n",
      "                          ae_emb253  173.000000              ae\n",
      "                          ae_emb254  149.342857              ae\n",
      "                          ae_emb255   56.285714              ae\n",
      "                          ae_emb256   53.542857              ae\n",
      "                          ae_emb257  133.657143              ae\n",
      "                          ae_emb258  115.600000              ae\n",
      "                          ae_emb259   89.171429              ae\n",
      "                          ae_emb260  105.771429              ae\n",
      "                          ae_emb261  107.828571              ae\n",
      "                          ae_emb262  175.114286              ae\n",
      "                          ae_emb263   72.685714              ae\n",
      "                          ae_emb264   42.657143              ae\n",
      "                          ae_emb265  122.971429              ae\n",
      "                          ae_emb266   90.600000              ae\n",
      "                          ae_emb267  191.657143              ae\n",
      "                          ae_emb268   27.285714              ae\n",
      "                          ae_emb269   88.171429              ae\n",
      "                          ae_emb270  162.800000              ae\n",
      "                          ae_emb271  143.942857              ae\n",
      "                          ae_emb272  141.257143              ae\n",
      "                          ae_emb273  218.771429              ae\n",
      "                          ae_emb274  176.828571              ae\n",
      "                          ae_emb275   25.857143              ae\n",
      "                          ae_emb276   61.885714              ae\n",
      "                          ae_emb277  188.685714              ae\n",
      "                          ae_emb278  117.285714              ae\n",
      "                          ae_emb279  172.114286              ae\n",
      "                          ae_emb280   68.371429              ae\n",
      "                          ae_emb281  121.514286              ae\n",
      "                          ae_emb282  179.600000              ae\n",
      "                          ae_emb283  141.085714              ae\n",
      "                          ae_emb284  105.400000              ae\n",
      "                          ae_emb285  157.742857              ae\n",
      "                          ae_emb286  127.400000              ae\n",
      "                          ae_emb287   28.457143              ae\n",
      "                          ae_emb288  157.257143              ae\n",
      "                          ae_emb289   44.885714              ae\n",
      "                          ae_emb290   52.457143              ae\n",
      "                          ae_emb291  208.657143              ae\n",
      "                          ae_emb292  203.371429              ae\n",
      "                          ae_emb293  191.857143              ae\n",
      "                          ae_emb294  191.571429              ae\n",
      "                          ae_emb295  165.571429              ae\n",
      "                          ae_emb296  205.514286              ae\n",
      "                          ae_emb297  220.085714              ae\n",
      "                          ae_emb298  179.000000              ae\n",
      "                          ae_emb299  148.657143              ae\n",
      "                          ae_emb300  236.400000              ae\n",
      "                          ae_emb301  187.885714              ae\n",
      "                          ae_emb302  135.828571              ae\n",
      "                          ae_emb303  176.914286              ae\n",
      "                          ae_emb304  166.085714              ae\n",
      "                          ae_emb305  198.542857              ae\n",
      "                          ae_emb306  159.542857              ae\n",
      "                          ae_emb307  175.314286              ae\n",
      "                          ae_emb308  135.657143              ae\n",
      "                          ae_emb309  174.485714              ae\n",
      "                          ae_emb310  197.257143              ae\n",
      "                          ae_emb311  235.000000              ae\n",
      "                          ae_emb312  208.885714              ae\n",
      "                          ae_emb313  184.628571              ae\n",
      "                          ae_emb314  202.571429              ae\n",
      "                          ae_emb315  164.885714              ae\n",
      "                          ae_emb316  210.971429              ae\n",
      "                          ae_emb317  186.428571              ae\n",
      "                          ae_emb318  186.400000              ae\n",
      "                          ae_emb319  227.314286              ae\n",
      "                          ae_emb320  210.714286              ae\n",
      "                          ae_emb321  140.800000              ae\n",
      "                          ae_emb322  131.314286              ae\n",
      "                          ae_emb323  193.342857              ae\n",
      "                          ae_emb324  180.342857              ae\n",
      "                          ae_emb325  155.571429              ae\n",
      "                          ae_emb326  232.457143              ae\n",
      "                          ae_emb327  191.657143              ae\n",
      "                          ae_emb328  201.400000              ae\n",
      "                          ae_emb329  233.085714              ae\n",
      "                          ae_emb330  203.257143              ae\n",
      "                          ae_emb331  188.857143              ae\n",
      "                          ae_emb332  154.828571              ae\n",
      "                          ae_emb333  177.085714              ae\n",
      "                          ae_emb334  182.542857              ae\n",
      "                          ae_emb335  216.000000              ae\n",
      "                          ae_emb336  192.685714              ae\n",
      "                          ae_emb337  230.057143              ae\n",
      "                          ae_emb338  217.057143              ae\n",
      "                          ae_emb339  210.685714              ae\n",
      "                          ae_emb340  154.628571              ae\n",
      "                          ae_emb341  170.142857              ae\n",
      "                          ae_emb342  243.571429              ae\n",
      "                          ae_emb343  175.828571              ae\n",
      "                          ae_emb344  161.828571              ae\n",
      "                          ae_emb345  219.685714              ae\n",
      "                          ae_emb346  209.485714              ae\n",
      "                          ae_emb347  195.085714              ae\n",
      "                          ae_emb348  184.200000              ae\n",
      "                          ae_emb349  180.800000              ae\n",
      "                          ae_emb350  208.742857              ae\n",
      "                          ae_emb351  134.057143              ae\n",
      "                          ae_emb352  201.428571              ae\n",
      "                          ae_emb353  157.342857              ae\n",
      "                          ae_emb354  217.942857              ae\n",
      "                          ae_emb355  206.828571              ae\n",
      "                          ae_emb356  238.000000              ae\n",
      "                          ae_emb357  192.257143              ae\n",
      "                          ae_emb358  185.542857              ae\n",
      "                          ae_emb359  231.028571              ae\n",
      "                          ae_emb360  188.000000              ae\n",
      "                          ae_emb361  193.400000              ae\n",
      "                          ae_emb362  164.142857              ae\n",
      "                          ae_emb363  153.371429              ae\n",
      "                          ae_emb364  197.342857              ae\n",
      "                          ae_emb365  239.342857              ae\n",
      "                          ae_emb366  197.600000              ae\n",
      "                          ae_emb367  142.971429              ae\n",
      "                          ae_emb368  167.742857              ae\n",
      "                          ae_emb369  161.857143              ae\n",
      "                          ae_emb370  232.571429              ae\n",
      "                          ae_emb371  235.457143              ae\n",
      "                          ae_emb372  186.571429              ae\n",
      "                          ae_emb373  156.200000              ae\n",
      "                          ae_emb374  243.457143              ae\n",
      "                          ae_emb375  154.885714              ae\n",
      "                          ae_emb376  157.600000              ae\n",
      "                          ae_emb377  174.114286              ae\n",
      "                          ae_emb378  213.200000              ae\n",
      "                          ae_emb379  182.600000              ae\n",
      "                          ae_emb380  185.657143              ae\n",
      "                          ae_emb381  177.142857              ae\n",
      "                          ae_emb382  214.000000              ae\n",
      "                          ae_emb383  157.228571              ae\n",
      "                  trained-latents_0  170.028571 trained-latents\n",
      "                  trained-latents_1  190.771429 trained-latents\n",
      "                  trained-latents_2  180.228571 trained-latents\n",
      "                  trained-latents_3  156.828571 trained-latents\n",
      "                  trained-latents_4  154.971429 trained-latents\n",
      "                  trained-latents_5  175.857143 trained-latents\n",
      "                  trained-latents_6  171.285714 trained-latents\n",
      "                  trained-latents_7  203.171429 trained-latents\n",
      "                  trained-latents_8  156.028571 trained-latents\n",
      "                  trained-latents_9  206.971429 trained-latents\n",
      "                 trained-latents_10  175.400000 trained-latents\n",
      "                 trained-latents_11  192.057143 trained-latents\n",
      "                 trained-latents_12  139.800000 trained-latents\n",
      "                 trained-latents_13  177.685714 trained-latents\n",
      "                 trained-latents_14  198.428571 trained-latents\n",
      "                 trained-latents_15  171.571429 trained-latents\n",
      "                 trained-latents_16  179.400000 trained-latents\n",
      "                 trained-latents_17  152.114286 trained-latents\n",
      "                 trained-latents_18  178.028571 trained-latents\n",
      "                 trained-latents_19  164.742857 trained-latents\n",
      "                 trained-latents_20  178.428571 trained-latents\n",
      "                 trained-latents_21  210.828571 trained-latents\n",
      "                 trained-latents_22  198.428571 trained-latents\n",
      "                 trained-latents_23  227.828571 trained-latents\n",
      "                 trained-latents_24  182.228571 trained-latents\n",
      "                 trained-latents_25  200.885714 trained-latents\n",
      "                 trained-latents_26  219.600000 trained-latents\n",
      "                 trained-latents_27  190.028571 trained-latents\n",
      "                 trained-latents_28  220.828571 trained-latents\n",
      "                 trained-latents_29  175.314286 trained-latents\n",
      "                 trained-latents_30  187.742857 trained-latents\n",
      "                 trained-latents_31  167.742857 trained-latents\n",
      "                 trained-latents_32  172.742857 trained-latents\n",
      "                 trained-latents_33  190.371429 trained-latents\n",
      "                 trained-latents_34  209.228571 trained-latents\n",
      "                 trained-latents_35   39.142857 trained-latents\n",
      "                 trained-latents_36   67.257143 trained-latents\n",
      "                 trained-latents_37   48.885714 trained-latents\n",
      "                 trained-latents_38   39.314286 trained-latents\n",
      "                 trained-latents_39   44.885714 trained-latents\n",
      "                 trained-latents_40   43.714286 trained-latents\n",
      "                 trained-latents_41   56.885714 trained-latents\n",
      "                 trained-latents_42   62.285714 trained-latents\n",
      "                 trained-latents_43   63.142857 trained-latents\n",
      "                 trained-latents_44   44.314286 trained-latents\n",
      "                 trained-latents_45   30.857143 trained-latents\n",
      "                 trained-latents_46   53.057143 trained-latents\n",
      "                 trained-latents_47   56.428571 trained-latents\n",
      "                 trained-latents_48   68.314286 trained-latents\n",
      "                 trained-latents_49   50.771429 trained-latents\n",
      "                 trained-latents_50   27.171429 trained-latents\n",
      "                 trained-latents_51   27.400000 trained-latents\n",
      "                 trained-latents_52   46.428571 trained-latents\n",
      "                 trained-latents_53   65.885714 trained-latents\n",
      "                 trained-latents_54   39.571429 trained-latents\n",
      "                 trained-latents_55   35.257143 trained-latents\n",
      "                 trained-latents_56   35.057143 trained-latents\n",
      "                 trained-latents_57   41.885714 trained-latents\n",
      "                 trained-latents_58   67.828571 trained-latents\n",
      "                 trained-latents_59   48.057143 trained-latents\n",
      "                 trained-latents_60   48.200000 trained-latents\n",
      "                 trained-latents_61   48.400000 trained-latents\n",
      "                 trained-latents_62   35.514286 trained-latents\n",
      "                 trained-latents_63   49.514286 trained-latents\n",
      "                 trained-latents_64   53.485714 trained-latents\n",
      "                 trained-latents_65   56.428571 trained-latents\n",
      "                 trained-latents_66   53.714286 trained-latents\n",
      "                 trained-latents_67   53.457143 trained-latents\n",
      "                 trained-latents_68   34.457143 trained-latents\n",
      "                 trained-latents_69   52.400000 trained-latents\n",
      "                 trained-latents_70   47.114286 trained-latents\n",
      "                 trained-latents_71   37.200000 trained-latents\n",
      "                 trained-latents_72   31.942857 trained-latents\n",
      "                 trained-latents_73   57.171429 trained-latents\n",
      "                 trained-latents_74   54.457143 trained-latents\n",
      "                 trained-latents_75   56.028571 trained-latents\n",
      "                 trained-latents_76   41.314286 trained-latents\n",
      "                 trained-latents_77   58.057143 trained-latents\n",
      "                 trained-latents_78   56.257143 trained-latents\n",
      "                 trained-latents_79   47.771429 trained-latents\n",
      "                 trained-latents_80   43.000000 trained-latents\n",
      "                 trained-latents_81   24.285714 trained-latents\n",
      "                 trained-latents_82   34.085714 trained-latents\n",
      "                 trained-latents_83   81.857143 trained-latents\n",
      "                 trained-latents_84   40.914286 trained-latents\n",
      "                 trained-latents_85   42.885714 trained-latents\n",
      "                 trained-latents_86   29.200000 trained-latents\n",
      "                 trained-latents_87   47.800000 trained-latents\n",
      "                 trained-latents_88   37.857143 trained-latents\n",
      "                 trained-latents_89   46.200000 trained-latents\n",
      "                 trained-latents_90   65.200000 trained-latents\n",
      "                 trained-latents_91   36.514286 trained-latents\n",
      "                 trained-latents_92   32.971429 trained-latents\n",
      "                 trained-latents_93   60.628571 trained-latents\n",
      "                 trained-latents_94   63.457143 trained-latents\n",
      "                 trained-latents_95   47.857143 trained-latents\n",
      "                 trained-latents_96   58.371429 trained-latents\n",
      "                 trained-latents_97   33.228571 trained-latents\n",
      "                 trained-latents_98   44.285714 trained-latents\n",
      "                 trained-latents_99   80.228571 trained-latents\n",
      "                trained-latents_100   41.457143 trained-latents\n",
      "                trained-latents_101   48.142857 trained-latents\n",
      "                trained-latents_102   40.885714 trained-latents\n",
      "                trained-latents_103   36.485714 trained-latents\n",
      "                trained-latents_104   40.428571 trained-latents\n",
      "                trained-latents_105   47.800000 trained-latents\n",
      "                trained-latents_106   40.657143 trained-latents\n",
      "                trained-latents_107   41.828571 trained-latents\n",
      "                trained-latents_108   50.685714 trained-latents\n",
      "                trained-latents_109   26.914286 trained-latents\n",
      "                trained-latents_110   53.657143 trained-latents\n",
      "                trained-latents_111   44.200000 trained-latents\n",
      "                trained-latents_112   57.028571 trained-latents\n",
      "                trained-latents_113   48.571429 trained-latents\n",
      "                trained-latents_114   37.657143 trained-latents\n",
      "                trained-latents_115   34.542857 trained-latents\n",
      "                trained-latents_116   44.085714 trained-latents\n",
      "                trained-latents_117   58.657143 trained-latents\n",
      "                trained-latents_118   25.400000 trained-latents\n",
      "                trained-latents_119   46.085714 trained-latents\n",
      "                trained-latents_120   58.771429 trained-latents\n",
      "                trained-latents_121   50.342857 trained-latents\n",
      "                trained-latents_122   51.885714 trained-latents\n",
      "                trained-latents_123   33.571429 trained-latents\n",
      "                trained-latents_124   37.628571 trained-latents\n",
      "                trained-latents_125   42.571429 trained-latents\n",
      "                trained-latents_126   40.742857 trained-latents\n",
      "                trained-latents_127   52.228571 trained-latents\n",
      "                trained-latents_128   47.685714 trained-latents\n",
      "                trained-latents_129   33.600000 trained-latents\n",
      "                trained-latents_130   50.542857 trained-latents\n",
      "                trained-latents_131   63.971429 trained-latents\n",
      "                trained-latents_132   43.428571 trained-latents\n",
      "                trained-latents_133   34.457143 trained-latents\n",
      "                trained-latents_134   52.657143 trained-latents\n",
      "                trained-latents_135   52.571429 trained-latents\n",
      "                trained-latents_136   26.171429 trained-latents\n",
      "                trained-latents_137   79.685714 trained-latents\n",
      "                trained-latents_138   40.714286 trained-latents\n",
      "                trained-latents_139   35.057143 trained-latents\n",
      "                trained-latents_140   63.228571 trained-latents\n",
      "                trained-latents_141   55.885714 trained-latents\n",
      "                trained-latents_142   21.400000 trained-latents\n",
      "                trained-latents_143   56.685714 trained-latents\n",
      "                trained-latents_144   40.657143 trained-latents\n",
      "                trained-latents_145   16.685714 trained-latents\n",
      "                trained-latents_146   53.057143 trained-latents\n",
      "                trained-latents_147   52.800000 trained-latents\n",
      "                trained-latents_148   67.742857 trained-latents\n",
      "                trained-latents_149   52.742857 trained-latents\n",
      "                trained-latents_150   29.600000 trained-latents\n",
      "                trained-latents_151   52.057143 trained-latents\n",
      "                trained-latents_152   60.428571 trained-latents\n",
      "                trained-latents_153   42.428571 trained-latents\n",
      "                trained-latents_154   58.171429 trained-latents\n",
      "                trained-latents_155   33.828571 trained-latents\n",
      "                trained-latents_156   38.171429 trained-latents\n",
      "                trained-latents_157   30.600000 trained-latents\n",
      "                trained-latents_158   57.200000 trained-latents\n",
      "                trained-latents_159   58.571429 trained-latents\n",
      "                trained-latents_160   50.400000 trained-latents\n",
      "                trained-latents_161   68.885714 trained-latents\n",
      "                trained-latents_162   25.914286 trained-latents\n",
      "                trained-latents_163   38.828571 trained-latents\n",
      "                trained-latents_164   35.171429 trained-latents\n",
      "                trained-latents_165   47.885714 trained-latents\n",
      "                trained-latents_166   36.714286 trained-latents\n",
      "                trained-latents_167   20.942857 trained-latents\n",
      "                trained-latents_168   39.657143 trained-latents\n",
      "                trained-latents_169   51.514286 trained-latents\n",
      "                trained-latents_170   33.971429 trained-latents\n",
      "                trained-latents_171   23.800000 trained-latents\n",
      "                trained-latents_172   37.400000 trained-latents\n",
      "                trained-latents_173   39.514286 trained-latents\n",
      "                trained-latents_174   44.885714 trained-latents\n",
      "                trained-latents_175   38.200000 trained-latents\n",
      "                trained-latents_176   38.028571 trained-latents\n",
      "                trained-latents_177   62.114286 trained-latents\n",
      "                trained-latents_178   19.342857 trained-latents\n",
      "                trained-latents_179   32.771429 trained-latents\n",
      "                trained-latents_180   39.200000 trained-latents\n",
      "                trained-latents_181   63.514286 trained-latents\n",
      "                trained-latents_182   41.971429 trained-latents\n",
      "                trained-latents_183   34.428571 trained-latents\n",
      "                trained-latents_184   32.514286 trained-latents\n",
      "                trained-latents_185   54.542857 trained-latents\n",
      "                trained-latents_186   43.942857 trained-latents\n",
      "                trained-latents_187   29.971429 trained-latents\n",
      "                trained-latents_188   43.771429 trained-latents\n",
      "                trained-latents_189   31.057143 trained-latents\n",
      "                trained-latents_190   41.714286 trained-latents\n",
      "                trained-latents_191   30.771429 trained-latents\n",
      "                trained-latents_192   26.114286 trained-latents\n",
      "                trained-latents_193   40.371429 trained-latents\n",
      "                trained-latents_194   67.085714 trained-latents\n",
      "                trained-latents_195   51.314286 trained-latents\n",
      "                trained-latents_196   37.885714 trained-latents\n",
      "                trained-latents_197   31.800000 trained-latents\n",
      "                trained-latents_198   29.171429 trained-latents\n",
      "                trained-latents_199   56.885714 trained-latents\n",
      "                trained-latents_200   41.485714 trained-latents\n",
      "                trained-latents_201   39.971429 trained-latents\n",
      "                trained-latents_202   69.828571 trained-latents\n",
      "                trained-latents_203   47.257143 trained-latents\n",
      "                trained-latents_204   39.371429 trained-latents\n",
      "                trained-latents_205   29.857143 trained-latents\n",
      "                trained-latents_206   34.800000 trained-latents\n",
      "                trained-latents_207   39.657143 trained-latents\n",
      "                trained-latents_208   49.285714 trained-latents\n",
      "                trained-latents_209   39.800000 trained-latents\n",
      "                trained-latents_210   42.485714 trained-latents\n",
      "                trained-latents_211   21.971429 trained-latents\n",
      "                trained-latents_212   45.942857 trained-latents\n",
      "                trained-latents_213   51.000000 trained-latents\n",
      "                trained-latents_214   44.257143 trained-latents\n",
      "                trained-latents_215   40.685714 trained-latents\n",
      "                trained-latents_216   43.342857 trained-latents\n",
      "                trained-latents_217   47.342857 trained-latents\n",
      "                trained-latents_218   40.514286 trained-latents\n",
      "                trained-latents_219   26.428571 trained-latents\n",
      "                trained-latents_220   26.857143 trained-latents\n",
      "                trained-latents_221   30.114286 trained-latents\n",
      "                trained-latents_222   48.371429 trained-latents\n",
      "                trained-latents_223   47.257143 trained-latents\n",
      "                trained-latents_224   28.571429 trained-latents\n",
      "                trained-latents_225   45.885714 trained-latents\n",
      "                trained-latents_226   69.428571 trained-latents\n",
      "                trained-latents_227   66.114286 trained-latents\n",
      "                trained-latents_228   39.857143 trained-latents\n",
      "                trained-latents_229   31.571429 trained-latents\n",
      "                trained-latents_230   20.000000 trained-latents\n",
      "                trained-latents_231   41.828571 trained-latents\n",
      "                trained-latents_232   49.914286 trained-latents\n",
      "                trained-latents_233   37.142857 trained-latents\n",
      "                trained-latents_234   19.657143 trained-latents\n",
      "                trained-latents_235   22.828571 trained-latents\n",
      "                trained-latents_236   33.514286 trained-latents\n",
      "                trained-latents_237   26.028571 trained-latents\n",
      "                trained-latents_238   53.657143 trained-latents\n",
      "                trained-latents_239   46.314286 trained-latents\n",
      "                trained-latents_240   33.857143 trained-latents\n",
      "                trained-latents_241   67.742857 trained-latents\n",
      "                trained-latents_242   21.485714 trained-latents\n",
      "                trained-latents_243   40.885714 trained-latents\n",
      "                trained-latents_244   50.514286 trained-latents\n",
      "                trained-latents_245   31.171429 trained-latents\n",
      "                trained-latents_246   37.085714 trained-latents\n",
      "                trained-latents_247   60.057143 trained-latents\n",
      "                trained-latents_248   40.000000 trained-latents\n",
      "                trained-latents_249   32.714286 trained-latents\n",
      "                trained-latents_250   20.828571 trained-latents\n",
      "                trained-latents_251  110.342857 trained-latents\n",
      "                trained-latents_252   55.314286 trained-latents\n",
      "                trained-latents_253   27.428571 trained-latents\n",
      "                trained-latents_254   41.771429 trained-latents\n",
      "                trained-latents_255   42.400000 trained-latents\n",
      "                trained-latents_256   32.314286 trained-latents\n",
      "                trained-latents_257   47.771429 trained-latents\n",
      "                trained-latents_258   38.857143 trained-latents\n",
      "                trained-latents_259   34.400000 trained-latents\n",
      "                trained-latents_260   39.914286 trained-latents\n",
      "                trained-latents_261   25.457143 trained-latents\n",
      "                trained-latents_262   38.228571 trained-latents\n",
      "                trained-latents_263   49.028571 trained-latents\n",
      "                trained-latents_264   38.028571 trained-latents\n",
      "                trained-latents_265   25.400000 trained-latents\n",
      "                trained-latents_266   32.600000 trained-latents\n",
      "                trained-latents_267   22.828571 trained-latents\n",
      "                trained-latents_268   39.800000 trained-latents\n",
      "                trained-latents_269   55.971429 trained-latents\n",
      "                trained-latents_270   45.571429 trained-latents\n",
      "                trained-latents_271   37.000000 trained-latents\n",
      "                trained-latents_272   42.085714 trained-latents\n",
      "                trained-latents_273   45.857143 trained-latents\n",
      "                trained-latents_274   78.828571 trained-latents\n",
      "                trained-latents_275   65.885714 trained-latents\n",
      "                trained-latents_276   58.257143 trained-latents\n",
      "                trained-latents_277   29.857143 trained-latents\n",
      "                trained-latents_278   68.800000 trained-latents\n",
      "                trained-latents_279   19.885714 trained-latents\n",
      "                trained-latents_280   75.314286 trained-latents\n",
      "                trained-latents_281   22.400000 trained-latents\n",
      "                trained-latents_282   33.828571 trained-latents\n",
      "                trained-latents_283   30.400000 trained-latents\n",
      "                trained-latents_284   60.657143 trained-latents\n",
      "                trained-latents_285   41.085714 trained-latents\n",
      "                trained-latents_286   41.257143 trained-latents\n",
      "                trained-latents_287   19.514286 trained-latents\n",
      "                trained-latents_288   73.342857 trained-latents\n",
      "                trained-latents_289   42.514286 trained-latents\n",
      "                trained-latents_290   24.542857 trained-latents\n",
      "                trained-latents_291   90.342857 trained-latents\n",
      "                trained-latents_292  104.628571 trained-latents\n",
      "                trained-latents_293   80.342857 trained-latents\n",
      "                trained-latents_294  120.342857 trained-latents\n",
      "                trained-latents_295   63.314286 trained-latents\n",
      "                trained-latents_296   58.942857 trained-latents\n",
      "                trained-latents_297   82.714286 trained-latents\n",
      "                trained-latents_298   83.371429 trained-latents\n",
      "                trained-latents_299  108.857143 trained-latents\n",
      "                trained-latents_300   98.571429 trained-latents\n",
      "                trained-latents_301   52.914286 trained-latents\n",
      "                trained-latents_302  152.457143 trained-latents\n",
      "                trained-latents_303  111.371429 trained-latents\n",
      "                trained-latents_304   73.285714 trained-latents\n",
      "                trained-latents_305  118.285714 trained-latents\n",
      "                trained-latents_306   61.000000 trained-latents\n",
      "                trained-latents_307   74.371429 trained-latents\n",
      "                trained-latents_308  131.771429 trained-latents\n",
      "                trained-latents_309   88.400000 trained-latents\n",
      "                trained-latents_310   55.342857 trained-latents\n",
      "                trained-latents_311   94.885714 trained-latents\n",
      "                trained-latents_312   62.142857 trained-latents\n",
      "                trained-latents_313   92.200000 trained-latents\n",
      "                trained-latents_314  114.257143 trained-latents\n",
      "                trained-latents_315   68.828571 trained-latents\n",
      "                trained-latents_316   71.285714 trained-latents\n",
      "                trained-latents_317   85.400000 trained-latents\n",
      "                trained-latents_318   85.542857 trained-latents\n",
      "                trained-latents_319   77.142857 trained-latents\n",
      "                trained-latents_320   80.600000 trained-latents\n",
      "                trained-latents_321   46.771429 trained-latents\n",
      "                trained-latents_322   88.971429 trained-latents\n",
      "                trained-latents_323   88.028571 trained-latents\n",
      "                trained-latents_324   94.885714 trained-latents\n",
      "                trained-latents_325  100.428571 trained-latents\n",
      "                trained-latents_326   51.571429 trained-latents\n",
      "                trained-latents_327  101.914286 trained-latents\n",
      "                trained-latents_328   83.714286 trained-latents\n",
      "                trained-latents_329  138.171429 trained-latents\n",
      "                trained-latents_330  100.114286 trained-latents\n",
      "                trained-latents_331   54.914286 trained-latents\n",
      "                trained-latents_332   68.342857 trained-latents\n",
      "                trained-latents_333   83.028571 trained-latents\n",
      "                trained-latents_334   77.085714 trained-latents\n",
      "                trained-latents_335   61.428571 trained-latents\n",
      "                trained-latents_336   74.028571 trained-latents\n",
      "                trained-latents_337   58.114286 trained-latents\n",
      "                trained-latents_338   53.371429 trained-latents\n",
      "                trained-latents_339   74.771429 trained-latents\n",
      "                trained-latents_340   68.285714 trained-latents\n",
      "                trained-latents_341   63.742857 trained-latents\n",
      "                trained-latents_342   81.828571 trained-latents\n",
      "                trained-latents_343  120.371429 trained-latents\n",
      "                trained-latents_344   64.914286 trained-latents\n",
      "                trained-latents_345  131.742857 trained-latents\n",
      "                trained-latents_346   66.085714 trained-latents\n",
      "                trained-latents_347   70.257143 trained-latents\n",
      "                trained-latents_348   56.457143 trained-latents\n",
      "                trained-latents_349   92.257143 trained-latents\n",
      "                trained-latents_350   71.485714 trained-latents\n",
      "                trained-latents_351   87.400000 trained-latents\n",
      "                trained-latents_352   84.314286 trained-latents\n",
      "                trained-latents_353  100.542857 trained-latents\n",
      "                trained-latents_354   65.857143 trained-latents\n",
      "                trained-latents_355   75.914286 trained-latents\n",
      "                trained-latents_356   74.571429 trained-latents\n",
      "                trained-latents_357   37.657143 trained-latents\n",
      "                trained-latents_358  106.485714 trained-latents\n",
      "                trained-latents_359   45.228571 trained-latents\n",
      "                trained-latents_360   78.571429 trained-latents\n",
      "                trained-latents_361   66.457143 trained-latents\n",
      "                trained-latents_362   40.628571 trained-latents\n",
      "                trained-latents_363   80.714286 trained-latents\n",
      "                trained-latents_364   46.342857 trained-latents\n",
      "                trained-latents_365   45.600000 trained-latents\n",
      "                trained-latents_366   60.657143 trained-latents\n",
      "                trained-latents_367   58.000000 trained-latents\n",
      "                trained-latents_368   84.457143 trained-latents\n",
      "                trained-latents_369  119.571429 trained-latents\n",
      "                trained-latents_370   89.114286 trained-latents\n",
      "                trained-latents_371   82.342857 trained-latents\n",
      "                trained-latents_372   98.400000 trained-latents\n",
      "                trained-latents_373   58.571429 trained-latents\n",
      "                trained-latents_374   66.857143 trained-latents\n",
      "                trained-latents_375   73.714286 trained-latents\n",
      "                trained-latents_376   83.142857 trained-latents\n",
      "                trained-latents_377   80.142857 trained-latents\n",
      "                trained-latents_378   97.257143 trained-latents\n",
      "                trained-latents_379   83.428571 trained-latents\n",
      "                trained-latents_380   76.828571 trained-latents\n",
      "                trained-latents_381   60.600000 trained-latents\n",
      "                trained-latents_382   68.828571 trained-latents\n",
      "                trained-latents_383   80.742857 trained-latents\n",
      "                  subtile4_ch0_mean   89.371429        subtile4\n",
      "                   subtile4_ch0_std   84.742857        subtile4\n",
      "                   subtile4_ch0_min  111.542857        subtile4\n",
      "                   subtile4_ch0_max   89.057143        subtile4\n",
      "                  subtile4_ch1_mean   74.828571        subtile4\n",
      "                   subtile4_ch1_std   70.400000        subtile4\n",
      "                   subtile4_ch1_min   55.142857        subtile4\n",
      "                   subtile4_ch1_max   72.600000        subtile4\n",
      "                  subtile4_ch2_mean   64.771429        subtile4\n",
      "                   subtile4_ch2_std   81.171429        subtile4\n",
      "                   subtile4_ch2_min   99.571429        subtile4\n",
      "                   subtile4_ch2_max   61.457143        subtile4\n",
      "                exsubtiles_ch0_mean   94.685714      exsubtiles\n",
      "                 exsubtiles_ch0_std   42.771429      exsubtiles\n",
      "                 exsubtiles_ch0_min   97.571429      exsubtiles\n",
      "                 exsubtiles_ch0_max   93.771429      exsubtiles\n",
      "                exsubtiles_ch1_mean   78.400000      exsubtiles\n",
      "                 exsubtiles_ch1_std   45.257143      exsubtiles\n",
      "                 exsubtiles_ch1_min   90.942857      exsubtiles\n",
      "                 exsubtiles_ch1_max  145.800000      exsubtiles\n",
      "                exsubtiles_ch2_mean   61.000000      exsubtiles\n",
      "                 exsubtiles_ch2_std   72.028571      exsubtiles\n",
      "                 exsubtiles_ch2_min   68.485714      exsubtiles\n",
      "                 exsubtiles_ch2_max   91.400000      exsubtiles\n",
      "                      tile_ch0_mean   50.828571            tile\n",
      "                       tile_ch0_std   70.514286            tile\n",
      "                       tile_ch0_min   95.057143            tile\n",
      "                       tile_ch0_max   61.514286            tile\n",
      "                      tile_ch1_mean   84.885714            tile\n",
      "                       tile_ch1_std   87.828571            tile\n",
      "                       tile_ch1_min   86.400000            tile\n",
      "                       tile_ch1_max  107.942857            tile\n",
      "                      tile_ch2_mean   90.542857            tile\n",
      "                       tile_ch2_std   99.714286            tile\n",
      "                       tile_ch2_min   99.371429            tile\n",
      "                       tile_ch2_max   22.285714            tile\n",
      "                  contrast_diff_ch0   78.971429        contrast\n",
      "                  contrast_diff_ch1   56.457143        contrast\n",
      "                  contrast_diff_ch2   40.771429        contrast\n",
      "           wavelet-tile_approx_mean   32.771429    wavelet-tile\n",
      "            wavelet-tile_approx_std  108.400000    wavelet-tile\n",
      "            wavelet-tile_approx_min   77.200000    wavelet-tile\n",
      "            wavelet-tile_approx_max   67.371429    wavelet-tile\n",
      "     wavelet_tile_level1_band0_mean   39.000000         wavelet\n",
      "      wavelet_tile_level1_band0_std   81.428571         wavelet\n",
      "      wavelet_tile_level1_band0_min   87.314286         wavelet\n",
      "      wavelet_tile_level1_band0_max   42.171429         wavelet\n",
      "     wavelet_tile_level1_band1_mean   36.914286         wavelet\n",
      "      wavelet_tile_level1_band1_std   59.628571         wavelet\n",
      "      wavelet_tile_level1_band1_min   67.828571         wavelet\n",
      "      wavelet_tile_level1_band1_max   24.085714         wavelet\n",
      "     wavelet_tile_level1_band2_mean   42.628571         wavelet\n",
      "      wavelet_tile_level1_band2_std  101.542857         wavelet\n",
      "      wavelet_tile_level1_band2_min   81.314286         wavelet\n",
      "      wavelet_tile_level1_band2_max   38.142857         wavelet\n",
      "     wavelet_tile_level2_band0_mean   45.657143         wavelet\n",
      "      wavelet_tile_level2_band0_std   60.400000         wavelet\n",
      "      wavelet_tile_level2_band0_min   86.657143         wavelet\n",
      "      wavelet_tile_level2_band0_max   53.228571         wavelet\n",
      "     wavelet_tile_level2_band1_mean   26.571429         wavelet\n",
      "      wavelet_tile_level2_band1_std   49.114286         wavelet\n",
      "      wavelet_tile_level2_band1_min   51.714286         wavelet\n",
      "      wavelet_tile_level2_band1_max   18.571429         wavelet\n",
      "     wavelet_tile_level2_band2_mean   40.114286         wavelet\n",
      "      wavelet_tile_level2_band2_std   97.771429         wavelet\n",
      "      wavelet_tile_level2_band2_min   77.885714         wavelet\n",
      "      wavelet_tile_level2_band2_max   34.571429         wavelet\n",
      "      wavelet-subtile_0_approx_mean   35.371429 wavelet-subtile\n",
      "       wavelet-subtile_0_approx_std   53.085714 wavelet-subtile\n",
      "       wavelet-subtile_0_approx_min   81.971429 wavelet-subtile\n",
      "       wavelet-subtile_0_approx_max   66.371429 wavelet-subtile\n",
      "wavelet_subtile_0_level1_band0_mean  133.685714         wavelet\n",
      " wavelet_subtile_0_level1_band0_std  160.885714         wavelet\n",
      " wavelet_subtile_0_level1_band0_min  137.114286         wavelet\n",
      " wavelet_subtile_0_level1_band0_max   20.314286         wavelet\n",
      "wavelet_subtile_0_level1_band1_mean   74.457143         wavelet\n",
      " wavelet_subtile_0_level1_band1_std  167.342857         wavelet\n",
      " wavelet_subtile_0_level1_band1_min  259.514286         wavelet\n",
      " wavelet_subtile_0_level1_band1_max  278.857143         wavelet\n",
      "wavelet_subtile_0_level1_band2_mean  159.114286         wavelet\n",
      " wavelet_subtile_0_level1_band2_std  231.914286         wavelet\n",
      " wavelet_subtile_0_level1_band2_min  229.628571         wavelet\n",
      " wavelet_subtile_0_level1_band2_max  278.400000         wavelet\n",
      "wavelet_subtile_0_level2_band0_mean  175.114286         wavelet\n",
      " wavelet_subtile_0_level2_band0_std  215.885714         wavelet\n",
      " wavelet_subtile_0_level2_band0_min  226.771429         wavelet\n",
      " wavelet_subtile_0_level2_band0_max  312.742857         wavelet\n",
      "wavelet_subtile_0_level2_band1_mean  117.371429         wavelet\n",
      " wavelet_subtile_0_level2_band1_std  173.628571         wavelet\n",
      " wavelet_subtile_0_level2_band1_min  163.057143         wavelet\n",
      " wavelet_subtile_0_level2_band1_max  323.828571         wavelet\n",
      "wavelet_subtile_0_level2_band2_mean  142.571429         wavelet\n",
      " wavelet_subtile_0_level2_band2_std  191.857143         wavelet\n",
      " wavelet_subtile_0_level2_band2_min  190.342857         wavelet\n",
      " wavelet_subtile_0_level2_band2_max  258.485714         wavelet\n",
      "      wavelet-subtile_1_approx_mean  110.628571 wavelet-subtile\n",
      "       wavelet-subtile_1_approx_std  189.885714 wavelet-subtile\n",
      "       wavelet-subtile_1_approx_min  194.742857 wavelet-subtile\n",
      "       wavelet-subtile_1_approx_max  290.371429 wavelet-subtile\n",
      "wavelet_subtile_1_level1_band0_mean  120.571429         wavelet\n",
      " wavelet_subtile_1_level1_band0_std  163.000000         wavelet\n",
      " wavelet_subtile_1_level1_band0_min  143.371429         wavelet\n",
      " wavelet_subtile_1_level1_band0_max   94.028571         wavelet\n",
      "wavelet_subtile_1_level1_band1_mean  138.857143         wavelet\n",
      " wavelet_subtile_1_level1_band1_std  148.771429         wavelet\n",
      " wavelet_subtile_1_level1_band1_min  191.600000         wavelet\n",
      " wavelet_subtile_1_level1_band1_max  304.571429         wavelet\n",
      "wavelet_subtile_1_level1_band2_mean  200.485714         wavelet\n",
      " wavelet_subtile_1_level1_band2_std  231.600000         wavelet\n",
      " wavelet_subtile_1_level1_band2_min  248.285714         wavelet\n",
      " wavelet_subtile_1_level1_band2_max  300.485714         wavelet\n",
      "wavelet_subtile_1_level2_band0_mean  199.257143         wavelet\n",
      " wavelet_subtile_1_level2_band0_std  231.457143         wavelet\n",
      " wavelet_subtile_1_level2_band0_min  234.428571         wavelet\n",
      " wavelet_subtile_1_level2_band0_max  331.485714         wavelet\n",
      "wavelet_subtile_1_level2_band1_mean  173.514286         wavelet\n",
      " wavelet_subtile_1_level2_band1_std  224.085714         wavelet\n",
      " wavelet_subtile_1_level2_band1_min  224.800000         wavelet\n",
      " wavelet_subtile_1_level2_band1_max  308.142857         wavelet\n",
      "wavelet_subtile_1_level2_band2_mean  143.342857         wavelet\n",
      " wavelet_subtile_1_level2_band2_std  208.542857         wavelet\n",
      " wavelet_subtile_1_level2_band2_min  198.628571         wavelet\n",
      " wavelet_subtile_1_level2_band2_max  308.400000         wavelet\n",
      "      wavelet-subtile_2_approx_mean  162.342857 wavelet-subtile\n",
      "       wavelet-subtile_2_approx_std  208.228571 wavelet-subtile\n",
      "       wavelet-subtile_2_approx_min  201.742857 wavelet-subtile\n",
      "       wavelet-subtile_2_approx_max  320.428571 wavelet-subtile\n",
      "wavelet_subtile_2_level1_band0_mean  132.228571         wavelet\n",
      " wavelet_subtile_2_level1_band0_std  162.714286         wavelet\n",
      " wavelet_subtile_2_level1_band0_min  169.800000         wavelet\n",
      " wavelet_subtile_2_level1_band0_max   85.400000         wavelet\n",
      "wavelet_subtile_2_level1_band1_mean  143.514286         wavelet\n",
      " wavelet_subtile_2_level1_band1_std  146.428571         wavelet\n",
      " wavelet_subtile_2_level1_band1_min  177.514286         wavelet\n",
      " wavelet_subtile_2_level1_band1_max  314.457143         wavelet\n",
      "wavelet_subtile_2_level1_band2_mean  211.028571         wavelet\n",
      " wavelet_subtile_2_level1_band2_std  251.628571         wavelet\n",
      " wavelet_subtile_2_level1_band2_min  260.371429         wavelet\n",
      " wavelet_subtile_2_level1_band2_max  304.600000         wavelet\n",
      "wavelet_subtile_2_level2_band0_mean  186.028571         wavelet\n",
      " wavelet_subtile_2_level2_band0_std  224.371429         wavelet\n",
      " wavelet_subtile_2_level2_band0_min  241.628571         wavelet\n",
      " wavelet_subtile_2_level2_band0_max  317.514286         wavelet\n",
      "wavelet_subtile_2_level2_band1_mean  167.457143         wavelet\n",
      " wavelet_subtile_2_level2_band1_std  211.171429         wavelet\n",
      " wavelet_subtile_2_level2_band1_min  219.571429         wavelet\n",
      " wavelet_subtile_2_level2_band1_max  284.200000         wavelet\n",
      "wavelet_subtile_2_level2_band2_mean  138.542857         wavelet\n",
      " wavelet_subtile_2_level2_band2_std  188.000000         wavelet\n",
      " wavelet_subtile_2_level2_band2_min  197.800000         wavelet\n",
      " wavelet_subtile_2_level2_band2_max  293.600000         wavelet\n",
      "      wavelet-subtile_3_approx_mean  147.800000 wavelet-subtile\n",
      "       wavelet-subtile_3_approx_std  206.085714 wavelet-subtile\n",
      "       wavelet-subtile_3_approx_min  200.228571 wavelet-subtile\n",
      "       wavelet-subtile_3_approx_max  303.457143 wavelet-subtile\n",
      "wavelet_subtile_3_level1_band0_mean  127.114286         wavelet\n",
      " wavelet_subtile_3_level1_band0_std  164.142857         wavelet\n",
      " wavelet_subtile_3_level1_band0_min  185.542857         wavelet\n",
      " wavelet_subtile_3_level1_band0_max  109.142857         wavelet\n",
      "wavelet_subtile_3_level1_band1_mean  152.228571         wavelet\n",
      " wavelet_subtile_3_level1_band1_std  159.714286         wavelet\n",
      " wavelet_subtile_3_level1_band1_min  215.200000         wavelet\n",
      " wavelet_subtile_3_level1_band1_max  311.742857         wavelet\n",
      "wavelet_subtile_3_level1_band2_mean  223.971429         wavelet\n",
      " wavelet_subtile_3_level1_band2_std  256.828571         wavelet\n",
      " wavelet_subtile_3_level1_band2_min  285.200000         wavelet\n",
      " wavelet_subtile_3_level1_band2_max  305.314286         wavelet\n",
      "wavelet_subtile_3_level2_band0_mean  210.714286         wavelet\n",
      " wavelet_subtile_3_level2_band0_std  252.485714         wavelet\n",
      " wavelet_subtile_3_level2_band0_min  246.314286         wavelet\n",
      " wavelet_subtile_3_level2_band0_max  312.285714         wavelet\n",
      "wavelet_subtile_3_level2_band1_mean  166.428571         wavelet\n",
      " wavelet_subtile_3_level2_band1_std  223.800000         wavelet\n",
      " wavelet_subtile_3_level2_band1_min  226.685714         wavelet\n",
      " wavelet_subtile_3_level2_band1_max  315.171429         wavelet\n",
      "wavelet_subtile_3_level2_band2_mean  156.914286         wavelet\n",
      " wavelet_subtile_3_level2_band2_std  201.885714         wavelet\n",
      " wavelet_subtile_3_level2_band2_min  212.228571         wavelet\n",
      " wavelet_subtile_3_level2_band2_max  326.228571         wavelet\n",
      "      wavelet-subtile_4_approx_mean  158.571429 wavelet-subtile\n",
      "       wavelet-subtile_4_approx_std  212.542857 wavelet-subtile\n",
      "       wavelet-subtile_4_approx_min  209.085714 wavelet-subtile\n",
      "       wavelet-subtile_4_approx_max  319.542857 wavelet-subtile\n",
      "wavelet_subtile_4_level1_band0_mean  147.514286         wavelet\n",
      " wavelet_subtile_4_level1_band0_std  195.828571         wavelet\n",
      " wavelet_subtile_4_level1_band0_min  177.800000         wavelet\n",
      " wavelet_subtile_4_level1_band0_max   82.342857         wavelet\n",
      "wavelet_subtile_4_level1_band1_mean  139.200000         wavelet\n",
      " wavelet_subtile_4_level1_band1_std  149.428571         wavelet\n",
      " wavelet_subtile_4_level1_band1_min  171.028571         wavelet\n",
      " wavelet_subtile_4_level1_band1_max  301.257143         wavelet\n",
      "wavelet_subtile_4_level1_band2_mean  185.542857         wavelet\n",
      " wavelet_subtile_4_level1_band2_std  246.485714         wavelet\n",
      " wavelet_subtile_4_level1_band2_min  246.628571         wavelet\n",
      " wavelet_subtile_4_level1_band2_max  311.542857         wavelet\n",
      "wavelet_subtile_4_level2_band0_mean  197.228571         wavelet\n",
      " wavelet_subtile_4_level2_band0_std  238.971429         wavelet\n",
      " wavelet_subtile_4_level2_band0_min  236.571429         wavelet\n",
      " wavelet_subtile_4_level2_band0_max  324.571429         wavelet\n",
      "wavelet_subtile_4_level2_band1_mean  159.085714         wavelet\n",
      " wavelet_subtile_4_level2_band1_std  216.428571         wavelet\n",
      " wavelet_subtile_4_level2_band1_min  215.571429         wavelet\n",
      " wavelet_subtile_4_level2_band1_max  300.428571         wavelet\n",
      "wavelet_subtile_4_level2_band2_mean  143.228571         wavelet\n",
      " wavelet_subtile_4_level2_band2_std  200.914286         wavelet\n",
      " wavelet_subtile_4_level2_band2_min  208.371429         wavelet\n",
      " wavelet_subtile_4_level2_band2_max  300.457143         wavelet\n",
      "      wavelet-subtile_5_approx_mean  163.885714 wavelet-subtile\n",
      "       wavelet-subtile_5_approx_std  207.514286 wavelet-subtile\n",
      "       wavelet-subtile_5_approx_min  203.971429 wavelet-subtile\n",
      "       wavelet-subtile_5_approx_max  315.314286 wavelet-subtile\n",
      "wavelet_subtile_5_level1_band0_mean  128.942857         wavelet\n",
      " wavelet_subtile_5_level1_band0_std  163.285714         wavelet\n",
      " wavelet_subtile_5_level1_band0_min  162.342857         wavelet\n",
      " wavelet_subtile_5_level1_band0_max   26.628571         wavelet\n",
      "wavelet_subtile_5_level1_band1_mean  107.971429         wavelet\n",
      " wavelet_subtile_5_level1_band1_std  130.714286         wavelet\n",
      " wavelet_subtile_5_level1_band1_min  135.171429         wavelet\n",
      " wavelet_subtile_5_level1_band1_max  246.942857         wavelet\n",
      "wavelet_subtile_5_level1_band2_mean  182.457143         wavelet\n",
      " wavelet_subtile_5_level1_band2_std  240.057143         wavelet\n",
      " wavelet_subtile_5_level1_band2_min  236.000000         wavelet\n",
      " wavelet_subtile_5_level1_band2_max  254.057143         wavelet\n",
      "wavelet_subtile_5_level2_band0_mean  172.171429         wavelet\n",
      " wavelet_subtile_5_level2_band0_std  248.628571         wavelet\n",
      " wavelet_subtile_5_level2_band0_min  235.942857         wavelet\n",
      " wavelet_subtile_5_level2_band0_max  324.628571         wavelet\n",
      "wavelet_subtile_5_level2_band1_mean  168.800000         wavelet\n",
      " wavelet_subtile_5_level2_band1_std  214.685714         wavelet\n",
      " wavelet_subtile_5_level2_band1_min  226.714286         wavelet\n",
      " wavelet_subtile_5_level2_band1_max  243.257143         wavelet\n",
      "wavelet_subtile_5_level2_band2_mean  148.057143         wavelet\n",
      " wavelet_subtile_5_level2_band2_std  210.371429         wavelet\n",
      " wavelet_subtile_5_level2_band2_min  204.228571         wavelet\n",
      " wavelet_subtile_5_level2_band2_max  249.200000         wavelet\n",
      "      wavelet-subtile_6_approx_mean  143.800000 wavelet-subtile\n",
      "       wavelet-subtile_6_approx_std  209.171429 wavelet-subtile\n",
      "       wavelet-subtile_6_approx_min  201.542857 wavelet-subtile\n",
      "       wavelet-subtile_6_approx_max  325.400000 wavelet-subtile\n",
      "wavelet_subtile_6_level1_band0_mean  119.857143         wavelet\n",
      " wavelet_subtile_6_level1_band0_std  170.742857         wavelet\n",
      " wavelet_subtile_6_level1_band0_min  153.342857         wavelet\n",
      " wavelet_subtile_6_level1_band0_max   98.542857         wavelet\n",
      "wavelet_subtile_6_level1_band1_mean  140.914286         wavelet\n",
      " wavelet_subtile_6_level1_band1_std  170.571429         wavelet\n",
      " wavelet_subtile_6_level1_band1_min  213.314286         wavelet\n",
      " wavelet_subtile_6_level1_band1_max  296.885714         wavelet\n",
      "wavelet_subtile_6_level1_band2_mean  192.742857         wavelet\n",
      " wavelet_subtile_6_level1_band2_std  263.142857         wavelet\n",
      " wavelet_subtile_6_level1_band2_min  239.828571         wavelet\n",
      " wavelet_subtile_6_level1_band2_max  300.685714         wavelet\n",
      "wavelet_subtile_6_level2_band0_mean  205.028571         wavelet\n",
      " wavelet_subtile_6_level2_band0_std  255.742857         wavelet\n",
      " wavelet_subtile_6_level2_band0_min  263.428571         wavelet\n",
      " wavelet_subtile_6_level2_band0_max  328.085714         wavelet\n",
      "wavelet_subtile_6_level2_band1_mean  159.257143         wavelet\n",
      " wavelet_subtile_6_level2_band1_std  220.228571         wavelet\n",
      " wavelet_subtile_6_level2_band1_min  236.228571         wavelet\n",
      " wavelet_subtile_6_level2_band1_max  289.885714         wavelet\n",
      "wavelet_subtile_6_level2_band2_mean  157.571429         wavelet\n",
      " wavelet_subtile_6_level2_band2_std  206.428571         wavelet\n",
      " wavelet_subtile_6_level2_band2_min  204.257143         wavelet\n",
      " wavelet_subtile_6_level2_band2_max  279.028571         wavelet\n",
      "      wavelet-subtile_7_approx_mean  153.200000 wavelet-subtile\n",
      "       wavelet-subtile_7_approx_std  213.257143 wavelet-subtile\n",
      "       wavelet-subtile_7_approx_min  193.714286 wavelet-subtile\n",
      "       wavelet-subtile_7_approx_max  321.600000 wavelet-subtile\n",
      "wavelet_subtile_7_level1_band0_mean  128.400000         wavelet\n",
      " wavelet_subtile_7_level1_band0_std  170.428571         wavelet\n",
      " wavelet_subtile_7_level1_band0_min  167.971429         wavelet\n",
      " wavelet_subtile_7_level1_band0_max   96.971429         wavelet\n",
      "wavelet_subtile_7_level1_band1_mean  148.514286         wavelet\n",
      " wavelet_subtile_7_level1_band1_std  156.171429         wavelet\n",
      " wavelet_subtile_7_level1_band1_min  192.571429         wavelet\n",
      " wavelet_subtile_7_level1_band1_max  298.971429         wavelet\n",
      "wavelet_subtile_7_level1_band2_mean  188.000000         wavelet\n",
      " wavelet_subtile_7_level1_band2_std  237.228571         wavelet\n",
      " wavelet_subtile_7_level1_band2_min  230.200000         wavelet\n",
      " wavelet_subtile_7_level1_band2_max  329.114286         wavelet\n",
      "wavelet_subtile_7_level2_band0_mean  198.257143         wavelet\n",
      " wavelet_subtile_7_level2_band0_std  246.971429         wavelet\n",
      " wavelet_subtile_7_level2_band0_min  241.657143         wavelet\n",
      " wavelet_subtile_7_level2_band0_max  314.657143         wavelet\n",
      "wavelet_subtile_7_level2_band1_mean  169.800000         wavelet\n",
      " wavelet_subtile_7_level2_band1_std  208.000000         wavelet\n",
      " wavelet_subtile_7_level2_band1_min  218.971429         wavelet\n",
      " wavelet_subtile_7_level2_band1_max  318.000000         wavelet\n",
      "wavelet_subtile_7_level2_band2_mean  162.914286         wavelet\n",
      " wavelet_subtile_7_level2_band2_std  218.200000         wavelet\n",
      " wavelet_subtile_7_level2_band2_min  209.685714         wavelet\n",
      " wavelet_subtile_7_level2_band2_max  305.942857         wavelet\n",
      "      wavelet-subtile_8_approx_mean  162.857143 wavelet-subtile\n",
      "       wavelet-subtile_8_approx_std  209.085714 wavelet-subtile\n",
      "       wavelet-subtile_8_approx_min  205.342857 wavelet-subtile\n",
      "       wavelet-subtile_8_approx_max  312.400000 wavelet-subtile\n",
      "wavelet_subtile_8_level1_band0_mean  126.485714         wavelet\n",
      " wavelet_subtile_8_level1_band0_std  178.800000         wavelet\n",
      " wavelet_subtile_8_level1_band0_min  189.457143         wavelet\n",
      " wavelet_subtile_8_level1_band0_max   95.342857         wavelet\n",
      "wavelet_subtile_8_level1_band1_mean  151.800000         wavelet\n",
      " wavelet_subtile_8_level1_band1_std  148.257143         wavelet\n",
      " wavelet_subtile_8_level1_band1_min  190.857143         wavelet\n",
      " wavelet_subtile_8_level1_band1_max  301.114286         wavelet\n",
      "wavelet_subtile_8_level1_band2_mean  188.257143         wavelet\n",
      " wavelet_subtile_8_level1_band2_std  258.914286         wavelet\n",
      " wavelet_subtile_8_level1_band2_min  232.200000         wavelet\n",
      " wavelet_subtile_8_level1_band2_max  314.085714         wavelet\n",
      "wavelet_subtile_8_level2_band0_mean  192.371429         wavelet\n",
      " wavelet_subtile_8_level2_band0_std  239.771429         wavelet\n",
      " wavelet_subtile_8_level2_band0_min  243.971429         wavelet\n",
      " wavelet_subtile_8_level2_band0_max  320.428571         wavelet\n",
      "wavelet_subtile_8_level2_band1_mean  167.657143         wavelet\n",
      " wavelet_subtile_8_level2_band1_std  214.942857         wavelet\n",
      " wavelet_subtile_8_level2_band1_min  217.057143         wavelet\n",
      " wavelet_subtile_8_level2_band1_max  285.514286         wavelet\n",
      "wavelet_subtile_8_level2_band2_mean  155.228571         wavelet\n",
      " wavelet_subtile_8_level2_band2_std  218.257143         wavelet\n",
      " wavelet_subtile_8_level2_band2_min  197.371429         wavelet\n",
      " wavelet_subtile_8_level2_band2_max  286.428571         wavelet\n",
      "                    sobel-tile_mean  149.514286      sobel-tile\n",
      "                     sobel-tile_std  217.914286      sobel-tile\n",
      "                     sobel-tile_min  209.914286      sobel-tile\n",
      "                     sobel-tile_max  318.600000      sobel-tile\n",
      "               sobel-subtile_0_mean  124.142857   sobel-subtile\n",
      "                sobel-subtile_0_std  174.542857   sobel-subtile\n",
      "                sobel-subtile_0_min  166.514286   sobel-subtile\n",
      "                sobel-subtile_0_max  104.400000   sobel-subtile\n",
      "               sobel-subtile_1_mean  154.828571   sobel-subtile\n",
      "                sobel-subtile_1_std  171.742857   sobel-subtile\n",
      "                sobel-subtile_1_min  210.971429   sobel-subtile\n",
      "                sobel-subtile_1_max  296.028571   sobel-subtile\n",
      "               sobel-subtile_2_mean  201.057143   sobel-subtile\n",
      "                sobel-subtile_2_std  244.657143   sobel-subtile\n",
      "                sobel-subtile_2_min  229.285714   sobel-subtile\n",
      "                sobel-subtile_2_max  312.085714   sobel-subtile\n",
      "               sobel-subtile_3_mean  210.942857   sobel-subtile\n",
      "                sobel-subtile_3_std  258.514286   sobel-subtile\n",
      "                sobel-subtile_3_min  234.428571   sobel-subtile\n",
      "                sobel-subtile_3_max  299.314286   sobel-subtile\n",
      "               sobel-subtile_4_mean  159.371429   sobel-subtile\n",
      "                sobel-subtile_4_std  205.057143   sobel-subtile\n",
      "                sobel-subtile_4_min  222.057143   sobel-subtile\n",
      "                sobel-subtile_4_max  298.142857   sobel-subtile\n",
      "               sobel-subtile_5_mean  160.314286   sobel-subtile\n",
      "                sobel-subtile_5_std  220.485714   sobel-subtile\n",
      "                sobel-subtile_5_min  210.685714   sobel-subtile\n",
      "                sobel-subtile_5_max  311.628571   sobel-subtile\n",
      "               sobel-subtile_6_mean  136.942857   sobel-subtile\n",
      "                sobel-subtile_6_std  205.428571   sobel-subtile\n",
      "                sobel-subtile_6_min  209.342857   sobel-subtile\n",
      "                sobel-subtile_6_max  328.000000   sobel-subtile\n",
      "               sobel-subtile_7_mean  136.314286   sobel-subtile\n",
      "                sobel-subtile_7_std  169.342857   sobel-subtile\n",
      "                sobel-subtile_7_min  161.285714   sobel-subtile\n",
      "                sobel-subtile_7_max  150.657143   sobel-subtile\n",
      "               sobel-subtile_8_mean   89.857143   sobel-subtile\n",
      "                sobel-subtile_8_std  249.228571   sobel-subtile\n",
      "                sobel-subtile_8_min  204.171429   sobel-subtile\n",
      "                sobel-subtile_8_max  142.200000   sobel-subtile\n",
      "                    hsv-tile_H_mean  110.514286        hsv-tile\n",
      "                     hsv-tile_H_std  309.200000        hsv-tile\n",
      "                     hsv-tile_H_min  142.628571        hsv-tile\n",
      "                     hsv-tile_H_max  134.085714        hsv-tile\n",
      "                    hsv-tile_S_mean  110.200000        hsv-tile\n",
      "                     hsv-tile_S_std  296.942857        hsv-tile\n",
      "                     hsv-tile_S_min  149.714286        hsv-tile\n",
      "                     hsv-tile_S_max  151.028571        hsv-tile\n",
      "                    hsv-tile_V_mean  120.742857        hsv-tile\n",
      "                     hsv-tile_V_std  320.885714        hsv-tile\n",
      "                     hsv-tile_V_min  157.971429        hsv-tile\n",
      "                     hsv-tile_V_max  136.542857        hsv-tile\n",
      "               hsv-subtile_0_H_mean  105.628571     hsv-subtile\n",
      "                hsv-subtile_0_H_std  290.971429     hsv-subtile\n",
      "                hsv-subtile_0_H_min  134.314286     hsv-subtile\n",
      "                hsv-subtile_0_H_max  123.142857     hsv-subtile\n",
      "               hsv-subtile_0_S_mean   93.342857     hsv-subtile\n",
      "                hsv-subtile_0_S_std  291.000000     hsv-subtile\n",
      "                hsv-subtile_0_S_min  147.057143     hsv-subtile\n",
      "                hsv-subtile_0_S_max  134.885714     hsv-subtile\n",
      "               hsv-subtile_0_V_mean  119.542857     hsv-subtile\n",
      "                hsv-subtile_0_V_std  310.571429     hsv-subtile\n",
      "                hsv-subtile_0_V_min  155.571429     hsv-subtile\n",
      "                hsv-subtile_0_V_max  144.400000     hsv-subtile\n",
      "               hsv-subtile_1_H_mean  111.685714     hsv-subtile\n",
      "                hsv-subtile_1_H_std  295.142857     hsv-subtile\n",
      "                hsv-subtile_1_H_min  138.771429     hsv-subtile\n",
      "                hsv-subtile_1_H_max  125.171429     hsv-subtile\n",
      "               hsv-subtile_1_S_mean  106.800000     hsv-subtile\n",
      "                hsv-subtile_1_S_std  301.857143     hsv-subtile\n",
      "                hsv-subtile_1_S_min  143.885714     hsv-subtile\n",
      "                hsv-subtile_1_S_max  123.142857     hsv-subtile\n",
      "               hsv-subtile_1_V_mean  109.400000     hsv-subtile\n",
      "                hsv-subtile_1_V_std  300.571429     hsv-subtile\n",
      "                hsv-subtile_1_V_min  157.000000     hsv-subtile\n",
      "                hsv-subtile_1_V_max   79.028571     hsv-subtile\n",
      "               hsv-subtile_2_H_mean  113.857143     hsv-subtile\n",
      "                hsv-subtile_2_H_std   99.914286     hsv-subtile\n",
      "                hsv-subtile_2_H_min   94.285714     hsv-subtile\n",
      "                hsv-subtile_2_H_max   73.000000     hsv-subtile\n",
      "               hsv-subtile_2_S_mean  112.828571     hsv-subtile\n",
      "                hsv-subtile_2_S_std   81.485714     hsv-subtile\n",
      "                hsv-subtile_2_S_min  187.200000     hsv-subtile\n",
      "                hsv-subtile_2_S_max   24.200000     hsv-subtile\n",
      "               hsv-subtile_2_V_mean   43.742857     hsv-subtile\n",
      "                hsv-subtile_2_V_std   21.628571     hsv-subtile\n",
      "                hsv-subtile_2_V_min    3.542857     hsv-subtile\n",
      "                hsv-subtile_2_V_max  111.285714     hsv-subtile\n",
      "               hsv-subtile_3_H_mean  117.914286     hsv-subtile\n",
      "                hsv-subtile_3_H_std  135.857143     hsv-subtile\n",
      "                hsv-subtile_3_H_min   86.971429     hsv-subtile\n",
      "                hsv-subtile_3_H_max  136.400000     hsv-subtile\n",
      "               hsv-subtile_3_S_mean  137.828571     hsv-subtile\n",
      "                hsv-subtile_3_S_std  150.714286     hsv-subtile\n",
      "                hsv-subtile_3_S_min  140.771429     hsv-subtile\n",
      "                hsv-subtile_3_S_max  115.457143     hsv-subtile\n",
      "               hsv-subtile_3_V_mean  124.628571     hsv-subtile\n",
      "                hsv-subtile_3_V_std  102.857143     hsv-subtile\n",
      "                hsv-subtile_3_V_min   37.028571     hsv-subtile\n",
      "                hsv-subtile_3_V_max   92.314286     hsv-subtile\n",
      "               hsv-subtile_4_H_mean  107.342857     hsv-subtile\n",
      "                hsv-subtile_4_H_std  115.800000     hsv-subtile\n",
      "                hsv-subtile_4_H_min   85.714286     hsv-subtile\n",
      "                hsv-subtile_4_H_max  127.171429     hsv-subtile\n",
      "               hsv-subtile_4_S_mean  129.142857     hsv-subtile\n",
      "                hsv-subtile_4_S_std  123.542857     hsv-subtile\n",
      "                hsv-subtile_4_S_min  142.200000     hsv-subtile\n",
      "                hsv-subtile_4_S_max   97.942857     hsv-subtile\n",
      "               hsv-subtile_4_V_mean  132.285714     hsv-subtile\n",
      "                hsv-subtile_4_V_std  100.028571     hsv-subtile\n",
      "                hsv-subtile_4_V_min   43.657143     hsv-subtile\n",
      "                hsv-subtile_4_V_max  108.542857     hsv-subtile\n",
      "               hsv-subtile_5_H_mean  124.371429     hsv-subtile\n",
      "                hsv-subtile_5_H_std  123.542857     hsv-subtile\n",
      "                hsv-subtile_5_H_min   91.457143     hsv-subtile\n",
      "                hsv-subtile_5_H_max  144.314286     hsv-subtile\n",
      "               hsv-subtile_5_S_mean  142.342857     hsv-subtile\n",
      "                hsv-subtile_5_S_std  142.028571     hsv-subtile\n",
      "                hsv-subtile_5_S_min  155.685714     hsv-subtile\n",
      "                hsv-subtile_5_S_max  128.257143     hsv-subtile\n",
      "               hsv-subtile_5_V_mean  127.828571     hsv-subtile\n",
      "                hsv-subtile_5_V_std  115.628571     hsv-subtile\n",
      "                hsv-subtile_5_V_min   38.457143     hsv-subtile\n",
      "                hsv-subtile_5_V_max   94.485714     hsv-subtile\n",
      "               hsv-subtile_6_H_mean   99.857143     hsv-subtile\n",
      "                hsv-subtile_6_H_std  134.600000     hsv-subtile\n",
      "                hsv-subtile_6_H_min   78.057143     hsv-subtile\n",
      "                hsv-subtile_6_H_max  115.000000     hsv-subtile\n",
      "               hsv-subtile_6_S_mean  123.057143     hsv-subtile\n",
      "                hsv-subtile_6_S_std  130.171429     hsv-subtile\n",
      "                hsv-subtile_6_S_min  137.514286     hsv-subtile\n",
      "                hsv-subtile_6_S_max   92.828571     hsv-subtile\n",
      "               hsv-subtile_6_V_mean  122.257143     hsv-subtile\n",
      "                hsv-subtile_6_V_std   91.028571     hsv-subtile\n",
      "                hsv-subtile_6_V_min   39.371429     hsv-subtile\n",
      "                hsv-subtile_6_V_max   78.542857     hsv-subtile\n",
      "               hsv-subtile_7_H_mean   84.914286     hsv-subtile\n",
      "                hsv-subtile_7_H_std  110.028571     hsv-subtile\n",
      "                hsv-subtile_7_H_min   68.771429     hsv-subtile\n",
      "                hsv-subtile_7_H_max   64.857143     hsv-subtile\n",
      "               hsv-subtile_7_S_mean  105.942857     hsv-subtile\n",
      "                hsv-subtile_7_S_std  103.371429     hsv-subtile\n",
      "                hsv-subtile_7_S_min  126.428571     hsv-subtile\n",
      "                hsv-subtile_7_S_max   21.314286     hsv-subtile\n",
      "               hsv-subtile_7_V_mean   62.457143     hsv-subtile\n",
      "                hsv-subtile_7_V_std   23.000000     hsv-subtile\n",
      "                hsv-subtile_7_V_min   32.171429     hsv-subtile\n",
      "                hsv-subtile_7_V_max  100.171429     hsv-subtile\n",
      "               hsv-subtile_8_H_mean  109.371429     hsv-subtile\n",
      "                hsv-subtile_8_H_std  130.971429     hsv-subtile\n",
      "                hsv-subtile_8_H_min   86.485714     hsv-subtile\n",
      "                hsv-subtile_8_H_max  112.514286     hsv-subtile\n",
      "               hsv-subtile_8_S_mean  126.800000     hsv-subtile\n",
      "                hsv-subtile_8_S_std  146.457143     hsv-subtile\n",
      "                hsv-subtile_8_S_min  130.542857     hsv-subtile\n",
      "                hsv-subtile_8_S_max  106.171429     hsv-subtile\n",
      "               hsv-subtile_8_V_mean  114.142857     hsv-subtile\n",
      "                hsv-subtile_8_V_std  106.371429     hsv-subtile\n",
      "                hsv-subtile_8_V_min   40.028571     hsv-subtile\n",
      "                hsv-subtile_8_V_max  113.571429     hsv-subtile\n",
      "                     he-tile_H_mean  128.371429         he-tile\n",
      "                      he-tile_H_std  137.542857         he-tile\n",
      "                      he-tile_H_min   84.171429         he-tile\n",
      "                      he-tile_H_max  142.885714         he-tile\n",
      "                     he-tile_E_mean  142.142857         he-tile\n",
      "                      he-tile_E_std  144.600000         he-tile\n",
      "                      he-tile_E_min  152.828571         he-tile\n",
      "                      he-tile_E_max  113.085714         he-tile\n",
      "                he-subtile_0_H_mean  127.542857      he-subtile\n",
      "                 he-subtile_0_H_std  108.942857      he-subtile\n",
      "                 he-subtile_0_H_min   35.857143      he-subtile\n",
      "                 he-subtile_0_H_max  103.857143      he-subtile\n",
      "                he-subtile_0_E_mean  118.200000      he-subtile\n",
      "                 he-subtile_0_E_std  126.428571      he-subtile\n",
      "                 he-subtile_0_E_min   86.142857      he-subtile\n",
      "                 he-subtile_0_E_max  122.200000      he-subtile\n",
      "                he-subtile_1_H_mean  131.285714      he-subtile\n",
      "                 he-subtile_1_H_std  138.342857      he-subtile\n",
      "                 he-subtile_1_H_min  147.600000      he-subtile\n",
      "                 he-subtile_1_H_max  107.114286      he-subtile\n",
      "                he-subtile_1_E_mean  119.142857      he-subtile\n",
      "                 he-subtile_1_E_std  109.285714      he-subtile\n",
      "                 he-subtile_1_E_min   37.485714      he-subtile\n",
      "                 he-subtile_1_E_max  126.257143      he-subtile\n",
      "                he-subtile_2_H_mean  118.257143      he-subtile\n",
      "                 he-subtile_2_H_std  134.771429      he-subtile\n",
      "                 he-subtile_2_H_min   94.200000      he-subtile\n",
      "                 he-subtile_2_H_max  126.114286      he-subtile\n",
      "                he-subtile_2_E_mean  128.400000      he-subtile\n",
      "                 he-subtile_2_E_std  140.342857      he-subtile\n",
      "                 he-subtile_2_E_min  159.171429      he-subtile\n",
      "                 he-subtile_2_E_max  112.971429      he-subtile\n",
      "                he-subtile_3_H_mean  122.828571      he-subtile\n",
      "                 he-subtile_3_H_std  110.171429      he-subtile\n",
      "                 he-subtile_3_H_min   41.228571      he-subtile\n",
      "                 he-subtile_3_H_max   39.542857      he-subtile\n",
      "                he-subtile_3_E_mean   89.571429      he-subtile\n",
      "                 he-subtile_3_E_std   31.028571      he-subtile\n",
      "                 he-subtile_3_E_min   98.057143      he-subtile\n",
      "                 he-subtile_3_E_max  112.171429      he-subtile\n",
      "                he-subtile_4_H_mean  142.514286      he-subtile\n",
      "                 he-subtile_4_H_std   17.771429      he-subtile\n",
      "                 he-subtile_4_H_min  206.914286      he-subtile\n",
      "                 he-subtile_4_H_max   94.857143      he-subtile\n",
      "                he-subtile_4_E_mean  120.485714      he-subtile\n",
      "                 he-subtile_4_E_std   91.400000      he-subtile\n",
      "                 he-subtile_4_E_min  124.885714      he-subtile\n",
      "                 he-subtile_4_E_max  174.485714      he-subtile\n",
      "                he-subtile_5_H_mean  157.114286      he-subtile\n",
      "                 he-subtile_5_H_std   52.428571      he-subtile\n",
      "                 he-subtile_5_H_min  162.342857      he-subtile\n",
      "                 he-subtile_5_H_max   82.171429      he-subtile\n",
      "                he-subtile_5_E_mean  121.371429      he-subtile\n",
      "                 he-subtile_5_E_std   91.571429      he-subtile\n",
      "                 he-subtile_5_E_min  120.914286      he-subtile\n",
      "                 he-subtile_5_E_max  155.028571      he-subtile\n",
      "                he-subtile_6_H_mean  144.600000      he-subtile\n",
      "                 he-subtile_6_H_std   48.971429      he-subtile\n",
      "                 he-subtile_6_H_min  158.228571      he-subtile\n",
      "                 he-subtile_6_H_max  106.028571      he-subtile\n",
      "                he-subtile_6_E_mean  130.657143      he-subtile\n",
      "                 he-subtile_6_E_std  103.485714      he-subtile\n",
      "                 he-subtile_6_E_min  140.571429      he-subtile\n",
      "                 he-subtile_6_E_max  171.800000      he-subtile\n",
      "                he-subtile_7_H_mean  150.200000      he-subtile\n",
      "                 he-subtile_7_H_std   62.000000      he-subtile\n",
      "                 he-subtile_7_H_min  160.485714      he-subtile\n",
      "                 he-subtile_7_H_max   83.485714      he-subtile\n",
      "                he-subtile_7_E_mean  120.600000      he-subtile\n",
      "                 he-subtile_7_E_std   90.028571      he-subtile\n",
      "                 he-subtile_7_E_min  115.314286      he-subtile\n",
      "                 he-subtile_7_E_max  147.742857      he-subtile\n",
      "                he-subtile_8_H_mean  138.085714      he-subtile\n",
      "                 he-subtile_8_H_std   51.771429      he-subtile\n",
      "                 he-subtile_8_H_min  148.685714      he-subtile\n",
      "                 he-subtile_8_H_max   30.314286      he-subtile\n",
      "                he-subtile_8_E_mean   91.457143      he-subtile\n",
      "                 he-subtile_8_E_std   53.428571      he-subtile\n",
      "                 he-subtile_8_E_min   79.942857      he-subtile\n",
      "                 he-subtile_8_E_max  113.742857      he-subtile\n",
      "                         oof_pred_0  142.085714             oof\n",
      "                         oof_pred_1   50.400000             oof\n",
      "                         oof_pred_2  140.971429             oof\n",
      "                         oof_pred_3   92.057143             oof\n",
      "                         oof_pred_4  122.114286             oof\n",
      "                         oof_pred_5  100.942857             oof\n",
      "                         oof_pred_6  125.028571             oof\n",
      "                         oof_pred_7  138.342857             oof\n",
      "                         oof_pred_8  147.285714             oof\n",
      "                         oof_pred_9   58.742857             oof\n",
      "                        oof_pred_10  152.200000             oof\n",
      "                        oof_pred_11   89.542857             oof\n",
      "                        oof_pred_12  123.542857             oof\n",
      "                        oof_pred_13   90.942857             oof\n",
      "                        oof_pred_14  130.828571             oof\n",
      "                        oof_pred_15  145.457143             oof\n",
      "                        oof_pred_16  157.971429             oof\n",
      "                        oof_pred_17   56.371429             oof\n",
      "                        oof_pred_18  155.400000             oof\n",
      "                        oof_pred_19   86.285714             oof\n",
      "                        oof_pred_20  122.685714             oof\n",
      "                        oof_pred_21   83.885714             oof\n",
      "                        oof_pred_22  134.371429             oof\n",
      "                        oof_pred_23  145.571429             oof\n",
      "                        oof_pred_24  161.314286             oof\n",
      "                        oof_pred_25   51.828571             oof\n",
      "                        oof_pred_26  157.714286             oof\n",
      "                        oof_pred_27   96.228571             oof\n",
      "                        oof_pred_28  130.857143             oof\n",
      "                        oof_pred_29   93.685714             oof\n",
      "                        oof_pred_30  132.914286             oof\n",
      "                        oof_pred_31  145.685714             oof\n",
      "                        oof_pred_32  165.457143             oof\n",
      "                        oof_pred_33   58.600000             oof\n",
      "                        oof_pred_34  160.171429             oof\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) 计算平均 feature_importances\n",
    "importances = np.vstack([\n",
    "    est.feature_importances_ \n",
    "    for est in meta_model.estimators_\n",
    "]).mean(axis=0)\n",
    "\n",
    "# 2) 构造 DataFrame 并排序\n",
    "df_imp = pd.DataFrame({\n",
    "    \"feature\": name,\n",
    "    \"importance\": importances\n",
    "})\n",
    "\n",
    "# 2) 提取“类别”标签 —— 这里我们以第一个下划线前的字符串当作类别\n",
    "df_imp['category'] = df_imp['feature'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# 3) 按类别聚合（求和或求平均都可以，下面示例用求和）\n",
    "cat_imp = df_imp.groupby('category')['importance'] \\\n",
    "            .mean() \\\n",
    "            .sort_values(ascending=False)\n",
    "\n",
    "print(cat_imp)\n",
    "\n",
    "\n",
    "# 方法 A：临时设置最大行数，打印时不截断\n",
    "pd.set_option('display.max_rows', df_imp.shape[0])\n",
    "print(df_imp)\n",
    "# 如果代码运行在脚本里，记得在打印完后恢复默认：\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "# 方法 B：直接 to_string()\n",
    "print(df_imp.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Base model\n",
    "# lgb_base = lgb.LGBMRegressor(\n",
    "#     objective='l2',\n",
    "#     metric='rmse',\n",
    "#     n_estimators=12000,\n",
    "#     max_depth=15,\n",
    "#     learning_rate=0.008,\n",
    "#     num_leaves=32,\n",
    "#     colsample_bytree=0.25\n",
    "# )\n",
    "\n",
    "lgb_base = lgb.LGBMRegressor(\n",
    "    objective='l2',\n",
    "    metric='rmse',\n",
    "    learning_rate=0.007522970004049377,\n",
    "    n_estimators=12000,\n",
    "    max_depth=11,\n",
    "    num_leaves=194,\n",
    "    colsample_bytree=0.7619407413363416,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    min_data_in_leaf=20,\n",
    "    reg_alpha=0.7480401395491829,\n",
    "    reg_lambda=0.2589860348178542,\n",
    "    verbosity=-1\n",
    ")\n",
    "# 將每個 target 分別 early stopping\n",
    "meta_model = MultiOutputRegressor(lgb_base)\n",
    "\n",
    "print(\"Training LightGBM on OOF meta-features with early stopping...\")\n",
    "meta_model.estimators_ = []\n",
    "\n",
    "for i in range(y_train.shape[1]):\n",
    "    print(f\"Training target {i}...\")\n",
    "    model  = lgb.LGBMRegressor(\n",
    "        objective='l2',\n",
    "        metric='rmse',\n",
    "        learning_rate=0.007522970004049377,\n",
    "        n_estimators=12000,\n",
    "        max_depth=11,\n",
    "        num_leaves=194,\n",
    "        colsample_bytree=0.7619407413363416,\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        min_data_in_leaf=20,\n",
    "        reg_alpha=0.7480401395491829,\n",
    "        reg_lambda=0.2589860348178542,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train[:, i],\n",
    "        eval_set=[(X_val, y_val[:, i])],\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=200),\n",
    "            log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    meta_model.estimators_.append(model)\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(meta_model, os.path.join(save_root, 'meta_model.pkl'))\n",
    "\n",
    "\n",
    "# --- 3) Prepare test meta-features ---\n",
    "n_test = len(test_dataset)\n",
    "test_preds = []\n",
    "test_latents = []\n",
    "\n",
    "for fold_id in range(n_folds):\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"all\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = True\n",
    "    )\n",
    "\n",
    "    net.decoder = nn.Sequential(\n",
    "        nn.Linear(64+64, 256),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "    )\n",
    "\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = []\n",
    "    latents = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            tiles = batch['tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "\n",
    "            center = subtiles[:, 4].contiguous()\n",
    "            f_c = net.enc_center(center)\n",
    "            f_n = net.enc_neigh(subtiles)\n",
    "            fuse = torch.cat([f_c, f_n], dim=1)\n",
    "\n",
    "            out = net.decoder(fuse)\n",
    "\n",
    "            preds.append(out.cpu())\n",
    "            latents.append(fuse.cpu())  # image embedding (128D)\n",
    "\n",
    "    test_preds.append(torch.cat(preds, dim=0).numpy())      # shape: (n_test, 35)\n",
    "    test_latents.append(torch.cat(latents, dim=0).numpy())  # shape: (n_test, 128)\n",
    "\n",
    "# === Stack + Average ===\n",
    "test_preds = np.mean(np.stack(test_preds, axis=0), axis=0)      # (n_test, 35)\n",
    "test_latents = np.mean(np.stack(test_latents, axis=0), axis=0)  # (n_test, 128)\n",
    "\n",
    "with h5py.File(\"dataset/elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    spot_array = np.array(test_spots['S_7'])\n",
    "    df = pd.DataFrame(spot_array)\n",
    "\n",
    "xy = df[[\"x\", \"y\"]].to_numpy()  # shape: (n_test, 2)\n",
    "\n",
    "# 合併為最終 test meta features\n",
    "test_meta = np.concatenate([test_preds, xy, test_latents], axis=1)  # shape: (n_test, 35+2+128)\n",
    "\n",
    "\n",
    "\n",
    "final_preds = meta_model.predict(test_meta)\n",
    "\n",
    "# --- Save submission ---\n",
    "import h5py\n",
    "import pandas as pd\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spot_ids = pd.DataFrame(np.array(f[\"spots/Test\"][\"S_7\"]))\n",
    "sub = pd.DataFrame(final_preds, columns=[f\"C{i+1}\" for i in range(C)])\n",
    "sub.insert(0, 'ID', test_spot_ids.index)\n",
    "sub.to_csv(os.path.join(save_root, 'submission_stacked.csv'), index=False)\n",
    "print(f\"✅ Saved stacked submission in {save_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import rankdata\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.operate_model import predict\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "import h5py\n",
    "import pandas as pd\n",
    "# ---------------- Settings ----------------\n",
    "save_root  = save_folder  # your save_folder path\n",
    "n_folds    = len([d for d in os.listdir(save_root) if d.startswith('fold')])\n",
    "n_samples  = len(full_dataset)\n",
    "C          = 35  # num cell types\n",
    "start_fold = 0\n",
    "BATCH_SIZE = 64\n",
    "# If optimizing Spearman, convert labels to ranks\n",
    "\n",
    "# --- 1) Prepare OOF meta-features ---\n",
    "# Initialize matrix for OOF predictions\n",
    "n_samples = len(full_dataset)\n",
    "oof_preds = np.zeros((n_samples, C), dtype=np.float32)\n",
    "# True labels (raw or rank)\n",
    "# importDataset returns a dict-like sample, so label is under key 'label'\n",
    "y_true = np.vstack([ full_dataset[i]['label'].cpu().numpy() for i in range(n_samples) ])\n",
    "y_meta = y_true\n",
    "\n",
    "# Build CV splitter (must match first stage splits)\n",
    "logo = LeaveOneGroupOut()\n",
    "image_latents = np.zeros((n_samples, 128), dtype=np.float32)\n",
    "\n",
    "# Loop over folds, load best model, predict on validation indices\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "        logo.split(X=np.zeros(n_samples), y=None, groups=slide_idx)):\n",
    "    # Load model\n",
    "    # if fold_id > start_fold:\n",
    "    #     print(f\"⏭️ Skipping fold {fold_id}\")\n",
    "    #     continue\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    print(f\"Loading model from {ckpt_path}...\")\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"all\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = True\n",
    "    )\n",
    "\n",
    "    # 2) monkey‐patch 一个新的 head\n",
    "    net.decoder  = nn.Sequential(\n",
    "        nn.Linear(64+64, 256),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "        \n",
    "    )\n",
    "    net = net.to(device)    # Alternatively, if your model requires specific args, replace with:\n",
    "    # net = VisionMLP_MultiTask(tile_dim=64, subtile_dim=64, output_dim=35).to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.to(device).eval()\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_ds = Subset(full_dataset, va_idx)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    preds = []\n",
    "    latents = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            tiles    = batch['tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "\n",
    "            center = subtiles[:, 4].contiguous()\n",
    "            f_c = net.enc_center(center)\n",
    "            f_n = net.enc_neigh(subtiles)\n",
    "            fuse = torch.cat([f_c, f_n], dim=1)\n",
    "\n",
    "            output = net.decoder(fuse)\n",
    "\n",
    "            preds.append(output.cpu())\n",
    "            latents.append(fuse.cpu())  # ⬅️ 收集 latent vector\n",
    "\n",
    "    preds = torch.cat(preds, dim=0).numpy()    # (n_val, 35)\n",
    "    latents = torch.cat(latents, dim=0).numpy()  # (n_val, 128)\n",
    "\n",
    "    oof_preds[va_idx] = preds\n",
    "    image_latents[va_idx] = latents\n",
    "\n",
    "    print(f\"Fold {fold_id}: OOF preds shape {preds.shape}, Latent shape: {latents.shape}\")\n",
    "\n",
    "\n",
    "    \n",
    "with h5py.File(\"dataset/realign/filtered_dataset.h5\", \"r\") as f:\n",
    "    train_spots = f[\"spots/Train\"]\n",
    "    \n",
    "    train_spot_tables = {}\n",
    "    \n",
    "    for slide_name in train_spots.keys():\n",
    "        spot_array = np.array(train_spots[slide_name])\n",
    "        df = pd.DataFrame(spot_array)\n",
    "        df[\"slide_name\"] = slide_name\n",
    "        train_spot_tables[slide_name] = df\n",
    "        print(f\"✅ 已讀取 slide: {slide_name}\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Step 2: 合併所有 slide 的資料\n",
    "# -----------------------------------------------------\n",
    "all_train_spots_df = pd.concat(train_spot_tables.values(), ignore_index=True)\n",
    "# 提取 x, y\n",
    "xy = all_train_spots_df[[\"x\", \"y\"]].to_numpy()  # shape: (8348, 2)\n",
    "\n",
    "# 合併成新的 meta feature\n",
    "meta_features = np.concatenate([oof_preds, xy, image_latents], axis=1)\n",
    "# --- 2) Train LightGBM meta-model ---\n",
    "# Choose objective: regression on rank (for Spearman) or raw (for MSE)\n",
    "# 將 meta features 拆成訓練集與 early stopping 用的驗證集\n",
    "X_train, X_val, y_train, y_val = train_test_split(meta_features, y_meta, test_size=0.2, random_state=42)\n",
    "print(\"Meta feature shape:\", X_train.shape)\n",
    "print(\"Feature std (min/max):\", np.min(np.std(X_train, axis=0)), np.max(np.std(X_train, axis=0)))\n",
    "\n",
    "\n",
    "# # Base model\n",
    "# lgb_base = lgb.LGBMRegressor(\n",
    "#     objective='l2',\n",
    "#     metric='rmse',\n",
    "#     n_estimators=12000,\n",
    "#     max_depth=15,\n",
    "#     learning_rate=0.008,\n",
    "#     num_leaves=32,\n",
    "#     colsample_bytree=0.25\n",
    "# )\n",
    "import optuna\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'device': 'gpu',                # ✅ GPU 支援\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 0.005, 0.1),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 4, 15),\n",
    "        'num_leaves': trial.suggest_int(\"num_leaves\", 32, 256),\n",
    "        'min_data_in_leaf': trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float(\"reg_alpha\", 0, 1),\n",
    "        'reg_lambda': trial.suggest_float(\"reg_lambda\", 0, 1),\n",
    "        'n_estimators': 12000\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    multi_model = MultiOutputRegressor(model)\n",
    "    multi_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = multi_model.predict(X_val)\n",
    "    rmse = np.mean([\n",
    "        np.sqrt(mean_squared_error(y_val[:, i], y_pred[:, i]))\n",
    "        for i in range(y_val.shape[1])\n",
    "    ])\n",
    "\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Use best params to train final models\n",
    "best_params = study.best_trial.params\n",
    "best_params['objective'] = 'l2'\n",
    "best_params['metric'] = 'rmse'\n",
    "best_params['verbosity'] = -1\n",
    "\n",
    "# Train final models with best parameters\n",
    "meta_model = MultiOutputRegressor(lgb.LGBMRegressor(**best_params))\n",
    "meta_model.estimators_ = []\n",
    "\n",
    "print(\"Training LightGBM on OOF meta-features with best Optuna params...\")\n",
    "for i in range(y_train.shape[1]):\n",
    "    print(f\"Training target {i}...\")\n",
    "    model = lgb.LGBMRegressor(**best_params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train[:, i],\n",
    "        eval_set=[(X_val, y_val[:, i])],\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=200),\n",
    "            log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    meta_model.estimators_.append(model)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(meta_model, os.path.join(save_root, 'meta_model.pkl'))\n",
    "# 保存模型\n",
    "\n",
    "\n",
    "# --- 3) Prepare test meta-features ---\n",
    "n_test = len(test_dataset)\n",
    "test_preds = []\n",
    "test_latents = []\n",
    "\n",
    "for fold_id in range(n_folds):\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"all\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = True\n",
    "    )\n",
    "\n",
    "    net.decoder = nn.Sequential(\n",
    "        nn.Linear(64+64, 256),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "    )\n",
    "\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = []\n",
    "    latents = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            tiles = batch['tile'].to(device)\n",
    "            subtiles = batch['subtiles'].to(device)\n",
    "\n",
    "            center = subtiles[:, 4].contiguous()\n",
    "            f_c = net.enc_center(center)\n",
    "            f_n = net.enc_neigh(subtiles)\n",
    "            fuse = torch.cat([f_c, f_n], dim=1)\n",
    "\n",
    "            out = net.decoder(fuse)\n",
    "\n",
    "            preds.append(out.cpu())\n",
    "            latents.append(fuse.cpu())  # image embedding (128D)\n",
    "\n",
    "    test_preds.append(torch.cat(preds, dim=0).numpy())      # shape: (n_test, 35)\n",
    "    test_latents.append(torch.cat(latents, dim=0).numpy())  # shape: (n_test, 128)\n",
    "\n",
    "# === Stack + Average ===\n",
    "test_preds = np.mean(np.stack(test_preds, axis=0), axis=0)      # (n_test, 35)\n",
    "test_latents = np.mean(np.stack(test_latents, axis=0), axis=0)  # (n_test, 128)\n",
    "\n",
    "with h5py.File(\"dataset/elucidata_ai_challenge_data.h5\", \"r\") as f:\n",
    "    test_spots = f[\"spots/Test\"]\n",
    "    spot_array = np.array(test_spots['S_7'])\n",
    "    df = pd.DataFrame(spot_array)\n",
    "\n",
    "xy = df[[\"x\", \"y\"]].to_numpy()  # shape: (n_test, 2)\n",
    "\n",
    "# 合併為最終 test meta features\n",
    "test_meta = np.concatenate([test_preds, xy, test_latents], axis=1)  # shape: (n_test, 35+2+128)\n",
    "\n",
    "\n",
    "\n",
    "final_preds = meta_model.predict(test_meta)\n",
    "\n",
    "# --- Save submission ---\n",
    "import h5py\n",
    "import pandas as pd\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spot_ids = pd.DataFrame(np.array(f[\"spots/Test\"][\"S_7\"]))\n",
    "sub = pd.DataFrame(final_preds, columns=[f\"C{i+1}\" for i in range(C)])\n",
    "sub.insert(0, 'ID', test_spot_ids.index)\n",
    "sub.to_csv(os.path.join(save_root, 'submission_stacked.csv'), index=False)\n",
    "print(\"✅ Saved stacked submission.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import rankdata\n",
    "from python_scripts.import_data import importDataset\n",
    "from python_scripts.operate_model import predict\n",
    "\n",
    "# --- 配置: 只用哪些 fold 的结果来训练/预测 meta-model ---\n",
    "meta_folds = [0]  # 例如只用 fold0, fold2, fold4\n",
    "\n",
    "# 1) 准备 full_dataset, slide_idx, test_dataset 等\n",
    "full_dataset = importDataset(\n",
    "    grouped_data, model,\n",
    "    image_keys=['tile','subtiles'],\n",
    "    transform=lambda x: x\n",
    ")\n",
    "n_samples = len(full_dataset)\n",
    "C = 35  # 类别数\n",
    "\n",
    "# 2) 预留 oof_preds 和 fold_ids\n",
    "oof_preds    = np.zeros((n_samples, C), dtype=np.float32)\n",
    "oof_fold_ids = np.full(n_samples, -1, dtype=int)\n",
    "\n",
    "# 真标签\n",
    "y_true = np.vstack([ full_dataset[i]['label'].cpu().numpy() for i in range(n_samples) ])\n",
    "y_meta = y_true.copy()  # 不做 rank 时直接用 raw\n",
    "\n",
    "# 3) 生成 OOF 预测并记录 fold id\n",
    "logo = LeaveOneGroupOut()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(\n",
    "        logo.split(X=np.zeros(n_samples), y=None, groups=slide_idx)):\n",
    "\n",
    "    # 如果当前 fold 不在我们想要的 meta_folds 列表里，就跳过\n",
    "    if fold_id not in meta_folds:\n",
    "        print(f\"⏭️ Skipping OOF for fold {fold_id}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n>>> Generating OOF for fold {fold_id}\")\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"all\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = True\n",
    "    )\n",
    "\n",
    "    # 2) monkey‐patch 一个新的 head\n",
    "    net.decoder  = nn.Sequential(\n",
    "        nn.Linear(64+64, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "        \n",
    "    )\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    val_loader = DataLoader(Subset(full_dataset, va_idx), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = predict(net, val_loader, device)  # (n_val, C)\n",
    "\n",
    "    oof_preds[va_idx]    = preds\n",
    "    oof_fold_ids[va_idx] = fold_id\n",
    "\n",
    "    print(f\"  → Fold {fold_id} OOF preds shape: {preds.shape}\")\n",
    "# 4) 只选取 meta_folds 的行来训练 meta-model\n",
    "mask = np.isin(oof_fold_ids, meta_folds)\n",
    "X_meta = oof_preds[mask]\n",
    "y_meta_sub = y_meta[mask]\n",
    "\n",
    "print(f\"\\nTraining meta-model on folds {meta_folds}:\")\n",
    "print(f\"  使用样本数：{X_meta.shape[0]} / {n_samples}\")\n",
    "\n",
    "lgb_base = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=1000,\n",
    "    num_leaves=31,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    n_jobs=-1,\n",
    "    force_col_wise=True\n",
    ")\n",
    "meta_model = MultiOutputRegressor(lgb_base)\n",
    "meta_model.fit(X_meta, y_meta_sub)\n",
    "joblib.dump(meta_model, os.path.join(save_root, 'meta_model.pkl'))\n",
    "\n",
    "# 5) 准备 test_meta，只平均 meta_folds 中的预测\n",
    "n_folds = len([d for d in os.listdir(save_root) if d.startswith('fold')])\n",
    "n_test  = len(test_dataset)\n",
    "test_meta = np.zeros((n_test, C), dtype=np.float32)\n",
    "\n",
    "for fold_id in range(n_folds):\n",
    "    if fold_id not in meta_folds:\n",
    "        continue\n",
    "    ckpt_path = os.path.join(save_root, f\"fold{fold_id}\", \"best_model.pt\")\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"all\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = True\n",
    "    )\n",
    "\n",
    "    # 2) monkey‐patch 一个新的 head\n",
    "    net.decoder  = nn.Sequential(\n",
    "        nn.Linear(64+64, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "        \n",
    "    )\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    preds = predict(net, loader, device)\n",
    "    test_meta += preds\n",
    "\n",
    "# 平均时除以参与的 folds 数目\n",
    "test_meta /= len(meta_folds)\n",
    "\n",
    "# 6) 用 meta-model 做最终预测\n",
    "final_preds = meta_model.predict(test_meta)\n",
    "\n",
    "# --- Save submission ---\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spot_ids = pd.DataFrame(np.array(f[\"spots/Test\"][\"S_7\"]))\n",
    "\n",
    "sub = pd.DataFrame(final_preds, columns=[f\"C{i+1}\" for i in range(C)])\n",
    "sub.insert(0, 'ID', test_spot_ids.index)\n",
    "sub.to_csv(os.path.join(save_root, 'submission_stacked.csv'), index=False)\n",
    "print(\"✅ Saved stacked submission.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 從 '<class 'list'>' 推斷樣本數量: 2088\n",
      "Model forward signature: (tile, subtiles)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from python_scripts.operate_model import get_model_inputs\n",
    "from python_scripts.import_data import load_node_feature_data\n",
    "\n",
    "\n",
    "image_keys = [ 'tile', 'subtiles']\n",
    "\n",
    "model = VisionMLP_MultiTask(tile_dim=tile_dim, subtile_dim=center_dim, output_dim=C)\n",
    "\n",
    "# 用法示例\n",
    "from python_scripts.import_data import importDataset\n",
    "# 假设你的 model 已经定义好并实例化为 `model`\n",
    "test_dataset = load_node_feature_data(\"dataset/spot-rank/filtered_directly_rank/masked/test/Macenko/test_dataset.pt\", model)\n",
    "test_dataset = importDataset(\n",
    "        data_dict=test_dataset,\n",
    "        model=model,\n",
    "        image_keys=image_keys,\n",
    "        transform=lambda x: x,  # identity transform\n",
    "        print_sig=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset.check_item(1000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 讀 test spot index\n",
    "with h5py.File(\"./dataset/elucidata_ai_challenge_data.h5\",\"r\") as f:\n",
    "    test_spots     = f[\"spots/Test\"]\n",
    "    test_spot_table= pd.DataFrame(np.array(test_spots['S_7']))\n",
    "\n",
    "fold_ckpts = sorted(glob.glob(os.path.join(save_folder, \"fold*\", \"best_model.pt\")))\n",
    "models = []\n",
    "for ckpt in fold_ckpts:\n",
    "    net = PretrainedEncoderRegressor(\n",
    "        ae_checkpoint=checkpoint_path,\n",
    "        ae_type=\"all\",\n",
    "        center_dim=64, neighbor_dim=64, hidden_dim=128,\n",
    "        tile_size=26, output_dim=35,\n",
    "        freeze_encoder = False\n",
    "    )\n",
    "\n",
    "    # 2) monkey‐patch 一个新的 head\n",
    "    net.decoder  = nn.Sequential(\n",
    "        nn.Linear(64+64, 256),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.SiLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(64, 35)\n",
    "        \n",
    "    )\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(ckpt, map_location=\"cpu\"))\n",
    "    net.to(device).eval()\n",
    "    models.append(net)\n",
    "\n",
    "all_fold_preds = []\n",
    "for fold_id, net in enumerate(models):\n",
    "    # 推論\n",
    "    with torch.no_grad():\n",
    "        preds = predict(net, test_loader, device)  # (N_test,35) numpy array\n",
    "\n",
    "    # 1) 存每一折的原始預測\n",
    "    df_fold = pd.DataFrame(preds, columns=[f\"C{i+1}\" for i in range(preds.shape[1])])\n",
    "    df_fold.insert(0, \"ID\", test_spot_table.index)\n",
    "    path_fold = os.path.join(save_folder, f\"submission_fold{fold_id}.csv\")\n",
    "    df_fold.to_csv(path_fold, index=False)\n",
    "    print(f\"✅ Saved fold {fold_id} predictions to {path_fold}\")\n",
    "\n",
    "    all_fold_preds.append(preds)\n",
    "\n",
    "# 2) 做 rank‐average ensemble\n",
    "all_fold_preds = np.stack(all_fold_preds, axis=0)       # (K, N_test, 35)\n",
    "ranks          = all_fold_preds.argsort(axis=2).argsort(axis=2).astype(float)\n",
    "mean_rank      = ranks.mean(axis=0)                    # (N_test,35)\n",
    "\n",
    "# 3) 存 final ensemble\n",
    "df_ens = pd.DataFrame(mean_rank, columns=[f\"C{i+1}\" for i in range(mean_rank.shape[1])])\n",
    "df_ens.insert(0, \"ID\", test_spot_table.index)\n",
    "path_ens = os.path.join(save_folder, \"submission_rank_ensemble.csv\")\n",
    "df_ens.to_csv(path_ens, index=False)\n",
    "print(f\"✅ Saved rank‐ensemble submission to {path_ens}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialhackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
